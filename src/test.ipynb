{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LRScheduler\n",
    "from torch.nn import Transformer\n",
    "import json\n",
    "import random\n",
    "import copy\n",
    "import warnings\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, nhead=4, num_layers=4, d_ff=1024, max_len=32, \n",
    "                 dropout=0.3, device=\"cuda\", pad_token_id=0, start_token_id=1, end_token_id=2):\n",
    "        super(CharTransformer, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.start_token_id = start_token_id\n",
    "        self.end_token_id = end_token_id\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model).to(device)\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout after embedding\n",
    "        \n",
    "        # Create sinusoidal positional encodings\n",
    "        self.register_buffer(\"positional_encoding\", self._generate_sinusoidal_encoding(max_len, d_model))\n",
    "        \n",
    "        self.transformer = Transformer(\n",
    "            d_model=d_model, nhead=nhead, num_encoder_layers=num_layers, \n",
    "            num_decoder_layers=num_layers, dim_feedforward=d_ff, activation=\"gelu\", batch_first=True, \n",
    "            dropout=dropout \n",
    "        ).to(device)\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size).to(device)\n",
    "    \n",
    "    def _generate_sinusoidal_encoding(self, max_len, d_model):\n",
    "        position = torch.arange(max_len, device=self.device).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, device=self.device) * -(math.log(10000.0) / d_model))\n",
    "        encoding = torch.zeros(max_len, d_model, device=self.device)\n",
    "        encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        return encoding\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.tril(torch.ones(sz, sz))\n",
    "        return torch.log(mask).to(self.device)\n",
    "        \n",
    "    def forward(self, src, tgt, feature_mask, tgt_is_causal=False, src_mask=None, tgt_mask=None, \n",
    "                src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        src, tgt = src.to(self.device), tgt.to(self.device)\n",
    "        \n",
    "        # Compute embeddings and apply dropout\n",
    "        src_emb = self.embedding(src)\n",
    "        tgt_emb = self.embedding(tgt)\n",
    "\n",
    "        # Apply positional encoding\n",
    "        src_emb += (1 - feature_mask[:, :src.shape[1], None]) * self.positional_encoding[:src.shape[1], :]\n",
    "        tgt_emb += self.positional_encoding[:tgt.shape[1], :].unsqueeze(0)\n",
    "        \n",
    "        src_emb = self.dropout(src_emb)\n",
    "        tgt_emb = self.dropout(tgt_emb)\n",
    "\n",
    "        # Compute key padding masks if not provided\n",
    "        if src_key_padding_mask is None:\n",
    "            src_key_padding_mask = (src == self.pad_token_id) \n",
    "        if tgt_key_padding_mask is None:\n",
    "            tgt_key_padding_mask = (tgt == self.pad_token_id)\n",
    "        if tgt_is_causal: \n",
    "            if tgt_mask is None: \n",
    "                tgt_mask = self._generate_square_subsequent_mask(tgt.shape[1])\n",
    "            else:\n",
    "                tgt_mask += self._generate_square_subsequent_mask(tgt.shape[1])\n",
    "\n",
    "        # Transformer forward pass\n",
    "        transformer_output = self.transformer(\n",
    "            src_emb, tgt_emb, \n",
    "            src_mask=src_mask, \n",
    "            tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            tgt_is_causal=tgt_is_causal\n",
    "        )\n",
    "        output = self.fc_out(transformer_output)\n",
    "        return output\n",
    "    \n",
    "    def generate(self, src, feature_mask, max_len=32, beam_size=5):\n",
    "        self.eval()\n",
    "        src, feature_mask = src.to(self.device), feature_mask.to(self.device)\n",
    "\n",
    "        # Initialize beams: (sequence, log probability)\n",
    "        beams = torch.full((1, 1), self.start_token_id, device=self.device)  \n",
    "        beam_scores = torch.zeros(1, device=self.device)  # Log probabilities\n",
    "\n",
    "        completed_sequences = []  # Store completed sequences\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            # Expand `src` to match the number of beams\n",
    "            src_expanded = src.expand(beams.shape[0], -1)\n",
    "            feature_mask_expanded = feature_mask.expand(beams.shape[0], -1)\n",
    "\n",
    "            # Forward pass on all beams at once\n",
    "            out = self.forward(src_expanded, beams, feature_mask_expanded)  \n",
    "            logits = out[:, -1, :]  # Get last-step logits (shape: [beams, vocab_size])\n",
    "            log_probs = torch.log_softmax(logits, dim=-1)  # Convert logits to log-probabilities\n",
    "\n",
    "            # Get top-k candidates for each beam (shape: [beams, beam_size])\n",
    "            topk_log_probs, topk_ids = log_probs.topk(beam_size, dim=-1)\n",
    "\n",
    "            # Compute new scores by adding log probabilities (broadcasted)\n",
    "            expanded_scores = beam_scores.unsqueeze(1) + topk_log_probs  # Shape: [beams, beam_size]\n",
    "            expanded_scores = expanded_scores.view(-1)  # Flatten to [beams * beam_size]\n",
    "\n",
    "            # Get top-k overall candidates\n",
    "            topk_scores, topk_indices = expanded_scores.topk(beam_size)\n",
    "\n",
    "            # Convert flat indices to beam/token indices\n",
    "            beam_indices = topk_indices // beam_size  # Which original beam did this come from?\n",
    "            token_indices = topk_indices % beam_size  # Which token was selected?\n",
    "\n",
    "            # Append new tokens to sequences\n",
    "            new_beams = torch.cat([beams[beam_indices], topk_ids.view(-1, 1)[topk_indices]], dim=-1)\n",
    "\n",
    "            # Check for completed sequences\n",
    "            eos_mask = (new_beams[:, -1] == self.end_token_id)\n",
    "            if eos_mask.any():\n",
    "                for i in range(beam_size):\n",
    "                    if eos_mask[i]:\n",
    "                        completed_sequences.append((new_beams[i], topk_scores[i]))\n",
    "\n",
    "            # Keep only unfinished sequences\n",
    "            beams = new_beams[~eos_mask]\n",
    "            beam_scores = topk_scores[~eos_mask]\n",
    "\n",
    "            # If all sequences finished, stop early\n",
    "            if len(beams) == 0 or len(completed_sequences) >= beam_size:\n",
    "                break\n",
    "\n",
    "        # Choose the best sequence from completed ones\n",
    "        if completed_sequences:\n",
    "            best_sequence = max(completed_sequences, key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            best_sequence = beams[0]  # If no sequence completed, return best unfinished one\n",
    "\n",
    "        return best_sequence\n",
    "    \n",
    "    def greedy_batch_decode(self, src, feature_mask, max_len=32):\n",
    "        self.eval()\n",
    "        batch_size = src.shape[0]\n",
    "        src, feature_mask = src.to(self.device), feature_mask.to(self.device)\n",
    "        outputs = torch.full((batch_size, 1), self.start_token_id, device=self.device)\n",
    "        ended = torch.zeros(batch_size, dtype=torch.bool, device=self.device)\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            out = self.forward(src, outputs, feature_mask)\n",
    "            next_tokens = out[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "            outputs = torch.cat([outputs, next_tokens], dim=1)\n",
    "            ended |= (next_tokens.squeeze() == self.end_token_id)\n",
    "            if ended.all():\n",
    "                break\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples: [(['<s>', '<PRS>', 'r', 'a', 'f', 't', '</s>'], ['<s>', 'r', 'a', 'f', 't', '</s>']), (['<s>', '<3SG>', 'r', 'a', 'f', 't', '</s>'], ['<s>', 'r', 'a', 'f', 't', 's', '</s>']), (['<s>', '<PST>', 'r', 'a', 'f', 't', '</s>'], ['<s>', 'r', 'a', 'f', 't', 'e', 'd', '</s>']), (['<s>', '<PRS.PTCP>', 'r', 'a', 'f', 't', '</s>'], ['<s>', 'r', 'a', 'f', 't', 'i', 'n', 'g', '</s>']), (['<s>', '<PST.PTCP>', 'r', 'a', 'f', 't', '</s>'], ['<s>', 'r', 'a', 'f', 't', 'e', 'd', '</s>'])]\n",
      "Test examples: [(['<s>', '<PRS>', 'c', 'r', 'i', 'n', 'c', 'h', '</s>'], ['<s>', 'c', 'r', 'i', 'n', 'c', 'h', '</s>']), (['<s>', '<3SG>', 'c', 'r', 'i', 'n', 'c', 'h', '</s>'], ['<s>', 'c', 'r', 'i', 'n', 'c', 'h', 'e', 's', '</s>']), (['<s>', '<PST>', 'c', 'r', 'i', 'n', 'c', 'h', '</s>'], ['<s>', 'c', 'r', 'i', 'n', 'c', 'h', 'e', 'd', '</s>']), (['<s>', '<PRS.PTCP>', 'c', 'r', 'i', 'n', 'c', 'h', '</s>'], ['<s>', 'c', 'r', 'i', 'n', 'c', 'h', 'i', 'n', 'g', '</s>']), (['<s>', '<PST.PTCP>', 'c', 'r', 'i', 'n', 'c', 'h', '</s>'], ['<s>', 'c', 'r', 'i', 'n', 'c', 'h', 'e', 'd', '</s>'])]\n",
      "Validation examples: [(['<s>', '<PRS>', 'r', 'e', 'c', 'r', 'u', 'i', 't', '</s>'], ['<s>', 'r', 'e', 'c', 'r', 'u', 'i', 't', '</s>']), (['<s>', '<3SG>', 'r', 'e', 'c', 'r', 'u', 'i', 't', '</s>'], ['<s>', 'r', 'e', 'c', 'r', 'u', 'i', 't', 's', '</s>']), (['<s>', '<PST>', 'r', 'e', 'c', 'r', 'u', 'i', 't', '</s>'], ['<s>', 'r', 'e', 'c', 'r', 'u', 'i', 't', 'e', 'd', '</s>']), (['<s>', '<PRS.PTCP>', 'r', 'e', 'c', 'r', 'u', 'i', 't', '</s>'], ['<s>', 'r', 'e', 'c', 'r', 'u', 'i', 't', 'i', 'n', 'g', '</s>']), (['<s>', '<PST.PTCP>', 'r', 'e', 'c', 'r', 'u', 'i', 't', '</s>'], ['<s>', 'r', 'e', 'c', 'r', 'u', 'i', 't', 'e', 'd', '</s>'])]\n"
     ]
    }
   ],
   "source": [
    "# Data preparation\n",
    "\n",
    "def prepare_data(data, val_size=1000, train_size=2000, test_ratio=0.4):\n",
    "    paradigms = list(data.items())\n",
    "    random.shuffle(paradigms)\n",
    "    \n",
    "    val_set = paradigms[:val_size]\n",
    "    train_test_set = paradigms[val_size:val_size + train_size]\n",
    "    \n",
    "    train_size = int((1 - test_ratio) * len(train_test_set))\n",
    "    train_set = train_test_set[:train_size]\n",
    "    test_set = train_test_set[train_size:]\n",
    "    \n",
    "    return train_set, test_set, val_set\n",
    "\n",
    "def generate_examples(paradigm):\n",
    "    lemma = list(paradigm[0])  # Convert lemma to list of characters\n",
    "    forms = paradigm[1]\n",
    "    examples = []\n",
    "    \n",
    "    for tag, form in forms.items():\n",
    "        src = ['<s>', f'<{tag}>'] + lemma + ['</s>']\n",
    "        tgt = ['<s>'] + list(form) + ['</s>']\n",
    "        examples.append((src, tgt))\n",
    "    \n",
    "    return examples\n",
    "\n",
    "# Load dataset\n",
    "with open(\"/home/minhk/Assignments/CSCI 5801/project/data/processed/eng_v.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "train_set, test_set, val_set = prepare_data(data, val_size=1000, train_size=1000, test_ratio=0.1)\n",
    "\n",
    "train_examples = [ex for paradigm in train_set for ex in generate_examples(paradigm)]\n",
    "test_examples = [ex for paradigm in test_set for ex in generate_examples(paradigm)]\n",
    "val_examples = [ex for paradigm in val_set for ex in generate_examples(paradigm)]\n",
    "\n",
    "print(\"Train examples:\", train_examples[:5])  # Show first 5 examples\n",
    "print(\"Test examples:\", test_examples[:5])\n",
    "print(\"Validation examples:\", val_examples[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InverseSquareLRWithWarmup(LRScheduler):\n",
    "    \"\"\"\n",
    "    Implements an inverse square learning rate scheduler with warmup steps.\n",
    "    \n",
    "    During warmup, the learning rate increases linearly from init_lr to max_lr.\n",
    "    After warmup, the learning rate decreases according to the inverse square of the step number:\n",
    "    lr = max_lr * (warmup_steps / step)^2 for step > warmup_steps\n",
    "    \n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        init_lr (float): Initial learning rate during warmup phase. Default: 0.0\n",
    "        max_lr (float): Maximum learning rate after warmup phase. Default: 0.1\n",
    "        warmup_steps (int): Number of warmup steps. Default: 1000\n",
    "        last_epoch (int): The index of the last epoch. Default: -1\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer, init_lr=0.0, max_lr=0.001, warmup_steps=1000, last_epoch=-1):\n",
    "        self.init_lr = init_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        super(InverseSquareLRWithWarmup, self).__init__(optimizer, last_epoch)\n",
    "        \n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\")\n",
    "        \n",
    "        if self.last_epoch < self.warmup_steps:\n",
    "            # Linear warmup phase\n",
    "            alpha = self.last_epoch / self.warmup_steps\n",
    "            return [self.init_lr + alpha * (self.max_lr - self.init_lr) for _ in self.base_lrs]\n",
    "        else:\n",
    "            # Inverse square decay phase\n",
    "            decay_factor = math.sqrt(self.warmup_steps / self.last_epoch)\n",
    "            return [self.max_lr * decay_factor for _ in self.base_lrs]\n",
    "            \n",
    "    def _get_closed_form_lr(self):\n",
    "        if self.last_epoch < self.warmup_steps:\n",
    "            # Linear warmup phase\n",
    "            alpha = self.last_epoch / self.warmup_steps\n",
    "            return [self.init_lr + alpha * (self.max_lr - self.init_lr) for _ in self.base_lrs]\n",
    "        else:\n",
    "            # Inverse square decay phase\n",
    "            decay_factor = (self.warmup_steps / self.last_epoch) ** 2\n",
    "            return [self.max_lr * decay_factor for _ in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampler for scheduled learning, gradually replaces ground truth (teacher forcing) with model input\n",
    "\n",
    "class ScheduledSampler():\n",
    "    def __init__(self, base_rate=0.5, warmup_steps=1000):\n",
    "        self.base_rate = base_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.step_count = 0        \n",
    "        self.sampling_rate = 1\n",
    "    \n",
    "    def step(self):\n",
    "        self.step_count += 1\n",
    "        if self.step_count > self.warmup_steps:\n",
    "            self.sampling_rate = self.base_rate + (1 - self.base_rate) * math.sqrt(self.warmup_steps / self.step_count)\n",
    "\n",
    "    def sample(self, logits, truth_ids):\n",
    "        \"\"\"\n",
    "        Selects truth_ids with probability `sampling_rate`, \n",
    "        otherwise samples using Gumbel noise.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, vocab_size = logits.shape\n",
    "        \n",
    "        # Decide per-token whether to take ground truth (1) or Gumbel sample (0)\n",
    "        mask = torch.bernoulli(torch.full((batch_size, seq_len), self.sampling_rate, device=logits.device, dtype=float)).bool()\n",
    "        \n",
    "        # Gumbel-sampled predictions\n",
    "        gumbel_preds = self._gumbel_sample(logits)\n",
    "        \n",
    "        # Use ground truth where mask == True, else use gumbel_preds\n",
    "        return torch.where(mask, truth_ids, gumbel_preds)\n",
    "    \n",
    "    def _gumbel_sample(self, logits):\n",
    "        gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits)))  # Generate Gumbel noise\n",
    "        return (logits + gumbel_noise).argmax(dim=-1)  # Apply Gumbel noise and take the argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ', 'E', '<PST.PTCP>', 'ë', '<PRS.PTCP>', 'ê', 'ö', 'ï', 'w', 'R', 'o', 'q', 'i', 'h', 'T', 't', 'k', \"'\", 'p', 'l', '’', 'v', 's', '*', 'è', '<3SG>', '0', 'n', 'u', 'e', 'b', 'U', '4', 'æ', 'f', 'ä', '9', 'r', '6', '<PST>', 'é', '8', '<PRS>', 'z', '-', 'û', 'j', 'x', '1', 'y', 'c', 'd', 'm', 'œ', 'g', '/', 'a'}\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "def tokenize(sequence, char_to_idx):\n",
    "    return [char_to_idx[char] for char in sequence]\n",
    "\n",
    "# Build character vocabulary\n",
    "all_chars = set()\n",
    "for word, inflect in data.items():\n",
    "    all_chars.update(word)\n",
    "    for tag, forms in inflect.items():\n",
    "        all_chars.add(f\"<{tag}>\")\n",
    "        all_chars.update(forms)\n",
    "print(all_chars)\n",
    "char_to_idx = {char: i for i, char in enumerate(sorted(all_chars), start=3)}  # Reserve 0, 1, 2 for special tokens\n",
    "char_to_idx['<pad>'] = 0\n",
    "char_to_idx['<s>'] = 1\n",
    "char_to_idx['</s>'] = 2\n",
    "idx_to_char = {\n",
    "    i: char for char, i in char_to_idx.items()\n",
    "}\n",
    "vocab_size = len(char_to_idx)\n",
    "max_len = 32\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CharTransformer(vocab_size, device=device, max_len=max_len)\n",
    "\n",
    "def pad_sequence(sequence, max_len, pad_token='<pad>'):\n",
    "    return sequence + [pad_token] * (max_len - len(sequence))\n",
    "\n",
    "def create_feature_mask(sequence):\n",
    "    \"\"\"Create a feature mask where tags (enclosed in < >) are marked as 1, else 0.\"\"\"\n",
    "    return torch.tensor([1 if char.startswith('<') and char.endswith('>') else 0 for char in sequence], device=device)\n",
    "\n",
    "def create_padding_mask(sequence, pad_token='<pad>'):\n",
    "    \"\"\"Create a padding mask where padding tokens are marked as True (to be ignored).\"\"\"\n",
    "    return (sequence == pad_token)\n",
    "\n",
    "def train_model(model, train_examples, test_examples, epochs=1000, batch_size=256, patience=20):\n",
    "    optimizer = optim.AdamW(model.parameters(), betas=(0.99, 0.98))\n",
    "    scheduler = InverseSquareLRWithWarmup(optimizer, init_lr=1e-5, max_lr=1e-3, warmup_steps=4000)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    # sampler = ScheduledSampler(base_rate=0.5, warmup_steps=4000)\n",
    "\n",
    "    pad_token = char_to_idx['<pad>']\n",
    "    best_test_loss = float('inf')  # Initialize the best test loss to a very large value\n",
    "    best_model_state = copy.deepcopy(model.state_dict())  # Store best model parameters\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        random.shuffle(train_examples)\n",
    "        \n",
    "        for i in range(0, len(train_examples), batch_size):\n",
    "            batch = train_examples[i:i+batch_size]\n",
    "            src_batch, tgt_batch = zip(*batch)\n",
    "\n",
    "            max_batch_len = max(max(len(s) for s in src_batch), max(len(t) for t in tgt_batch))\n",
    "            \n",
    "            # Pad sequences to the maximum length (max_len) in the batch\n",
    "            src_padded = [pad_sequence(seq, max_batch_len) for seq in src_batch]\n",
    "            tgt_padded = [pad_sequence(seq, max_batch_len) for seq in tgt_batch]\n",
    "            \n",
    "            # Convert padded sequences to tensors\n",
    "            src_tensor = torch.tensor([tokenize(seq, char_to_idx) for seq in src_padded], device=device)\n",
    "            tgt_tensor = torch.tensor([tokenize(seq, char_to_idx) for seq in tgt_padded], device=device)\n",
    "            \n",
    "            # Create the feature mask\n",
    "            feature_mask_src = torch.stack([create_feature_mask(seq) for seq in src_padded], dim=0)\n",
    "\n",
    "            # Shift target tensor for teacher forcing (the model will predict next token)\n",
    "            tgt_input = tgt_tensor[:, :-1]  # Remove the last token (it's not used as input)\n",
    "            tgt_expected = tgt_tensor[:, 1:]  # The target sequence for the loss is shifted by 1\n",
    "\n",
    "            # First round of predictions (using teacher forcing)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src_tensor, tgt_input, feature_mask_src, tgt_is_causal=True)\n",
    "\n",
    "            \"\"\" # Apparently teacher forcing all the way is better now\n",
    "            # Sample from model output and true target based on current sampling rate\n",
    "            sampled_tgt_input = sampler.sample(output, tgt_expected)\n",
    "\n",
    "            # Ensure proper alignment for next round of input:\n",
    "            # Take the first element of each item in the target sequence (start token)\n",
    "            # Concatenate with the sampled output (excluding the last token)\n",
    "            sampled_tgt_input = torch.cat(\n",
    "                [tgt_input[:, :1], sampled_tgt_input[:, :-1]], dim=1\n",
    "            )\n",
    "\n",
    "            # Second round of predictions using the sampled input\n",
    "            output = model(src_tensor, sampled_tgt_input, feature_mask_src, tgt_is_causal=True)\n",
    "            \"\"\"\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(output.reshape(-1, vocab_size), tgt_expected.reshape(-1))\n",
    "\n",
    "            # Apply padding mask to loss (ignores padded tokens)\n",
    "            tgt_mask = (tgt_input != pad_token).float().view(-1)\n",
    "            loss = loss * tgt_mask  # Element-wise multiply with the mask to ignore padding tokens\n",
    "            loss = loss.sum() / tgt_mask.sum()  # Normalize the loss (average over non-padding tokens)\n",
    "            \n",
    "            # Apply loss\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            # sampler.step()  # Update the sampler (teacher forcing rate)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss * batch_size / len(train_examples)}\")\n",
    "        \n",
    "        # Evaluate on the test set\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(test_examples), batch_size):\n",
    "                batch = test_examples[i:i+batch_size]\n",
    "                src_batch, tgt_batch = zip(*batch)\n",
    "\n",
    "                max_batch_len = max(max(len(s) for s in src_batch), max(len(t) for t in tgt_batch))\n",
    "\n",
    "                # Pad sequences to the maximum length (max_len) in the batch\n",
    "                src_padded = [pad_sequence(seq, max_batch_len) for seq in src_batch]\n",
    "                tgt_padded = [pad_sequence(seq, max_batch_len) for seq in tgt_batch]\n",
    "\n",
    "                # Convert padded sequences to tensors\n",
    "                src_tensor = torch.tensor([tokenize(seq, char_to_idx) for seq in src_padded], device=device)\n",
    "                tgt_tensor = torch.tensor([tokenize(seq, char_to_idx) for seq in tgt_padded], device=device)\n",
    "\n",
    "                # Create the feature mask\n",
    "                feature_mask_src = torch.stack([create_feature_mask(seq) for seq in src_padded], dim=0)\n",
    "\n",
    "                # Shift target tensor\n",
    "                tgt_input = tgt_tensor[:, :-1]\n",
    "                tgt_expected = tgt_tensor[:, 1:]\n",
    "\n",
    "                # Forward pass\n",
    "                output = model(src_tensor, tgt_input, feature_mask_src, tgt_is_causal=True)\n",
    "\n",
    "                # Compute the loss\n",
    "                loss = criterion(output.reshape(-1, vocab_size), tgt_expected.reshape(-1))\n",
    "\n",
    "                # Apply padding mask to loss\n",
    "                tgt_mask = (tgt_expected != pad_token).float().view(-1)\n",
    "                loss = loss * tgt_mask  # Element-wise multiply with the mask to ignore padding tokens\n",
    "                loss = loss.sum() / tgt_mask.sum()  # Normalize the loss (average over non-padding tokens)\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "\n",
    "        test_loss = test_loss * batch_size / len(test_examples)\n",
    "        print(f\"Test Loss after Epoch {epoch+1}: {test_loss}\")\n",
    "\n",
    "        # Early stopping based on test set loss\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            best_epoch = epoch\n",
    "            patience_count = 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())  # Save best model state\n",
    "        else:\n",
    "            patience_count += 1\n",
    "            if patience_count == patience:\n",
    "                patience_count = 0\n",
    "                patience = int(patience * math.sqrt(2))\n",
    "                # Rollback to best model state (undo last epoch update)\n",
    "                model.load_state_dict(best_model_state)\n",
    "                print(f\"Rolling back to best model from epoch {best_epoch + 1}\")\n",
    "                print(f\"Best test loss: {best_test_loss}\")\n",
    "        \n",
    "    model.load_state_dict(best_model_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2250 11250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhk/Assignments/CSCI 5801/project/venv/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3.5518573827107747\n",
      "Test Loss after Epoch 1: 3.6221088119506835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhk/Assignments/CSCI 5801/project/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 2.858058196767171\n",
      "Test Loss after Epoch 2: 2.8798387512207033\n",
      "Epoch 3, Loss: 2.6143098167419434\n",
      "Test Loss after Epoch 3: 2.627322847175598\n",
      "Epoch 4, Loss: 2.3491372575971816\n",
      "Test Loss after Epoch 4: 2.4874822744369505\n",
      "Epoch 5, Loss: 2.1714695446650185\n",
      "Test Loss after Epoch 5: 2.444875334739685\n",
      "Epoch 6, Loss: 2.057154402669271\n",
      "Test Loss after Epoch 6: 2.354931348991394\n",
      "Epoch 7, Loss: 1.96064908235338\n",
      "Test Loss after Epoch 7: 2.23118809261322\n",
      "Epoch 8, Loss: 1.8575548332850138\n",
      "Test Loss after Epoch 8: 2.1422642566680907\n",
      "Epoch 9, Loss: 1.7842606801986693\n",
      "Test Loss after Epoch 9: 2.078327885055542\n",
      "Epoch 10, Loss: 1.724966934246487\n",
      "Test Loss after Epoch 10: 2.0150924133300783\n",
      "Epoch 11, Loss: 1.6728838064829508\n",
      "Test Loss after Epoch 11: 1.9940813583374024\n",
      "Epoch 12, Loss: 1.6380140181223553\n",
      "Test Loss after Epoch 12: 1.9518109386444091\n",
      "Epoch 13, Loss: 1.6004921262105305\n",
      "Test Loss after Epoch 13: 1.910543930053711\n",
      "Epoch 14, Loss: 1.5613300988727146\n",
      "Test Loss after Epoch 14: 1.8629816896438598\n",
      "Epoch 15, Loss: 1.515402473364936\n",
      "Test Loss after Epoch 15: 1.829632162475586\n",
      "Epoch 16, Loss: 1.4905084277682834\n",
      "Test Loss after Epoch 16: 1.788470140838623\n",
      "Epoch 17, Loss: 1.4490942122353447\n",
      "Test Loss after Epoch 17: 1.744593337249756\n",
      "Epoch 18, Loss: 1.4261920509550305\n",
      "Test Loss after Epoch 18: 1.6785683097839355\n",
      "Epoch 19, Loss: 1.3863426234987046\n",
      "Test Loss after Epoch 19: 1.6379283382415772\n",
      "Epoch 20, Loss: 1.3498139198091295\n",
      "Test Loss after Epoch 20: 1.5992365085601807\n",
      "Epoch 21, Loss: 1.3168283339182536\n",
      "Test Loss after Epoch 21: 1.5238842290878296\n",
      "Epoch 22, Loss: 1.277321119901869\n",
      "Test Loss after Epoch 22: 1.4587434301376343\n",
      "Epoch 23, Loss: 1.2318759097417196\n",
      "Test Loss after Epoch 23: 1.434624366569519\n",
      "Epoch 24, Loss: 1.1912902156194052\n",
      "Test Loss after Epoch 24: 1.3808311542510987\n",
      "Epoch 25, Loss: 1.1480663352330527\n",
      "Test Loss after Epoch 25: 1.3610104068756104\n",
      "Epoch 26, Loss: 1.1062931253857082\n",
      "Test Loss after Epoch 26: 1.3532987621307373\n",
      "Epoch 27, Loss: 1.0736667699813842\n",
      "Test Loss after Epoch 27: 1.3575142578125\n",
      "Epoch 28, Loss: 1.0419758355140687\n",
      "Test Loss after Epoch 28: 1.3297006099700928\n",
      "Epoch 29, Loss: 1.0169061305575902\n",
      "Test Loss after Epoch 29: 1.298392141532898\n",
      "Epoch 30, Loss: 0.9912519543647766\n",
      "Test Loss after Epoch 30: 1.331456522464752\n",
      "Epoch 31, Loss: 0.9760444882604811\n",
      "Test Loss after Epoch 31: 1.29418309841156\n",
      "Epoch 32, Loss: 0.9578995235443115\n",
      "Test Loss after Epoch 32: 1.2655479637145997\n",
      "Epoch 33, Loss: 0.9388069834603203\n",
      "Test Loss after Epoch 33: 1.291317744064331\n",
      "Epoch 34, Loss: 0.923989458656311\n",
      "Test Loss after Epoch 34: 1.2684272290229797\n",
      "Epoch 35, Loss: 0.9150944248729282\n",
      "Test Loss after Epoch 35: 1.2826864196777343\n",
      "Epoch 36, Loss: 0.9072399486965603\n",
      "Test Loss after Epoch 36: 1.2406029913902283\n",
      "Epoch 37, Loss: 0.8927543786048889\n",
      "Test Loss after Epoch 37: 1.2414135468482972\n",
      "Epoch 38, Loss: 0.8812295785162184\n",
      "Test Loss after Epoch 38: 1.2641603216171264\n",
      "Epoch 39, Loss: 0.8745203137291803\n",
      "Test Loss after Epoch 39: 1.2435719093322755\n",
      "Epoch 40, Loss: 0.8663462860955132\n",
      "Test Loss after Epoch 40: 1.2553888438224792\n",
      "Epoch 41, Loss: 0.8571766895400154\n",
      "Test Loss after Epoch 41: 1.2382619645118713\n",
      "Epoch 42, Loss: 0.8525380914582147\n",
      "Test Loss after Epoch 42: 1.2002012142181397\n",
      "Epoch 43, Loss: 0.8470056429545084\n",
      "Test Loss after Epoch 43: 1.2292127988815307\n",
      "Epoch 44, Loss: 0.8394393853611416\n",
      "Test Loss after Epoch 44: 1.2455897008895873\n",
      "Epoch 45, Loss: 0.8338963259802924\n",
      "Test Loss after Epoch 45: 1.2863036301612853\n",
      "Epoch 46, Loss: 0.8298220064163208\n",
      "Test Loss after Epoch 46: 1.2204194582939147\n",
      "Epoch 47, Loss: 0.8251721775054932\n",
      "Test Loss after Epoch 47: 1.2382452447891235\n",
      "Epoch 48, Loss: 0.8214196952078078\n",
      "Test Loss after Epoch 48: 1.2944308387756347\n",
      "Epoch 49, Loss: 0.8184556672943963\n",
      "Test Loss after Epoch 49: 1.2497328899383544\n",
      "Epoch 50, Loss: 0.8155141453213162\n",
      "Test Loss after Epoch 50: 1.2642775115013123\n",
      "Epoch 51, Loss: 0.8134323932859633\n",
      "Test Loss after Epoch 51: 1.2541974164962768\n",
      "Epoch 52, Loss: 0.8115207695431179\n",
      "Test Loss after Epoch 52: 1.2711667015075683\n",
      "Epoch 53, Loss: 0.807901838376787\n",
      "Test Loss after Epoch 53: 1.2438536584854125\n",
      "Epoch 54, Loss: 0.8050681627273559\n",
      "Test Loss after Epoch 54: 1.2462052751541137\n",
      "Epoch 55, Loss: 0.8028064496252272\n",
      "Test Loss after Epoch 55: 1.2681573222160338\n",
      "Epoch 56, Loss: 0.8001105762057834\n",
      "Test Loss after Epoch 56: 1.3056697377204896\n",
      "Epoch 57, Loss: 0.7987891761779785\n",
      "Test Loss after Epoch 57: 1.2742555421829225\n",
      "Epoch 58, Loss: 0.7969402118576897\n",
      "Test Loss after Epoch 58: 1.2194114924430848\n",
      "Epoch 59, Loss: 0.796007205359141\n",
      "Test Loss after Epoch 59: 1.2086738625526428\n",
      "Epoch 60, Loss: 0.7930911056306627\n",
      "Test Loss after Epoch 60: 1.22424142370224\n",
      "Epoch 61, Loss: 0.7934618052270678\n",
      "Test Loss after Epoch 61: 1.2261009757041932\n",
      "Epoch 62, Loss: 0.7922877513143751\n",
      "Test Loss after Epoch 62: 1.1901798754692077\n",
      "Epoch 63, Loss: 0.790446359263526\n",
      "Test Loss after Epoch 63: 1.2171851503372193\n",
      "Epoch 64, Loss: 0.7884514848391215\n",
      "Test Loss after Epoch 64: 1.2363411131858826\n",
      "Epoch 65, Loss: 0.7870889529016283\n",
      "Test Loss after Epoch 65: 1.2206547679901123\n",
      "Epoch 66, Loss: 0.7871719210200839\n",
      "Test Loss after Epoch 66: 1.1796146997451782\n",
      "Epoch 67, Loss: 0.7860349145041572\n",
      "Test Loss after Epoch 67: 1.208159745311737\n",
      "Epoch 68, Loss: 0.7852614633772108\n",
      "Test Loss after Epoch 68: 1.293713048362732\n",
      "Epoch 69, Loss: 0.7843291042433844\n",
      "Test Loss after Epoch 69: 1.240034122276306\n",
      "Epoch 70, Loss: 0.7834077228969998\n",
      "Test Loss after Epoch 70: 1.2066922243118285\n",
      "Epoch 71, Loss: 0.7829672449747721\n",
      "Test Loss after Epoch 71: 1.1636048535346986\n",
      "Epoch 72, Loss: 0.782566553624471\n",
      "Test Loss after Epoch 72: 1.176196816444397\n",
      "Epoch 73, Loss: 0.7817169707192315\n",
      "Test Loss after Epoch 73: 1.2008155738830566\n",
      "Epoch 74, Loss: 0.7803007602267795\n",
      "Test Loss after Epoch 74: 1.1724749416351319\n",
      "Epoch 75, Loss: 0.780197599389818\n",
      "Test Loss after Epoch 75: 1.2007503650665283\n",
      "Epoch 76, Loss: 0.7782716682540046\n",
      "Test Loss after Epoch 76: 1.1393117036819458\n",
      "Epoch 77, Loss: 0.7781828042666117\n",
      "Test Loss after Epoch 77: 1.146851539516449\n",
      "Epoch 78, Loss: 0.7788409591039022\n",
      "Test Loss after Epoch 78: 1.169536217880249\n",
      "Epoch 79, Loss: 0.7791077935642666\n",
      "Test Loss after Epoch 79: 1.1861137565612794\n",
      "Epoch 80, Loss: 0.7781307768079969\n",
      "Test Loss after Epoch 80: 1.1665914021492005\n",
      "Epoch 81, Loss: 0.7781399221526252\n",
      "Test Loss after Epoch 81: 1.117943309688568\n",
      "Epoch 82, Loss: 0.777779561413659\n",
      "Test Loss after Epoch 82: 1.1422246361732482\n",
      "Epoch 83, Loss: 0.776756579653422\n",
      "Test Loss after Epoch 83: 1.1761061389923095\n",
      "Epoch 84, Loss: 0.7760919722557068\n",
      "Test Loss after Epoch 84: 1.1532465393066407\n",
      "Epoch 85, Loss: 0.7746404076364305\n",
      "Test Loss after Epoch 85: 1.145421728038788\n",
      "Epoch 86, Loss: 0.7759588154474895\n",
      "Test Loss after Epoch 86: 1.1140497588157654\n",
      "Epoch 87, Loss: 0.7767813545333014\n",
      "Test Loss after Epoch 87: 1.1687454186439514\n",
      "Epoch 88, Loss: 0.7748235970920987\n",
      "Test Loss after Epoch 88: 1.1522142814636231\n",
      "Epoch 89, Loss: 0.7745523344039917\n",
      "Test Loss after Epoch 89: 1.1517997044563293\n",
      "Epoch 90, Loss: 0.7756285308096144\n",
      "Test Loss after Epoch 90: 1.106888384628296\n",
      "Epoch 91, Loss: 0.7755718642552694\n",
      "Test Loss after Epoch 91: 1.1532622531890868\n",
      "Epoch 92, Loss: 0.7749024427413941\n",
      "Test Loss after Epoch 92: 1.1639998071670532\n",
      "Epoch 93, Loss: 0.7733931867493523\n",
      "Test Loss after Epoch 93: 1.15270069065094\n",
      "Epoch 94, Loss: 0.7730256860203213\n",
      "Test Loss after Epoch 94: 1.1484672416687012\n",
      "Epoch 95, Loss: 0.7734613793479072\n",
      "Test Loss after Epoch 95: 1.1582398911476135\n",
      "Epoch 96, Loss: 0.7737577332178751\n",
      "Test Loss after Epoch 96: 1.1263162469863892\n",
      "Epoch 97, Loss: 0.7723867686695522\n",
      "Test Loss after Epoch 97: 1.1340018727302552\n",
      "Epoch 98, Loss: 0.7722454723570082\n",
      "Test Loss after Epoch 98: 1.1296306995391845\n",
      "Epoch 99, Loss: 0.7730283977826437\n",
      "Test Loss after Epoch 99: 1.1560393213272095\n",
      "Epoch 100, Loss: 0.7719896220101251\n",
      "Test Loss after Epoch 100: 1.1647743041992187\n",
      "Epoch 101, Loss: 0.7707379897647434\n",
      "Test Loss after Epoch 101: 1.1543237752914428\n",
      "Epoch 102, Loss: 0.7705545514848497\n",
      "Test Loss after Epoch 102: 1.1285272737503051\n",
      "Epoch 103, Loss: 0.7708440858840943\n",
      "Test Loss after Epoch 103: 1.1313377054214477\n",
      "Epoch 104, Loss: 0.7676842438485887\n",
      "Test Loss after Epoch 104: 1.1645803857803345\n",
      "Epoch 105, Loss: 0.7691073687023586\n",
      "Test Loss after Epoch 105: 1.1329613214492797\n",
      "Epoch 106, Loss: 0.7691994863828023\n",
      "Test Loss after Epoch 106: 1.111848676586151\n",
      "Epoch 107, Loss: 0.7696509695053101\n",
      "Test Loss after Epoch 107: 1.141548009300232\n",
      "Epoch 108, Loss: 0.7689400344106886\n",
      "Test Loss after Epoch 108: 1.1582532517433166\n",
      "Epoch 109, Loss: 0.7684184292475382\n",
      "Test Loss after Epoch 109: 1.1685564155578614\n",
      "Epoch 110, Loss: 0.7684339554574755\n",
      "Test Loss after Epoch 110: 1.1749844942092895\n",
      "Rolling back to best model from epoch 90\n",
      "Best test loss: 1.106888384628296\n",
      "Epoch 111, Loss: 0.7716101671748691\n",
      "Test Loss after Epoch 111: 1.161722072315216\n",
      "Epoch 112, Loss: 0.7724470558272468\n",
      "Test Loss after Epoch 112: 1.1757365970611573\n",
      "Epoch 113, Loss: 0.7712960793071323\n",
      "Test Loss after Epoch 113: 1.1413649273872375\n",
      "Epoch 114, Loss: 0.7750364704344007\n",
      "Test Loss after Epoch 114: 1.1196573944091797\n",
      "Epoch 115, Loss: 0.7730677415317959\n",
      "Test Loss after Epoch 115: 1.1377207300186156\n",
      "Epoch 116, Loss: 0.7757481393496195\n",
      "Test Loss after Epoch 116: 1.1322822463989257\n",
      "Epoch 117, Loss: 0.7752628646320767\n",
      "Test Loss after Epoch 117: 1.1316868268966676\n",
      "Epoch 118, Loss: 0.7732414841016133\n",
      "Test Loss after Epoch 118: 1.1134179772377013\n",
      "Epoch 119, Loss: 0.775106151760949\n",
      "Test Loss after Epoch 119: 1.1483100269317628\n",
      "Epoch 120, Loss: 0.7748701736132304\n",
      "Test Loss after Epoch 120: 1.1542525693893433\n",
      "Epoch 121, Loss: 0.7756718093554179\n",
      "Test Loss after Epoch 121: 1.1282564559936523\n",
      "Epoch 122, Loss: 0.7744874355634054\n",
      "Test Loss after Epoch 122: 1.1247290309906006\n",
      "Epoch 123, Loss: 0.7735942767884996\n",
      "Test Loss after Epoch 123: 1.140553404045105\n",
      "Epoch 124, Loss: 0.7724663165092468\n",
      "Test Loss after Epoch 124: 1.1400609028816222\n",
      "Epoch 125, Loss: 0.7733912573072645\n",
      "Test Loss after Epoch 125: 1.1251402868270874\n",
      "Epoch 126, Loss: 0.7734250299241807\n",
      "Test Loss after Epoch 126: 1.0915335302352904\n",
      "Epoch 127, Loss: 0.772458505958981\n",
      "Test Loss after Epoch 127: 1.096284398651123\n",
      "Epoch 128, Loss: 0.7744726473914252\n",
      "Test Loss after Epoch 128: 1.122646618938446\n",
      "Epoch 129, Loss: 0.773276918358273\n",
      "Test Loss after Epoch 129: 1.1157954421043397\n",
      "Epoch 130, Loss: 0.7731385214805603\n",
      "Test Loss after Epoch 130: 1.097030751132965\n",
      "Epoch 131, Loss: 0.7723114571677314\n",
      "Test Loss after Epoch 131: 1.175963024997711\n",
      "Epoch 132, Loss: 0.7736600760353937\n",
      "Test Loss after Epoch 132: 1.113399720287323\n",
      "Epoch 133, Loss: 0.7738544731246101\n",
      "Test Loss after Epoch 133: 1.1297357244491577\n",
      "Epoch 134, Loss: 0.773515521812439\n",
      "Test Loss after Epoch 134: 1.0991528274536133\n",
      "Epoch 135, Loss: 0.772966324573093\n",
      "Test Loss after Epoch 135: 1.1278677841186524\n",
      "Epoch 136, Loss: 0.77109215309355\n",
      "Test Loss after Epoch 136: 1.115515932369232\n",
      "Epoch 137, Loss: 0.7729347386572096\n",
      "Test Loss after Epoch 137: 1.1113435170173644\n",
      "Epoch 138, Loss: 0.7717732545852661\n",
      "Test Loss after Epoch 138: 1.085611503124237\n",
      "Epoch 139, Loss: 0.7704064673317803\n",
      "Test Loss after Epoch 139: 1.082024335002899\n",
      "Epoch 140, Loss: 0.7717784145249261\n",
      "Test Loss after Epoch 140: 1.1100955159187318\n",
      "Epoch 141, Loss: 0.7692658823861016\n",
      "Test Loss after Epoch 141: 1.1219932782173156\n",
      "Epoch 142, Loss: 0.7699481056955125\n",
      "Test Loss after Epoch 142: 1.0964897988319398\n",
      "Epoch 143, Loss: 0.7706597577412924\n",
      "Test Loss after Epoch 143: 1.0892996538162232\n",
      "Epoch 144, Loss: 0.7706157137446933\n",
      "Test Loss after Epoch 144: 1.1252498854637145\n",
      "Epoch 145, Loss: 0.7700215383635627\n",
      "Test Loss after Epoch 145: 1.102137554168701\n",
      "Epoch 146, Loss: 0.7687221761491564\n",
      "Test Loss after Epoch 146: 1.076906524658203\n",
      "Epoch 147, Loss: 0.7692178508758545\n",
      "Test Loss after Epoch 147: 1.0767501259803771\n",
      "Epoch 148, Loss: 0.7693200775676303\n",
      "Test Loss after Epoch 148: 1.0924967267036438\n",
      "Epoch 149, Loss: 0.7688511809666951\n",
      "Test Loss after Epoch 149: 1.1080765097618104\n",
      "Epoch 150, Loss: 0.7677305841975742\n",
      "Test Loss after Epoch 150: 1.1049716515541077\n",
      "Epoch 151, Loss: 0.767175560675727\n",
      "Test Loss after Epoch 151: 1.0988966418266297\n",
      "Epoch 152, Loss: 0.7668886199527316\n",
      "Test Loss after Epoch 152: 1.1048013608932494\n",
      "Epoch 153, Loss: 0.7669360315110948\n",
      "Test Loss after Epoch 153: 1.107856173801422\n",
      "Epoch 154, Loss: 0.767672751543257\n",
      "Test Loss after Epoch 154: 1.117255485725403\n",
      "Epoch 155, Loss: 0.7665311481051975\n",
      "Test Loss after Epoch 155: 1.102784062767029\n",
      "Epoch 156, Loss: 0.7672123571395874\n",
      "Test Loss after Epoch 156: 1.141666470718384\n",
      "Epoch 157, Loss: 0.7662743087874518\n",
      "Test Loss after Epoch 157: 1.12149050989151\n",
      "Epoch 158, Loss: 0.7655222544352214\n",
      "Test Loss after Epoch 158: 1.1183430837631225\n",
      "Epoch 159, Loss: 0.7655136594560411\n",
      "Test Loss after Epoch 159: 1.154387010383606\n",
      "Epoch 160, Loss: 0.764448063384162\n",
      "Test Loss after Epoch 160: 1.1517377598762513\n",
      "Epoch 161, Loss: 0.7652452308336893\n",
      "Test Loss after Epoch 161: 1.1437152535438537\n",
      "Epoch 162, Loss: 0.7651685402552286\n",
      "Test Loss after Epoch 162: 1.108116325855255\n",
      "Epoch 163, Loss: 0.764127031633589\n",
      "Test Loss after Epoch 163: 1.0916821478843688\n",
      "Epoch 164, Loss: 0.763434697204166\n",
      "Test Loss after Epoch 164: 1.0983454982757568\n",
      "Epoch 165, Loss: 0.7633859972423978\n",
      "Test Loss after Epoch 165: 1.100750462436676\n",
      "Epoch 166, Loss: 0.7643111088434855\n",
      "Test Loss after Epoch 166: 1.1006678127288818\n",
      "Epoch 167, Loss: 0.7628020363065932\n",
      "Test Loss after Epoch 167: 1.1065826661109925\n",
      "Epoch 168, Loss: 0.76306246670617\n",
      "Test Loss after Epoch 168: 1.0959343092918397\n",
      "Epoch 169, Loss: 0.7627141844855414\n",
      "Test Loss after Epoch 169: 1.1132245522499085\n",
      "Epoch 170, Loss: 0.7624409923553467\n",
      "Test Loss after Epoch 170: 1.0989274622917176\n",
      "Epoch 171, Loss: 0.7621680848969353\n",
      "Test Loss after Epoch 171: 1.1011911153793335\n",
      "Epoch 172, Loss: 0.7617610843764411\n",
      "Test Loss after Epoch 172: 1.113904253578186\n",
      "Epoch 173, Loss: 0.7622956241819594\n",
      "Test Loss after Epoch 173: 1.1114085550308228\n",
      "Epoch 174, Loss: 0.7599737040519714\n",
      "Test Loss after Epoch 174: 1.0829112584114076\n",
      "Epoch 175, Loss: 0.7612926474677192\n",
      "Test Loss after Epoch 175: 1.0959852465629578\n",
      "Rolling back to best model from epoch 147\n",
      "Best test loss: 1.0767501259803771\n",
      "Epoch 176, Loss: 0.7658759728325738\n",
      "Test Loss after Epoch 176: 1.0831046644210816\n",
      "Epoch 177, Loss: 0.7635918655501471\n",
      "Test Loss after Epoch 177: 1.1048194849967956\n",
      "Epoch 178, Loss: 0.7630895822207133\n",
      "Test Loss after Epoch 178: 1.0916193303108215\n",
      "Epoch 179, Loss: 0.7640389710532295\n",
      "Test Loss after Epoch 179: 1.1040678945541382\n",
      "Epoch 180, Loss: 0.7652291795624627\n",
      "Test Loss after Epoch 180: 1.0969023262023925\n",
      "Epoch 181, Loss: 0.7651623639318678\n",
      "Test Loss after Epoch 181: 1.1002129831314087\n",
      "Epoch 182, Loss: 0.7646270755449931\n",
      "Test Loss after Epoch 182: 1.0971951206207275\n",
      "Epoch 183, Loss: 0.7647938510364957\n",
      "Test Loss after Epoch 183: 1.1193873737335205\n",
      "Epoch 184, Loss: 0.7638177768601312\n",
      "Test Loss after Epoch 184: 1.1017451436042787\n",
      "Epoch 185, Loss: 0.7643755184703404\n",
      "Test Loss after Epoch 185: 1.0876411180496215\n",
      "Epoch 186, Loss: 0.7622120993720161\n",
      "Test Loss after Epoch 186: 1.0812359649658203\n",
      "Epoch 187, Loss: 0.7624480437808566\n",
      "Test Loss after Epoch 187: 1.0892448070526124\n",
      "Epoch 188, Loss: 0.7626109814749824\n",
      "Test Loss after Epoch 188: 1.0852554356575013\n",
      "Epoch 189, Loss: 0.7627890658166674\n",
      "Test Loss after Epoch 189: 1.1019568634986878\n",
      "Epoch 190, Loss: 0.7622654679510329\n",
      "Test Loss after Epoch 190: 1.1087918519973754\n",
      "Epoch 191, Loss: 0.7621561392254299\n",
      "Test Loss after Epoch 191: 1.083349728870392\n",
      "Epoch 192, Loss: 0.7630325107998318\n",
      "Test Loss after Epoch 192: 1.07162614774704\n",
      "Epoch 193, Loss: 0.7619503763516744\n",
      "Test Loss after Epoch 193: 1.1044620131492615\n",
      "Epoch 194, Loss: 0.7615631151835124\n",
      "Test Loss after Epoch 194: 1.1332283435821533\n",
      "Epoch 195, Loss: 0.7608475536452399\n",
      "Test Loss after Epoch 195: 1.1154161833763123\n",
      "Epoch 196, Loss: 0.760295364189148\n",
      "Test Loss after Epoch 196: 1.0822389585494996\n",
      "Epoch 197, Loss: 0.7608511257489522\n",
      "Test Loss after Epoch 197: 1.0974653120994569\n",
      "Epoch 198, Loss: 0.7598863140318128\n",
      "Test Loss after Epoch 198: 1.098992652130127\n",
      "Epoch 199, Loss: 0.7596783328268263\n",
      "Test Loss after Epoch 199: 1.127757483291626\n",
      "Epoch 200, Loss: 0.7602717258877224\n",
      "Test Loss after Epoch 200: 1.1035025933265685\n",
      "Epoch 201, Loss: 0.7601597317907546\n",
      "Test Loss after Epoch 201: 1.1225891152381897\n",
      "Epoch 202, Loss: 0.7599867736498515\n",
      "Test Loss after Epoch 202: 1.1097589200019837\n",
      "Epoch 203, Loss: 0.7600297063721551\n",
      "Test Loss after Epoch 203: 1.0737226036071776\n",
      "Epoch 204, Loss: 0.7590255656666226\n",
      "Test Loss after Epoch 204: 1.1130451141357423\n",
      "Epoch 205, Loss: 0.759965490743849\n",
      "Test Loss after Epoch 205: 1.1013874250411988\n",
      "Epoch 206, Loss: 0.7590003078990513\n",
      "Test Loss after Epoch 206: 1.113057810497284\n",
      "Epoch 207, Loss: 0.7586710375361972\n",
      "Test Loss after Epoch 207: 1.0920366022109986\n",
      "Epoch 208, Loss: 0.7587362590048048\n",
      "Test Loss after Epoch 208: 1.0996233709335328\n",
      "Epoch 209, Loss: 0.759692870065901\n",
      "Test Loss after Epoch 209: 1.1301389904975891\n",
      "Epoch 210, Loss: 0.7577008191638522\n",
      "Test Loss after Epoch 210: 1.106889466381073\n",
      "Epoch 211, Loss: 0.7577518302387661\n",
      "Test Loss after Epoch 211: 1.0842362347602845\n",
      "Epoch 212, Loss: 0.7582743674384224\n",
      "Test Loss after Epoch 212: 1.0713753898620606\n",
      "Epoch 213, Loss: 0.7590752756860522\n",
      "Test Loss after Epoch 213: 1.1177388014793397\n",
      "Epoch 214, Loss: 0.7580111219512091\n",
      "Test Loss after Epoch 214: 1.084684384059906\n",
      "Epoch 215, Loss: 0.7566683567788866\n",
      "Test Loss after Epoch 215: 1.067800253868103\n",
      "Epoch 216, Loss: 0.7589965207099915\n",
      "Test Loss after Epoch 216: 1.0785076516151428\n",
      "Epoch 217, Loss: 0.7571066226959229\n",
      "Test Loss after Epoch 217: 1.062637370491028\n",
      "Epoch 218, Loss: 0.7571140895313687\n",
      "Test Loss after Epoch 218: 1.0984679830551147\n",
      "Epoch 219, Loss: 0.7576466028955248\n",
      "Test Loss after Epoch 219: 1.0972688885688782\n",
      "Epoch 220, Loss: 0.7573836231019762\n",
      "Test Loss after Epoch 220: 1.0594447954177857\n",
      "Epoch 221, Loss: 0.7563852833747864\n",
      "Test Loss after Epoch 221: 1.1040495616912842\n",
      "Epoch 222, Loss: 0.7573275722821553\n",
      "Test Loss after Epoch 222: 1.0852281261444092\n",
      "Epoch 223, Loss: 0.7560236426671346\n",
      "Test Loss after Epoch 223: 1.087278199481964\n",
      "Epoch 224, Loss: 0.756823646291097\n",
      "Test Loss after Epoch 224: 1.0829992599487304\n",
      "Epoch 225, Loss: 0.7566740438885159\n",
      "Test Loss after Epoch 225: 1.0624653907775878\n",
      "Epoch 226, Loss: 0.7570312711291843\n",
      "Test Loss after Epoch 226: 1.058602869129181\n",
      "Epoch 227, Loss: 0.7563652846548292\n",
      "Test Loss after Epoch 227: 1.0584572500228882\n",
      "Epoch 228, Loss: 0.7559379311561585\n",
      "Test Loss after Epoch 228: 1.0792976537704468\n",
      "Epoch 229, Loss: 0.7559687073389689\n",
      "Test Loss after Epoch 229: 1.0838260416984558\n",
      "Epoch 230, Loss: 0.7556569453451368\n",
      "Test Loss after Epoch 230: 1.0788498650550842\n",
      "Epoch 231, Loss: 0.7551057090123494\n",
      "Test Loss after Epoch 231: 1.0693910187721252\n",
      "Epoch 232, Loss: 0.7554600431442261\n",
      "Test Loss after Epoch 232: 1.086267709541321\n",
      "Epoch 233, Loss: 0.754799766974979\n",
      "Test Loss after Epoch 233: 1.0932339507102966\n",
      "Epoch 234, Loss: 0.7559503027809991\n",
      "Test Loss after Epoch 234: 1.0644590231895448\n",
      "Epoch 235, Loss: 0.7546236626836988\n",
      "Test Loss after Epoch 235: 1.0830488118171693\n",
      "Epoch 236, Loss: 0.7548849724027845\n",
      "Test Loss after Epoch 236: 1.0889209454536437\n",
      "Epoch 237, Loss: 0.7544922539710999\n",
      "Test Loss after Epoch 237: 1.0409574506759645\n",
      "Epoch 238, Loss: 0.7555371913062202\n",
      "Test Loss after Epoch 238: 1.066406197166443\n",
      "Epoch 239, Loss: 0.7549062974823846\n",
      "Test Loss after Epoch 239: 1.0660437910079956\n",
      "Epoch 240, Loss: 0.7539385336134169\n",
      "Test Loss after Epoch 240: 1.0790787791252137\n",
      "Epoch 241, Loss: 0.755165152698093\n",
      "Test Loss after Epoch 241: 1.0672176445961\n",
      "Epoch 242, Loss: 0.7546505188306173\n",
      "Test Loss after Epoch 242: 1.0914348440170287\n",
      "Epoch 243, Loss: 0.753802255958981\n",
      "Test Loss after Epoch 243: 1.0719813611984252\n",
      "Epoch 244, Loss: 0.753416816690233\n",
      "Test Loss after Epoch 244: 1.0722502621650696\n",
      "Epoch 245, Loss: 0.7536798829396566\n",
      "Test Loss after Epoch 245: 1.0846563533782958\n",
      "Epoch 246, Loss: 0.7544511178440518\n",
      "Test Loss after Epoch 246: 1.0784353639602662\n",
      "Epoch 247, Loss: 0.7549552821159363\n",
      "Test Loss after Epoch 247: 1.09538568983078\n",
      "Epoch 248, Loss: 0.7540175078921848\n",
      "Test Loss after Epoch 248: 1.0717182295799255\n",
      "Epoch 249, Loss: 0.7526963419278463\n",
      "Test Loss after Epoch 249: 1.0736541115760803\n",
      "Epoch 250, Loss: 0.7529656540870666\n",
      "Test Loss after Epoch 250: 1.0854659219741822\n",
      "Epoch 251, Loss: 0.7530812645700243\n",
      "Test Loss after Epoch 251: 1.0805738753318788\n",
      "Epoch 252, Loss: 0.7532560045454237\n",
      "Test Loss after Epoch 252: 1.091575794506073\n",
      "Epoch 253, Loss: 0.7533429053518507\n",
      "Test Loss after Epoch 253: 1.095031861782074\n",
      "Epoch 254, Loss: 0.7529216037644281\n",
      "Test Loss after Epoch 254: 1.0877176188468933\n",
      "Epoch 255, Loss: 0.7527621158705817\n",
      "Test Loss after Epoch 255: 1.0871369263648987\n",
      "Epoch 256, Loss: 0.7531909243583679\n",
      "Test Loss after Epoch 256: 1.0603367670059205\n",
      "Epoch 257, Loss: 0.7532745841238234\n",
      "Test Loss after Epoch 257: 1.0739432242393494\n",
      "Epoch 258, Loss: 0.7533751597192553\n",
      "Test Loss after Epoch 258: 1.0801854122161865\n",
      "Epoch 259, Loss: 0.7544511220614115\n",
      "Test Loss after Epoch 259: 1.0846923928260803\n",
      "Epoch 260, Loss: 0.7534101385010613\n",
      "Test Loss after Epoch 260: 1.1163718645095826\n",
      "Epoch 261, Loss: 0.7530278264045716\n",
      "Test Loss after Epoch 261: 1.1069969394683838\n",
      "Epoch 262, Loss: 0.7528028197182549\n",
      "Test Loss after Epoch 262: 1.0801162559509276\n",
      "Epoch 263, Loss: 0.7527376193364461\n",
      "Test Loss after Epoch 263: 1.1103910431861876\n",
      "Epoch 264, Loss: 0.7521496202786764\n",
      "Test Loss after Epoch 264: 1.079522013092041\n",
      "Epoch 265, Loss: 0.7522340201907688\n",
      "Test Loss after Epoch 265: 1.1078874876976013\n",
      "Epoch 266, Loss: 0.7518416496912639\n",
      "Test Loss after Epoch 266: 1.0956987718582154\n",
      "Epoch 267, Loss: 0.7522590206993951\n",
      "Test Loss after Epoch 267: 1.066491617679596\n",
      "Epoch 268, Loss: 0.7521607709778679\n",
      "Test Loss after Epoch 268: 1.0899306003570557\n",
      "Epoch 269, Loss: 0.7513314588122898\n",
      "Test Loss after Epoch 269: 1.0584779551506043\n",
      "Epoch 270, Loss: 0.7514484420458476\n",
      "Test Loss after Epoch 270: 1.0719715874671936\n",
      "Epoch 271, Loss: 0.7522960090531243\n",
      "Test Loss after Epoch 271: 1.0717070135116578\n",
      "Epoch 272, Loss: 0.751463826974233\n",
      "Test Loss after Epoch 272: 1.0839261512756349\n",
      "Epoch 273, Loss: 0.7517191796726651\n",
      "Test Loss after Epoch 273: 1.099760981273651\n",
      "Epoch 274, Loss: 0.7517269754621718\n",
      "Test Loss after Epoch 274: 1.080150644302368\n",
      "Epoch 275, Loss: 0.7510773902999031\n",
      "Test Loss after Epoch 275: 1.0894823941230773\n",
      "Epoch 276, Loss: 0.7511866093741523\n",
      "Test Loss after Epoch 276: 1.076820041370392\n",
      "Rolling back to best model from epoch 237\n",
      "Best test loss: 1.0409574506759645\n",
      "Epoch 277, Loss: 0.7528330160140991\n",
      "Test Loss after Epoch 277: 1.0584979011535645\n",
      "Epoch 278, Loss: 0.7519887111345926\n",
      "Test Loss after Epoch 278: 1.0755046869277953\n",
      "Epoch 279, Loss: 0.7523954670482211\n",
      "Test Loss after Epoch 279: 1.0860843998908996\n",
      "Epoch 280, Loss: 0.7531177257537842\n",
      "Test Loss after Epoch 280: 1.057842928314209\n",
      "Epoch 281, Loss: 0.7531138373480902\n",
      "Test Loss after Epoch 281: 1.0527920537948607\n",
      "Epoch 282, Loss: 0.7533038694699605\n",
      "Test Loss after Epoch 282: 1.0621992985725404\n",
      "Epoch 283, Loss: 0.7529145228173998\n",
      "Test Loss after Epoch 283: 1.0530856073379518\n",
      "Epoch 284, Loss: 0.7534920438448588\n",
      "Test Loss after Epoch 284: 1.0787820941925048\n",
      "Epoch 285, Loss: 0.7531539570914374\n",
      "Test Loss after Epoch 285: 1.06084651927948\n",
      "Epoch 286, Loss: 0.7530512685987685\n",
      "Test Loss after Epoch 286: 1.086535509777069\n",
      "Epoch 287, Loss: 0.7527247142155965\n",
      "Test Loss after Epoch 287: 1.0478428071022035\n",
      "Epoch 288, Loss: 0.7518163940323723\n",
      "Test Loss after Epoch 288: 1.0788555205345154\n",
      "Epoch 289, Loss: 0.7529175150341458\n",
      "Test Loss after Epoch 289: 1.064912714099884\n",
      "Epoch 290, Loss: 0.7519547529538473\n",
      "Test Loss after Epoch 290: 1.0544559603691102\n",
      "Epoch 291, Loss: 0.752166464413537\n",
      "Test Loss after Epoch 291: 1.0591449221611022\n",
      "Epoch 292, Loss: 0.7515529144816928\n",
      "Test Loss after Epoch 292: 1.0669719348907472\n",
      "Epoch 293, Loss: 0.7533500706460741\n",
      "Test Loss after Epoch 293: 1.061727597427368\n",
      "Epoch 294, Loss: 0.7523208155632019\n",
      "Test Loss after Epoch 294: 1.0470946327209472\n",
      "Epoch 295, Loss: 0.7521556089295282\n",
      "Test Loss after Epoch 295: 1.074553237915039\n",
      "Epoch 296, Loss: 0.7518035121070014\n",
      "Test Loss after Epoch 296: 1.0662515824317933\n",
      "Epoch 297, Loss: 0.751357562160492\n",
      "Test Loss after Epoch 297: 1.0709984844207763\n",
      "Epoch 298, Loss: 0.7520738786061605\n",
      "Test Loss after Epoch 298: 1.0658882843017579\n",
      "Epoch 299, Loss: 0.7517565707842508\n",
      "Test Loss after Epoch 299: 1.070336318874359\n",
      "Epoch 300, Loss: 0.7515483555157979\n",
      "Test Loss after Epoch 300: 1.0542795018196105\n",
      "Epoch 301, Loss: 0.7518658235973782\n",
      "Test Loss after Epoch 301: 1.0623260913848878\n",
      "Epoch 302, Loss: 0.751223332034217\n",
      "Test Loss after Epoch 302: 1.0620996444702149\n",
      "Epoch 303, Loss: 0.7517790197902255\n",
      "Test Loss after Epoch 303: 1.0739080198287965\n",
      "Epoch 304, Loss: 0.750844187174903\n",
      "Test Loss after Epoch 304: 1.0686815217971801\n",
      "Epoch 305, Loss: 0.751068225977156\n",
      "Test Loss after Epoch 305: 1.0647376789093017\n",
      "Epoch 306, Loss: 0.7510246648682488\n",
      "Test Loss after Epoch 306: 1.0863001051902772\n",
      "Epoch 307, Loss: 0.7511243379486932\n",
      "Test Loss after Epoch 307: 1.0700079404830933\n",
      "Epoch 308, Loss: 0.751305039162106\n",
      "Test Loss after Epoch 308: 1.0744146027565002\n",
      "Epoch 309, Loss: 0.7517180726157294\n",
      "Test Loss after Epoch 309: 1.0587337042808533\n",
      "Epoch 310, Loss: 0.750790495967865\n",
      "Test Loss after Epoch 310: 1.0844767444610595\n",
      "Epoch 311, Loss: 0.7507786557303535\n",
      "Test Loss after Epoch 311: 1.0789744374275207\n",
      "Epoch 312, Loss: 0.7513187856462267\n",
      "Test Loss after Epoch 312: 1.054390371990204\n",
      "Epoch 313, Loss: 0.7510147603988647\n",
      "Test Loss after Epoch 313: 1.0473483701705932\n",
      "Epoch 314, Loss: 0.7510728355513678\n",
      "Test Loss after Epoch 314: 1.0717327668190002\n",
      "Epoch 315, Loss: 0.7504305527475145\n",
      "Test Loss after Epoch 315: 1.0690531513214112\n",
      "Epoch 316, Loss: 0.7508414269129435\n",
      "Test Loss after Epoch 316: 1.0423888563156127\n",
      "Epoch 317, Loss: 0.7510049466027154\n",
      "Test Loss after Epoch 317: 1.096068465614319\n",
      "Epoch 318, Loss: 0.7510397524727715\n",
      "Test Loss after Epoch 318: 1.0724750010490418\n",
      "Epoch 319, Loss: 0.7500520067532858\n",
      "Test Loss after Epoch 319: 1.075154028224945\n",
      "Epoch 320, Loss: 0.7503119373957317\n",
      "Test Loss after Epoch 320: 1.0769189932823182\n",
      "Epoch 321, Loss: 0.7506108786159091\n",
      "Test Loss after Epoch 321: 1.054098317718506\n",
      "Epoch 322, Loss: 0.7504283238728842\n",
      "Test Loss after Epoch 322: 1.0538740343093873\n",
      "Epoch 323, Loss: 0.7506922588984172\n",
      "Test Loss after Epoch 323: 1.062906840801239\n",
      "Epoch 324, Loss: 0.7500101789792378\n",
      "Test Loss after Epoch 324: 1.0359860375404357\n",
      "Epoch 325, Loss: 0.7499417080349392\n",
      "Test Loss after Epoch 325: 1.0736196093559265\n",
      "Epoch 326, Loss: 0.7504811779340108\n",
      "Test Loss after Epoch 326: 1.034273471069336\n",
      "Epoch 327, Loss: 0.7496544446839226\n",
      "Test Loss after Epoch 327: 1.0598469986915589\n",
      "Epoch 328, Loss: 0.7496267050001356\n",
      "Test Loss after Epoch 328: 1.0368311331748963\n",
      "Epoch 329, Loss: 0.7502073447651333\n",
      "Test Loss after Epoch 329: 1.029150119113922\n",
      "Epoch 330, Loss: 0.7499313607427809\n",
      "Test Loss after Epoch 330: 1.0320470720291137\n",
      "Epoch 331, Loss: 0.750161466217041\n",
      "Test Loss after Epoch 331: 1.0565330585479735\n",
      "Epoch 332, Loss: 0.7497976556777954\n",
      "Test Loss after Epoch 332: 1.034527227497101\n",
      "Epoch 333, Loss: 0.7502627208073934\n",
      "Test Loss after Epoch 333: 1.0560398362159729\n",
      "Epoch 334, Loss: 0.7501937458886041\n",
      "Test Loss after Epoch 334: 1.0342209586143494\n",
      "Epoch 335, Loss: 0.7493868363380433\n",
      "Test Loss after Epoch 335: 1.084037951374054\n",
      "Epoch 336, Loss: 0.7492735812505086\n",
      "Test Loss after Epoch 336: 1.103026223564148\n",
      "Epoch 337, Loss: 0.7491516721407573\n",
      "Test Loss after Epoch 337: 1.075889202594757\n",
      "Epoch 338, Loss: 0.7495063921610514\n",
      "Test Loss after Epoch 338: 1.0733848690032959\n",
      "Epoch 339, Loss: 0.7493771806928846\n",
      "Test Loss after Epoch 339: 1.0751297552108765\n",
      "Epoch 340, Loss: 0.749024430179596\n",
      "Test Loss after Epoch 340: 1.0738061073303222\n",
      "Epoch 341, Loss: 0.7499096350140042\n",
      "Test Loss after Epoch 341: 1.0913971344947815\n",
      "Epoch 342, Loss: 0.7499638301955329\n",
      "Test Loss after Epoch 342: 1.0756316125869751\n",
      "Epoch 343, Loss: 0.7492993493186103\n",
      "Test Loss after Epoch 343: 1.1031942178726197\n",
      "Epoch 344, Loss: 0.7497662363476223\n",
      "Test Loss after Epoch 344: 1.0927052582740784\n",
      "Epoch 345, Loss: 0.7491320129182604\n",
      "Test Loss after Epoch 345: 1.085549900150299\n",
      "Epoch 346, Loss: 0.7498613230493334\n",
      "Test Loss after Epoch 346: 1.0670462721824645\n",
      "Epoch 347, Loss: 0.7494402787208557\n",
      "Test Loss after Epoch 347: 1.0493688376426697\n",
      "Epoch 348, Loss: 0.7494523719999525\n",
      "Test Loss after Epoch 348: 1.0315851635932922\n",
      "Epoch 349, Loss: 0.7491079992718167\n",
      "Test Loss after Epoch 349: 1.0822714301109313\n",
      "Epoch 350, Loss: 0.7490785389052497\n",
      "Test Loss after Epoch 350: 1.0490418256759644\n",
      "Epoch 351, Loss: 0.7490865582148234\n",
      "Test Loss after Epoch 351: 1.0611753341674806\n",
      "Epoch 352, Loss: 0.7493473829375373\n",
      "Test Loss after Epoch 352: 1.0535852252960205\n",
      "Epoch 353, Loss: 0.7499725074132284\n",
      "Test Loss after Epoch 353: 1.072458281326294\n",
      "Epoch 354, Loss: 0.7504096557299296\n",
      "Test Loss after Epoch 354: 1.0567930018424987\n",
      "Epoch 355, Loss: 0.7493784796396892\n",
      "Test Loss after Epoch 355: 1.081714896774292\n",
      "Epoch 356, Loss: 0.7495873633596633\n",
      "Test Loss after Epoch 356: 1.0576787296295167\n",
      "Epoch 357, Loss: 0.7492137158287896\n",
      "Test Loss after Epoch 357: 1.0363209064483643\n",
      "Epoch 358, Loss: 0.748789729891883\n",
      "Test Loss after Epoch 358: 1.0729453737258912\n",
      "Epoch 359, Loss: 0.7493402028825548\n",
      "Test Loss after Epoch 359: 1.055119473361969\n",
      "Epoch 360, Loss: 0.748581050713857\n",
      "Test Loss after Epoch 360: 1.0420801582336425\n",
      "Epoch 361, Loss: 0.7489858603159587\n",
      "Test Loss after Epoch 361: 1.0410368930816651\n",
      "Epoch 362, Loss: 0.7484058510462444\n",
      "Test Loss after Epoch 362: 1.0432160745620727\n",
      "Epoch 363, Loss: 0.748742573483785\n",
      "Test Loss after Epoch 363: 1.0504253874778748\n",
      "Epoch 364, Loss: 0.7483691811031765\n",
      "Test Loss after Epoch 364: 1.0442103951454162\n",
      "Epoch 365, Loss: 0.7481887540181478\n",
      "Test Loss after Epoch 365: 1.043368354988098\n",
      "Epoch 366, Loss: 0.7484791614108616\n",
      "Test Loss after Epoch 366: 1.0336330164909362\n",
      "Epoch 367, Loss: 0.7482246184455024\n",
      "Test Loss after Epoch 367: 1.069177287197113\n",
      "Epoch 368, Loss: 0.7480785522990756\n",
      "Test Loss after Epoch 368: 1.0608738667488098\n",
      "Epoch 369, Loss: 0.748182609324985\n",
      "Test Loss after Epoch 369: 1.0500838762283324\n",
      "Epoch 370, Loss: 0.7484386368009779\n",
      "Test Loss after Epoch 370: 1.030456990337372\n",
      "Epoch 371, Loss: 0.7483417619387309\n",
      "Test Loss after Epoch 371: 1.0477620741844178\n",
      "Epoch 372, Loss: 0.7477439364327325\n",
      "Test Loss after Epoch 372: 1.0660872319221497\n",
      "Epoch 373, Loss: 0.7481576847288344\n",
      "Test Loss after Epoch 373: 1.085757995223999\n",
      "Epoch 374, Loss: 0.7484877374119229\n",
      "Test Loss after Epoch 374: 1.1004784680366515\n",
      "Epoch 375, Loss: 0.7481550488789876\n",
      "Test Loss after Epoch 375: 1.0675821763038635\n",
      "Epoch 376, Loss: 0.7476548700120714\n",
      "Test Loss after Epoch 376: 1.0855666388511658\n",
      "Epoch 377, Loss: 0.7493435071839226\n",
      "Test Loss after Epoch 377: 1.0675816259384154\n",
      "Epoch 378, Loss: 0.7477619361241659\n",
      "Test Loss after Epoch 378: 1.0674038199424745\n",
      "Epoch 379, Loss: 0.7480269402503967\n",
      "Test Loss after Epoch 379: 1.0600089579582215\n",
      "Epoch 380, Loss: 0.7476321700731913\n",
      "Test Loss after Epoch 380: 1.076594884967804\n",
      "Epoch 381, Loss: 0.7484357078446282\n",
      "Test Loss after Epoch 381: 1.0706219964981079\n",
      "Epoch 382, Loss: 0.7480683420711094\n",
      "Test Loss after Epoch 382: 1.0631777724266052\n",
      "Epoch 383, Loss: 0.7478497141414219\n",
      "Test Loss after Epoch 383: 1.0494357924461364\n",
      "Epoch 384, Loss: 0.7479263583289253\n",
      "Test Loss after Epoch 384: 1.0440506942749024\n",
      "Rolling back to best model from epoch 329\n",
      "Best test loss: 1.029150119113922\n",
      "Epoch 385, Loss: 0.7491040117581685\n",
      "Test Loss after Epoch 385: 1.0452566588401795\n",
      "Epoch 386, Loss: 0.7480406930605571\n",
      "Test Loss after Epoch 386: 1.0588828533172607\n",
      "Epoch 387, Loss: 0.7482245825979444\n",
      "Test Loss after Epoch 387: 1.0437826852798462\n",
      "Epoch 388, Loss: 0.7491868913120694\n",
      "Test Loss after Epoch 388: 1.0336620719909668\n",
      "Epoch 389, Loss: 0.7486902782228257\n",
      "Test Loss after Epoch 389: 1.0520730298042298\n",
      "Epoch 390, Loss: 0.7490356968561809\n",
      "Test Loss after Epoch 390: 1.0650726047515868\n",
      "Epoch 391, Loss: 0.74901610511144\n",
      "Test Loss after Epoch 391: 1.0444028712272644\n",
      "Epoch 392, Loss: 0.748698717159695\n",
      "Test Loss after Epoch 392: 1.0537603553771973\n",
      "Epoch 393, Loss: 0.7487715678320991\n",
      "Test Loss after Epoch 393: 1.048820958328247\n",
      "Epoch 394, Loss: 0.7486292488098144\n",
      "Test Loss after Epoch 394: 1.069705106639862\n",
      "Epoch 395, Loss: 0.7488482816060384\n",
      "Test Loss after Epoch 395: 1.0517505346298217\n",
      "Epoch 396, Loss: 0.7488797178056505\n",
      "Test Loss after Epoch 396: 1.0584269229888916\n",
      "Epoch 397, Loss: 0.7487729173872206\n",
      "Test Loss after Epoch 397: 1.0587810926437378\n",
      "Epoch 398, Loss: 0.7481596310403612\n",
      "Test Loss after Epoch 398: 1.0672013803482057\n",
      "Epoch 399, Loss: 0.7489452597935995\n",
      "Test Loss after Epoch 399: 1.0667042295455933\n",
      "Epoch 400, Loss: 0.748485947142707\n",
      "Test Loss after Epoch 400: 1.092683509349823\n",
      "Epoch 401, Loss: 0.7489546413103739\n",
      "Test Loss after Epoch 401: 1.0790740156173706\n",
      "Epoch 402, Loss: 0.7482897935231527\n",
      "Test Loss after Epoch 402: 1.0536155143737793\n",
      "Epoch 403, Loss: 0.748212173016866\n",
      "Test Loss after Epoch 403: 1.0557315556526183\n",
      "Epoch 404, Loss: 0.7482356025589837\n",
      "Test Loss after Epoch 404: 1.0519374311447143\n",
      "Epoch 405, Loss: 0.7484665831353929\n",
      "Test Loss after Epoch 405: 1.0346338465690612\n",
      "Epoch 406, Loss: 0.7482943419456481\n",
      "Test Loss after Epoch 406: 1.0432201548576354\n",
      "Epoch 407, Loss: 0.7484487605730693\n",
      "Test Loss after Epoch 407: 1.0369757464408875\n",
      "Epoch 408, Loss: 0.7485505296813117\n",
      "Test Loss after Epoch 408: 1.0434658455848693\n",
      "Epoch 409, Loss: 0.7482529654290941\n",
      "Test Loss after Epoch 409: 1.0376971426963806\n",
      "Epoch 410, Loss: 0.7486950691435073\n",
      "Test Loss after Epoch 410: 1.049438620185852\n",
      "Epoch 411, Loss: 0.7482592303170098\n",
      "Test Loss after Epoch 411: 1.0359924901008606\n",
      "Epoch 412, Loss: 0.7484943228191799\n",
      "Test Loss after Epoch 412: 1.0274039044380188\n",
      "Epoch 413, Loss: 0.748240016025967\n",
      "Test Loss after Epoch 413: 1.0379264742851257\n",
      "Epoch 414, Loss: 0.7486129086494446\n",
      "Test Loss after Epoch 414: 1.0801496764183045\n",
      "Epoch 415, Loss: 0.7479168629434374\n",
      "Test Loss after Epoch 415: 1.0636959509849548\n",
      "Epoch 416, Loss: 0.7482539459652371\n",
      "Test Loss after Epoch 416: 1.0657650973320008\n",
      "Epoch 417, Loss: 0.7479717603153653\n",
      "Test Loss after Epoch 417: 1.0443469427108765\n",
      "Epoch 418, Loss: 0.7476882820447286\n",
      "Test Loss after Epoch 418: 1.0710923691749572\n",
      "Epoch 419, Loss: 0.7482681605762905\n",
      "Test Loss after Epoch 419: 1.065408099937439\n",
      "Epoch 420, Loss: 0.7479742211447822\n",
      "Test Loss after Epoch 420: 1.053311219215393\n",
      "Epoch 421, Loss: 0.7480534231609768\n",
      "Test Loss after Epoch 421: 1.076927248764038\n",
      "Epoch 422, Loss: 0.7478169747776455\n",
      "Test Loss after Epoch 422: 1.0591609017372132\n",
      "Epoch 423, Loss: 0.7480390503989326\n",
      "Test Loss after Epoch 423: 1.0598637563705444\n",
      "Epoch 424, Loss: 0.747726849799686\n",
      "Test Loss after Epoch 424: 1.0257576475143433\n",
      "Epoch 425, Loss: 0.7480365347438388\n",
      "Test Loss after Epoch 425: 1.0227447952270508\n",
      "Epoch 426, Loss: 0.7476670075734456\n",
      "Test Loss after Epoch 426: 1.0368459930419922\n",
      "Epoch 427, Loss: 0.7481921637535095\n",
      "Test Loss after Epoch 427: 1.0478491647720336\n",
      "Epoch 428, Loss: 0.7474915991465251\n",
      "Test Loss after Epoch 428: 1.038255687713623\n",
      "Epoch 429, Loss: 0.7478407016436259\n",
      "Test Loss after Epoch 429: 1.0446716773033142\n",
      "Epoch 430, Loss: 0.7474277778413561\n",
      "Test Loss after Epoch 430: 1.0342392155647278\n",
      "Epoch 431, Loss: 0.747750135951572\n",
      "Test Loss after Epoch 431: 1.0457692388534545\n",
      "Epoch 432, Loss: 0.747619138431549\n",
      "Test Loss after Epoch 432: 1.0175415453910828\n",
      "Epoch 433, Loss: 0.7477086076100667\n",
      "Test Loss after Epoch 433: 1.0354005056381226\n",
      "Epoch 434, Loss: 0.7477991796281602\n",
      "Test Loss after Epoch 434: 1.0364413036346436\n",
      "Epoch 435, Loss: 0.7476853551970588\n",
      "Test Loss after Epoch 435: 1.0466867170333862\n",
      "Epoch 436, Loss: 0.7477884590996636\n",
      "Test Loss after Epoch 436: 1.0356836022377014\n",
      "Epoch 437, Loss: 0.7472862242698669\n",
      "Test Loss after Epoch 437: 1.080931480026245\n",
      "Epoch 438, Loss: 0.7470245708359612\n",
      "Test Loss after Epoch 438: 1.0487846342086793\n",
      "Epoch 439, Loss: 0.7474232800271776\n",
      "Test Loss after Epoch 439: 1.0701384150505067\n",
      "Epoch 440, Loss: 0.7473047722180685\n",
      "Test Loss after Epoch 440: 1.0638165759086609\n",
      "Epoch 441, Loss: 0.747382616244422\n",
      "Test Loss after Epoch 441: 1.0560990099906922\n",
      "Epoch 442, Loss: 0.747736222881741\n",
      "Test Loss after Epoch 442: 1.0531031051635742\n",
      "Epoch 443, Loss: 0.7477258587201436\n",
      "Test Loss after Epoch 443: 1.0447425415992737\n",
      "Epoch 444, Loss: 0.7474752653121948\n",
      "Test Loss after Epoch 444: 1.064926454257965\n",
      "Epoch 445, Loss: 0.7474981487062242\n",
      "Test Loss after Epoch 445: 1.0455496430397033\n",
      "Epoch 446, Loss: 0.7469978559705946\n",
      "Test Loss after Epoch 446: 1.0466788411140442\n",
      "Epoch 447, Loss: 0.7471214246114095\n",
      "Test Loss after Epoch 447: 1.0370105712890625\n",
      "Epoch 448, Loss: 0.7471545962545607\n",
      "Test Loss after Epoch 448: 1.0529032276153565\n",
      "Epoch 449, Loss: 0.7471298234833611\n",
      "Test Loss after Epoch 449: 1.0563510773658753\n",
      "Epoch 450, Loss: 0.7472525022612677\n",
      "Test Loss after Epoch 450: 1.0693603121757507\n",
      "Epoch 451, Loss: 0.7473187169180976\n",
      "Test Loss after Epoch 451: 1.054679275894165\n",
      "Epoch 452, Loss: 0.7470997579256694\n",
      "Test Loss after Epoch 452: 1.044336941242218\n",
      "Epoch 453, Loss: 0.747345969496833\n",
      "Test Loss after Epoch 453: 1.0354505319595337\n",
      "Epoch 454, Loss: 0.7470199401749505\n",
      "Test Loss after Epoch 454: 1.0558718608856201\n",
      "Epoch 455, Loss: 0.7469876077863905\n",
      "Test Loss after Epoch 455: 1.0476992186546326\n",
      "Epoch 456, Loss: 0.746943314965566\n",
      "Test Loss after Epoch 456: 1.0483308104515077\n",
      "Epoch 457, Loss: 0.7474223775121901\n",
      "Test Loss after Epoch 457: 1.0327552025794984\n",
      "Epoch 458, Loss: 0.7473189383294847\n",
      "Test Loss after Epoch 458: 1.0261068069458008\n",
      "Epoch 459, Loss: 0.7472566521432664\n",
      "Test Loss after Epoch 459: 1.0302825434684753\n",
      "Epoch 460, Loss: 0.7469377311812507\n",
      "Test Loss after Epoch 460: 1.0165882934570312\n",
      "Epoch 461, Loss: 0.7470221142239041\n",
      "Test Loss after Epoch 461: 1.032333926296234\n",
      "Epoch 462, Loss: 0.7468451200697157\n",
      "Test Loss after Epoch 462: 1.0183167256355286\n",
      "Epoch 463, Loss: 0.746962130716112\n",
      "Test Loss after Epoch 463: 1.0405359086990356\n",
      "Epoch 464, Loss: 0.7469518551190694\n",
      "Test Loss after Epoch 464: 1.040184016418457\n",
      "Epoch 465, Loss: 0.7465574244393243\n",
      "Test Loss after Epoch 465: 1.0318373827934264\n",
      "Epoch 466, Loss: 0.7469107295354207\n",
      "Test Loss after Epoch 466: 1.0321415830612182\n",
      "Epoch 467, Loss: 0.7472229259173075\n",
      "Test Loss after Epoch 467: 1.0273670679092408\n",
      "Epoch 468, Loss: 0.7472129202312894\n",
      "Test Loss after Epoch 468: 1.020352774143219\n",
      "Epoch 469, Loss: 0.7469471443282233\n",
      "Test Loss after Epoch 469: 1.042768836212158\n",
      "Epoch 470, Loss: 0.7467333663622538\n",
      "Test Loss after Epoch 470: 1.017015490913391\n",
      "Epoch 471, Loss: 0.7473583158175151\n",
      "Test Loss after Epoch 471: 1.0495934247016907\n",
      "Epoch 472, Loss: 0.7473340069558886\n",
      "Test Loss after Epoch 472: 1.0432033971786498\n",
      "Epoch 473, Loss: 0.7472517536799113\n",
      "Test Loss after Epoch 473: 1.027318635749817\n",
      "Epoch 474, Loss: 0.747354085805681\n",
      "Test Loss after Epoch 474: 1.0374590242385864\n",
      "Epoch 475, Loss: 0.746839698653751\n",
      "Test Loss after Epoch 475: 1.0451050616264344\n",
      "Epoch 476, Loss: 0.7464051187091404\n",
      "Test Loss after Epoch 476: 1.0807759733200073\n",
      "Epoch 477, Loss: 0.746587405649821\n",
      "Test Loss after Epoch 477: 1.0789216592788697\n",
      "Epoch 478, Loss: 0.7466069531122843\n",
      "Test Loss after Epoch 478: 1.0669106165885924\n",
      "Epoch 479, Loss: 0.7462767675823635\n",
      "Test Loss after Epoch 479: 1.0722218329429627\n",
      "Epoch 480, Loss: 0.7463653194851345\n",
      "Test Loss after Epoch 480: 1.0665191359519959\n",
      "Epoch 481, Loss: 0.7466160373051961\n",
      "Test Loss after Epoch 481: 1.0649291681289672\n",
      "Epoch 482, Loss: 0.7469807166205512\n",
      "Test Loss after Epoch 482: 1.0586485684394837\n",
      "Epoch 483, Loss: 0.7465189705530803\n",
      "Test Loss after Epoch 483: 1.033243699359894\n",
      "Epoch 484, Loss: 0.7467318270259433\n",
      "Test Loss after Epoch 484: 1.0333651593208313\n",
      "Epoch 485, Loss: 0.7470998148600261\n",
      "Test Loss after Epoch 485: 1.0293973660469056\n",
      "Epoch 486, Loss: 0.7473539466328091\n",
      "Test Loss after Epoch 486: 1.0313742977142335\n",
      "Epoch 487, Loss: 0.7464436084429423\n",
      "Test Loss after Epoch 487: 1.0394473048210144\n",
      "Epoch 488, Loss: 0.7467783866776361\n",
      "Test Loss after Epoch 488: 1.0368920719146728\n",
      "Epoch 489, Loss: 0.746734842438168\n",
      "Test Loss after Epoch 489: 1.0604132488250733\n",
      "Epoch 490, Loss: 0.7464594973458184\n",
      "Test Loss after Epoch 490: 1.0504609524726867\n",
      "Epoch 491, Loss: 0.7462084273762173\n",
      "Test Loss after Epoch 491: 1.0355343772888184\n",
      "Epoch 492, Loss: 0.7468180825763279\n",
      "Test Loss after Epoch 492: 1.0547215591430663\n",
      "Epoch 493, Loss: 0.7469116868760851\n",
      "Test Loss after Epoch 493: 1.044052743911743\n",
      "Epoch 494, Loss: 0.7464328794797261\n",
      "Test Loss after Epoch 494: 1.0459907135009765\n",
      "Epoch 495, Loss: 0.7471394011073642\n",
      "Test Loss after Epoch 495: 1.0330382612228393\n",
      "Epoch 496, Loss: 0.7467687858581543\n",
      "Test Loss after Epoch 496: 1.034728129863739\n",
      "Epoch 497, Loss: 0.7462184183014764\n",
      "Test Loss after Epoch 497: 1.054208675479889\n",
      "Epoch 498, Loss: 0.7465106454849243\n",
      "Test Loss after Epoch 498: 1.03457797498703\n",
      "Epoch 499, Loss: 0.7466239469634162\n",
      "Test Loss after Epoch 499: 1.0565049899101258\n",
      "Epoch 500, Loss: 0.7464497953097026\n",
      "Test Loss after Epoch 500: 1.050790545463562\n",
      "Epoch 501, Loss: 0.7463984911282857\n",
      "Test Loss after Epoch 501: 1.0325891060829162\n",
      "Epoch 502, Loss: 0.7467083932664659\n",
      "Test Loss after Epoch 502: 1.051001183605194\n",
      "Epoch 503, Loss: 0.7460245652516683\n",
      "Test Loss after Epoch 503: 1.045207581424713\n",
      "Epoch 504, Loss: 0.7465942230118645\n",
      "Test Loss after Epoch 504: 1.0759774888038636\n",
      "Epoch 505, Loss: 0.7464742517789205\n",
      "Test Loss after Epoch 505: 1.0790661966323853\n",
      "Epoch 506, Loss: 0.7459295565711127\n",
      "Test Loss after Epoch 506: 1.0510853136062621\n",
      "Epoch 507, Loss: 0.7465698677592807\n",
      "Test Loss after Epoch 507: 1.068941692829132\n",
      "Epoch 508, Loss: 0.7463395725038316\n",
      "Test Loss after Epoch 508: 1.0470505655288695\n",
      "Epoch 509, Loss: 0.7467091798040602\n",
      "Test Loss after Epoch 509: 1.0485612617492677\n",
      "Epoch 510, Loss: 0.74611929558648\n",
      "Test Loss after Epoch 510: 1.0651308485984803\n",
      "Epoch 511, Loss: 0.7467454765107897\n",
      "Test Loss after Epoch 511: 1.0387808691978455\n",
      "Epoch 512, Loss: 0.7461263153817919\n",
      "Test Loss after Epoch 512: 1.0363133531570434\n",
      "Epoch 513, Loss: 0.7457494352446662\n",
      "Test Loss after Epoch 513: 1.0734394690513611\n",
      "Epoch 514, Loss: 0.7464534897168478\n",
      "Test Loss after Epoch 514: 1.065845887184143\n",
      "Epoch 515, Loss: 0.7460447411007352\n",
      "Test Loss after Epoch 515: 1.0459473105430603\n",
      "Epoch 516, Loss: 0.7467138062477112\n",
      "Test Loss after Epoch 516: 1.0609862361907958\n",
      "Epoch 517, Loss: 0.7460045475535922\n",
      "Test Loss after Epoch 517: 1.0599169710159302\n",
      "Epoch 518, Loss: 0.7461212650934855\n",
      "Test Loss after Epoch 518: 1.053211678981781\n",
      "Epoch 519, Loss: 0.7460330210579766\n",
      "Test Loss after Epoch 519: 1.0242314841270446\n",
      "Epoch 520, Loss: 0.7461874818589952\n",
      "Test Loss after Epoch 520: 1.0478162567138671\n",
      "Epoch 521, Loss: 0.7458454813957215\n",
      "Test Loss after Epoch 521: 1.0524710389137268\n",
      "Epoch 522, Loss: 0.7455548947652181\n",
      "Test Loss after Epoch 522: 1.0440413001060487\n",
      "Epoch 523, Loss: 0.7460176825205485\n",
      "Test Loss after Epoch 523: 1.050861884212494\n",
      "Epoch 524, Loss: 0.7461604359308879\n",
      "Test Loss after Epoch 524: 1.0336577829360962\n",
      "Epoch 525, Loss: 0.7458653767903646\n",
      "Test Loss after Epoch 525: 1.051034395313263\n",
      "Epoch 526, Loss: 0.7457566806687249\n",
      "Test Loss after Epoch 526: 1.0635156208992005\n",
      "Epoch 527, Loss: 0.7461001677513123\n",
      "Test Loss after Epoch 527: 1.0603866225242615\n",
      "Epoch 528, Loss: 0.7456540807406108\n",
      "Test Loss after Epoch 528: 1.0537849510192872\n",
      "Epoch 529, Loss: 0.7458818877538045\n",
      "Test Loss after Epoch 529: 1.0594208450317384\n",
      "Epoch 530, Loss: 0.7461060572942098\n",
      "Test Loss after Epoch 530: 1.0541028155326844\n",
      "Epoch 531, Loss: 0.7459825097401936\n",
      "Test Loss after Epoch 531: 1.0534563638687133\n",
      "Epoch 532, Loss: 0.7463475770526462\n",
      "Test Loss after Epoch 532: 1.0539239088058472\n",
      "Epoch 533, Loss: 0.7462381935013666\n",
      "Test Loss after Epoch 533: 1.0554928488731383\n",
      "Epoch 534, Loss: 0.7463764975971646\n",
      "Test Loss after Epoch 534: 1.048640950870514\n",
      "Epoch 535, Loss: 0.7459224186897278\n",
      "Test Loss after Epoch 535: 1.0271208457946777\n",
      "Epoch 536, Loss: 0.7460086426099142\n",
      "Test Loss after Epoch 536: 1.0487083042144776\n",
      "Epoch 537, Loss: 0.7453473036660089\n",
      "Test Loss after Epoch 537: 1.0476292273521424\n",
      "Rolling back to best model from epoch 460\n",
      "Best test loss: 1.0165882934570312\n",
      "Epoch 538, Loss: 0.7468399137390984\n",
      "Test Loss after Epoch 538: 1.0406120678901671\n",
      "Epoch 539, Loss: 0.7463099392255147\n",
      "Test Loss after Epoch 539: 1.0288188940048217\n",
      "Epoch 540, Loss: 0.7468664008670383\n",
      "Test Loss after Epoch 540: 1.0396180889129638\n",
      "Epoch 541, Loss: 0.7461395789782206\n",
      "Test Loss after Epoch 541: 1.0474124402999878\n",
      "Epoch 542, Loss: 0.7462777607705858\n",
      "Test Loss after Epoch 542: 1.0478780115127564\n",
      "Epoch 543, Loss: 0.7463666964530945\n",
      "Test Loss after Epoch 543: 1.0805559220314025\n",
      "Epoch 544, Loss: 0.7460747033331129\n",
      "Test Loss after Epoch 544: 1.0468371755599977\n",
      "Epoch 545, Loss: 0.7467417525821262\n",
      "Test Loss after Epoch 545: 1.0455698547363281\n",
      "Epoch 546, Loss: 0.7459732842657301\n",
      "Test Loss after Epoch 546: 1.0466947447776795\n",
      "Epoch 547, Loss: 0.7466434269481235\n",
      "Test Loss after Epoch 547: 1.0490179322242736\n",
      "Epoch 548, Loss: 0.7470890880054898\n",
      "Test Loss after Epoch 548: 1.066650938987732\n",
      "Epoch 549, Loss: 0.7466608298831516\n",
      "Test Loss after Epoch 549: 1.0705120182991028\n",
      "Epoch 550, Loss: 0.7468105735672845\n",
      "Test Loss after Epoch 550: 1.0802936823844909\n",
      "Epoch 551, Loss: 0.746584274260203\n",
      "Test Loss after Epoch 551: 1.076196116733551\n",
      "Epoch 552, Loss: 0.7461645942476061\n",
      "Test Loss after Epoch 552: 1.0649846601486206\n",
      "Epoch 553, Loss: 0.7461572644763522\n",
      "Test Loss after Epoch 553: 1.06328947763443\n",
      "Epoch 554, Loss: 0.7461702138794793\n",
      "Test Loss after Epoch 554: 1.0417395009994508\n",
      "Epoch 555, Loss: 0.7465094857109917\n",
      "Test Loss after Epoch 555: 1.0548479154586792\n",
      "Epoch 556, Loss: 0.7464527664396498\n",
      "Test Loss after Epoch 556: 1.0675435748100282\n",
      "Epoch 557, Loss: 0.7462030650032891\n",
      "Test Loss after Epoch 557: 1.059717340183258\n",
      "Epoch 558, Loss: 0.7463580339961582\n",
      "Test Loss after Epoch 558: 1.0337425961494446\n",
      "Epoch 559, Loss: 0.7464670400937399\n",
      "Test Loss after Epoch 559: 1.0462291166305542\n",
      "Epoch 560, Loss: 0.7462720019658406\n",
      "Test Loss after Epoch 560: 1.066396309566498\n",
      "Epoch 561, Loss: 0.7461460589514838\n",
      "Test Loss after Epoch 561: 1.0710569749832153\n",
      "Epoch 562, Loss: 0.7463481168746948\n",
      "Test Loss after Epoch 562: 1.0555151861190797\n",
      "Epoch 563, Loss: 0.7461990795983209\n",
      "Test Loss after Epoch 563: 1.0369778719902039\n",
      "Epoch 564, Loss: 0.7462781782892015\n",
      "Test Loss after Epoch 564: 1.0299500658035279\n",
      "Epoch 565, Loss: 0.7464185699780782\n",
      "Test Loss after Epoch 565: 1.0459275922775269\n",
      "Epoch 566, Loss: 0.7460916676627265\n",
      "Test Loss after Epoch 566: 1.0263621954917908\n",
      "Epoch 567, Loss: 0.746071949397193\n",
      "Test Loss after Epoch 567: 1.0330228130340575\n",
      "Epoch 568, Loss: 0.7463086634741889\n",
      "Test Loss after Epoch 568: 1.0415841081619264\n",
      "Epoch 569, Loss: 0.7456638186242845\n",
      "Test Loss after Epoch 569: 1.0348947008132934\n",
      "Epoch 570, Loss: 0.7460287783940633\n",
      "Test Loss after Epoch 570: 1.0433222571372986\n",
      "Epoch 571, Loss: 0.746343058151669\n",
      "Test Loss after Epoch 571: 1.037527307510376\n",
      "Epoch 572, Loss: 0.7461041489389207\n",
      "Test Loss after Epoch 572: 1.037121213722229\n",
      "Epoch 573, Loss: 0.7463141565852695\n",
      "Test Loss after Epoch 573: 1.059705592727661\n",
      "Epoch 574, Loss: 0.7453892326566908\n",
      "Test Loss after Epoch 574: 1.0595740933418274\n",
      "Epoch 575, Loss: 0.7460140007654826\n",
      "Test Loss after Epoch 575: 1.039132040309906\n",
      "Epoch 576, Loss: 0.7458186147054037\n",
      "Test Loss after Epoch 576: 1.0425759046554566\n",
      "Epoch 577, Loss: 0.7463292589505514\n",
      "Test Loss after Epoch 577: 1.0425823761940003\n",
      "Epoch 578, Loss: 0.7458478663126628\n",
      "Test Loss after Epoch 578: 1.0276200272560119\n",
      "Epoch 579, Loss: 0.7462713356229994\n",
      "Test Loss after Epoch 579: 1.0489369146347045\n",
      "Epoch 580, Loss: 0.7459486359066433\n",
      "Test Loss after Epoch 580: 1.0489860300064087\n",
      "Epoch 581, Loss: 0.7453264656914605\n",
      "Test Loss after Epoch 581: 1.0458945513725282\n",
      "Epoch 582, Loss: 0.7456142140388489\n",
      "Test Loss after Epoch 582: 1.0347071210861205\n",
      "Epoch 583, Loss: 0.7455798488828871\n",
      "Test Loss after Epoch 583: 1.015656809425354\n",
      "Epoch 584, Loss: 0.7454814431190491\n",
      "Test Loss after Epoch 584: 1.033464035320282\n",
      "Epoch 585, Loss: 0.7458394273757935\n",
      "Test Loss after Epoch 585: 1.038065223312378\n",
      "Epoch 586, Loss: 0.7455195849206713\n",
      "Test Loss after Epoch 586: 1.0260969003677367\n",
      "Epoch 587, Loss: 0.7456581315146552\n",
      "Test Loss after Epoch 587: 1.037306022644043\n",
      "Epoch 588, Loss: 0.7453487206988865\n",
      "Test Loss after Epoch 588: 1.0310069382667542\n",
      "Epoch 589, Loss: 0.7455144545025296\n",
      "Test Loss after Epoch 589: 1.035917052078247\n",
      "Epoch 590, Loss: 0.7459161748886108\n",
      "Test Loss after Epoch 590: 1.0417316250801087\n",
      "Epoch 591, Loss: 0.7458187960518731\n",
      "Test Loss after Epoch 591: 1.0371489976882935\n",
      "Epoch 592, Loss: 0.7454367074754503\n",
      "Test Loss after Epoch 592: 1.0356137058258057\n",
      "Epoch 593, Loss: 0.7454387613296509\n",
      "Test Loss after Epoch 593: 1.0330114071846008\n",
      "Epoch 594, Loss: 0.7454993668980069\n",
      "Test Loss after Epoch 594: 1.053650604915619\n",
      "Epoch 595, Loss: 0.7457026415295072\n",
      "Test Loss after Epoch 595: 1.028879548072815\n",
      "Epoch 596, Loss: 0.746117267036438\n",
      "Test Loss after Epoch 596: 1.0465209241867066\n",
      "Epoch 597, Loss: 0.7459285528394911\n",
      "Test Loss after Epoch 597: 1.037872880077362\n",
      "Epoch 598, Loss: 0.7455143279817369\n",
      "Test Loss after Epoch 598: 1.0196142406463624\n",
      "Epoch 599, Loss: 0.7455914951218499\n",
      "Test Loss after Epoch 599: 1.0515861461639404\n",
      "Epoch 600, Loss: 0.7455287302652995\n",
      "Test Loss after Epoch 600: 1.0614112132072449\n"
     ]
    }
   ],
   "source": [
    "for train_size in range(2500, 2501, 100):\n",
    "    train_set, test_set, val_set = prepare_data(data, val_size=1000, train_size=train_size, test_ratio=0.1)\n",
    "    train_examples = [ex for paradigm in train_set for ex in generate_examples(paradigm)]\n",
    "    test_examples = [ex for paradigm in test_set for ex in generate_examples(paradigm)]\n",
    "    print(len(train_set), len(train_examples))\n",
    "    model = CharTransformer(vocab_size, device=device, max_len=max_len)\n",
    "    train_model(model, train_examples, test_examples, batch_size=398, epochs=1500000//train_size)\n",
    "\n",
    "    save_dir = \"/home/minhk/Assignments/CSCI 5801/project/model/\"\n",
    "    filename=f\"base_transformers_alt_{train_size}.pth\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    torch.save(model, save_dir+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PRS>wuff wuff\n",
      "wuff\n",
      "<3SG>wuff wuffs\n",
      "wuffs\n",
      "<PST>wuff wuffed\n",
      "wufffufuffied\n",
      "<PRS.PTCP>wuff wuffing\n",
      "wuffing\n",
      "<PST.PTCP>wuff wuffed\n",
      "wufffufuffied\n"
     ]
    }
   ],
   "source": [
    "# Select a random paradigm from validation set\n",
    "random_paradigm = random.choice(val_set)\n",
    "\n",
    "# Generate examples from the paradigm\n",
    "generated_examples = generate_examples(random_paradigm)\n",
    "\n",
    "def list_to_word(arr):\n",
    "    return ''.join(arr[1:-1])\n",
    "\n",
    "for src, tgt in generated_examples: \n",
    "    print(list_to_word(src), list_to_word(tgt))\n",
    "    feature_mask = create_feature_mask(src).unsqueeze(0)\n",
    "    src_tokenized = torch.tensor(tokenize(src, char_to_idx), device=device).unsqueeze(0)\n",
    "    gen = model.generate(src_tokenized, feature_mask, beam_size=5).squeeze(0)\n",
    "    gen = list_to_word([idx_to_char[id.item()] for id in gen])\n",
    "    print(gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sing + PRS = sing, sing\n",
      "sing + 3SG = sings, sings\n",
      "sing + PST = singgung, singgung\n",
      "sing + PRS.PTCP = singing, singing\n",
      "sing + PST.PTCP = singgung, singgung\n"
     ]
    }
   ],
   "source": [
    "word = 'sing'\n",
    "tags = [\"PRS\", \"3SG\", \"PST\", \"PRS.PTCP\", \"PST.PTCP\"]\n",
    "\n",
    "for tag in tags: \n",
    "    tokens = [\"<s>\", f\"<{tag}>\"] + list(word) + [\"</s>\"]\n",
    "    feature_mask = create_feature_mask(tokens).unsqueeze(0)\n",
    "    src_tokenized = torch.tensor(tokenize(tokens, char_to_idx), device=device).unsqueeze(0)\n",
    "    gen = model.generate(src_tokenized, feature_mask).squeeze(0)\n",
    "    gen = list_to_word([idx_to_char[id.item()] for id in gen])\n",
    "    gen2 = model.greedy_batch_decode(src_tokenized, feature_mask).squeeze(0)\n",
    "    gen2 = list_to_word([idx_to_char[id.item()] for id in gen2])\n",
    "    print(f\"{word} + {tag} = {gen}, {gen2}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2500 loaded\n",
      "Model trained on 2500 paradigms; 23612 predictions correct out of 25000 total. Accuracy: 0.9445\n"
     ]
    }
   ],
   "source": [
    "# Perform greedy batched decoding\n",
    "train_set, test_set, val_set = prepare_data(data, val_size=5000, train_size=train_size, test_ratio=0.1)\n",
    "\n",
    "for train_size in range(2500, 2501, 100):\n",
    "    save_dir = f\"/home/minhk/Assignments/CSCI 5801/project/model/base_transformers_alt_{train_size}.pth\"\n",
    "\n",
    "    # Load model\n",
    "    model = torch.load(save_dir, map_location=device, weights_only=False)\n",
    "    model.to(device)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    print(f\"Model {train_size} loaded\")\n",
    "\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    batch_size = 32\n",
    "\n",
    "    all_src = []\n",
    "    all_tgt = []\n",
    "\n",
    "    for paradigm in val_set:\n",
    "        pairs = generate_examples(paradigm)\n",
    "        for src, tgt in pairs:\n",
    "            all_src.append(src)\n",
    "            all_tgt.append(tgt)\n",
    "\n",
    "    # Determine max sequence length in the batch for padding\n",
    "    def pad_sequences(sequences, pad_token, max_len=None):\n",
    "        \"\"\"Pads sequences to the max length with the given pad token.\"\"\"\n",
    "        max_len = max(len(seq) for seq in sequences)\n",
    "        return [seq + [pad_token] * (max_len - len(seq)) for seq in sequences]\n",
    "\n",
    "    # Process in batches\n",
    "    for i in range(0, len(all_src), batch_size):\n",
    "        src_batch = all_src[i:i+batch_size]\n",
    "        tgt_batch = all_tgt[i:i+batch_size]\n",
    "\n",
    "        try:\n",
    "            # Pad inputs\n",
    "            src_batch_padded = pad_sequences(src_batch, \"<pad>\")\n",
    "            tgt_batch_padded = pad_sequences(tgt_batch, \"<pad>\")  # Only for consistent tensor creation\n",
    "            \n",
    "            feature_mask = torch.stack([create_feature_mask(src) for src in src_batch_padded]).to(device)\n",
    "            src_tokenized = torch.tensor([tokenize(seq, char_to_idx) for seq in src_batch_padded], device=device)\n",
    "            \n",
    "            gen_batch = model.greedy_batch_decode(src_tokenized, feature_mask)\n",
    "            \n",
    "            for gen, tgt in zip(gen_batch, tgt_batch):\n",
    "                gen_str = \"\".join([idx_to_char[idx.item()] for idx in gen])\n",
    "                \n",
    "                # Trim at first stop token\n",
    "                gen_str = gen_str.split(\"</s>\")[0][3:]\n",
    "                \n",
    "                correct_predictions += (gen_str == list_to_word(tgt))\n",
    "                total_predictions += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Print accuracy\n",
    "    print(f\"Model trained on {train_size} paradigms; {correct_predictions} predictions correct out of {total_predictions} total. Accuracy: {correct_predictions / total_predictions:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20189 predictions correct out of 25000 total. Accuracy: 0.80756\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "for paradigm in val_set:\n",
    "    pairs = generate_examples(paradigm)\n",
    "    for src, tgt in pairs: \n",
    "        feature_mask = create_feature_mask(src).unsqueeze(0)\n",
    "        src_tokenized = torch.tensor(tokenize(src, char_to_idx), device=device).unsqueeze(0)\n",
    "        gen = model.generate(src_tokenized, feature_mask).squeeze(0)\n",
    "        gen = [idx_to_char[id.item()] for id in gen]\n",
    "        correct_predictions += (gen == tgt)\n",
    "        total_predictions += 1\n",
    "\n",
    "print(f\"{correct_predictions} predictions correct out of {total_predictions} total. Accuracy: {correct_predictions / total_predictions}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
