{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LRScheduler\n",
    "from torch.nn import Transformer\n",
    "import json\n",
    "import random\n",
    "import copy\n",
    "import warnings\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, nhead=4, num_layers=4, d_ff=1024, max_len=32, \n",
    "                 dropout=0.3, device=\"cuda\", pad_token_id=0, start_token_id=1, end_token_id=2):\n",
    "        super(CharTransformer, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.start_token_id = start_token_id\n",
    "        self.end_token_id = end_token_id\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model).to(device)\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout after embedding\n",
    "        \n",
    "        # Create sinusoidal positional encodings\n",
    "        self.register_buffer(\"positional_encoding\", self._generate_sinusoidal_encoding(max_len, d_model))\n",
    "        \n",
    "        self.transformer = Transformer(\n",
    "            d_model=d_model, nhead=nhead, num_encoder_layers=num_layers, \n",
    "            num_decoder_layers=num_layers, dim_feedforward=d_ff, activation=\"gelu\", batch_first=True, \n",
    "            dropout=dropout \n",
    "        ).to(device)\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size).to(device)\n",
    "    \n",
    "    def _generate_sinusoidal_encoding(self, max_len, d_model):\n",
    "        position = torch.arange(max_len, device=self.device).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, device=self.device) * -(math.log(10000.0) / d_model))\n",
    "        encoding = torch.zeros(max_len, d_model, device=self.device)\n",
    "        encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        return encoding\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.tril(torch.ones(sz, sz))\n",
    "        return torch.log(mask).to(self.device)\n",
    "        \n",
    "    def forward(self, src, tgt, feature_mask, tgt_is_causal=False, src_mask=None, tgt_mask=None, \n",
    "                src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        src, tgt = src.to(self.device), tgt.to(self.device)\n",
    "        \n",
    "        # Compute embeddings and apply dropout\n",
    "        src_emb = self.embedding(src)\n",
    "        tgt_emb = self.embedding(tgt)\n",
    "\n",
    "        # Apply positional encoding\n",
    "        src_emb += (1 - feature_mask[:, :src.shape[1], None]) * self.positional_encoding[:src.shape[1], :]\n",
    "        tgt_emb += self.positional_encoding[:tgt.shape[1], :].unsqueeze(0)\n",
    "        \n",
    "        src_emb = self.dropout(src_emb)\n",
    "        tgt_emb = self.dropout(tgt_emb)\n",
    "\n",
    "        # Compute key padding masks if not provided\n",
    "        if src_key_padding_mask is None:\n",
    "            src_key_padding_mask = (src == self.pad_token_id) \n",
    "        if tgt_key_padding_mask is None:\n",
    "            tgt_key_padding_mask = (tgt == self.pad_token_id)\n",
    "        if tgt_is_causal: \n",
    "            if tgt_mask is None: \n",
    "                tgt_mask = self._generate_square_subsequent_mask(tgt.shape[1])\n",
    "            else:\n",
    "                tgt_mask += self._generate_square_subsequent_mask(tgt.shape[1])\n",
    "\n",
    "        # Transformer forward pass\n",
    "        transformer_output = self.transformer(\n",
    "            src_emb, tgt_emb, \n",
    "            src_mask=src_mask, \n",
    "            tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            tgt_is_causal=tgt_is_causal\n",
    "        )\n",
    "        output = self.fc_out(transformer_output)\n",
    "        return output\n",
    "    \n",
    "    def generate(self, src, feature_mask, max_len=32, beam_size=5):\n",
    "        self.eval()\n",
    "        src, feature_mask = src.to(self.device), feature_mask.to(self.device)\n",
    "\n",
    "        # Initialize beams: (sequence, log probability)\n",
    "        beams = torch.full((1, 1), self.start_token_id, device=self.device)  \n",
    "        beam_scores = torch.zeros(1, device=self.device)  # Log probabilities\n",
    "\n",
    "        completed_sequences = []  # Store completed sequences\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            # Expand `src` to match the number of beams\n",
    "            src_expanded = src.expand(beams.shape[0], -1)\n",
    "            feature_mask_expanded = feature_mask.expand(beams.shape[0], -1)\n",
    "\n",
    "            # Forward pass on all beams at once\n",
    "            out = self.forward(src_expanded, beams, feature_mask_expanded)  \n",
    "            logits = out[:, -1, :]  # Get last-step logits (shape: [beams, vocab_size])\n",
    "            log_probs = torch.log_softmax(logits, dim=-1)  # Convert logits to log-probabilities\n",
    "\n",
    "            # Get top-k candidates for each beam (shape: [beams, beam_size])\n",
    "            topk_log_probs, topk_ids = log_probs.topk(beam_size, dim=-1)\n",
    "\n",
    "            # Compute new scores by adding log probabilities (broadcasted)\n",
    "            expanded_scores = beam_scores.unsqueeze(1) + topk_log_probs  # Shape: [beams, beam_size]\n",
    "            expanded_scores = expanded_scores.view(-1)  # Flatten to [beams * beam_size]\n",
    "\n",
    "            # Get top-k overall candidates\n",
    "            topk_scores, topk_indices = expanded_scores.topk(beam_size)\n",
    "\n",
    "            # Convert flat indices to beam/token indices\n",
    "            beam_indices = topk_indices // beam_size  # Which original beam did this come from?\n",
    "            token_indices = topk_indices % beam_size  # Which token was selected?\n",
    "\n",
    "            # Append new tokens to sequences\n",
    "            new_beams = torch.cat([beams[beam_indices], topk_ids.view(-1, 1)[topk_indices]], dim=-1)\n",
    "\n",
    "            # Check for completed sequences\n",
    "            eos_mask = (new_beams[:, -1] == self.end_token_id)\n",
    "            if eos_mask.any():\n",
    "                for i in range(beam_size):\n",
    "                    if eos_mask[i]:\n",
    "                        completed_sequences.append((new_beams[i], topk_scores[i]))\n",
    "\n",
    "            # Keep only unfinished sequences\n",
    "            beams = new_beams[~eos_mask]\n",
    "            beam_scores = topk_scores[~eos_mask]\n",
    "\n",
    "            # If all sequences finished, stop early\n",
    "            if len(beams) == 0 or len(completed_sequences) >= beam_size:\n",
    "                break\n",
    "\n",
    "        # Choose the best sequence from completed ones\n",
    "        if completed_sequences:\n",
    "            best_sequence = max(completed_sequences, key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            best_sequence = beams[0]  # If no sequence completed, return best unfinished one\n",
    "\n",
    "        return best_sequence\n",
    "    \n",
    "    def greedy_batch_decode(self, src, feature_mask, max_len=32):\n",
    "        self.eval()\n",
    "        batch_size = src.shape[0]\n",
    "        src, feature_mask = src.to(self.device), feature_mask.to(self.device)\n",
    "        outputs = torch.full((batch_size, 1), self.start_token_id, device=self.device)\n",
    "        ended = torch.zeros(batch_size, dtype=torch.bool, device=self.device)\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            out = self.forward(src, outputs, feature_mask)\n",
    "            next_tokens = out[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "            outputs = torch.cat([outputs, next_tokens], dim=1)\n",
    "            ended |= (next_tokens.squeeze() == self.end_token_id)\n",
    "            if ended.all():\n",
    "                break\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples: [(['<s>', '<PRS>', 'c', 'a', 'r', 'i', 'c', 'a', 't', 'u', 'r', 'e', '</s>'], ['<s>', 'c', 'a', 'r', 'i', 'c', 'a', 't', 'u', 'r', 'e', '</s>']), (['<s>', '<3SG>', 'c', 'a', 'r', 'i', 'c', 'a', 't', 'u', 'r', 'e', '</s>'], ['<s>', 'c', 'a', 'r', 'i', 'c', 'a', 't', 'u', 'r', 'e', 's', '</s>']), (['<s>', '<PST>', 'c', 'a', 'r', 'i', 'c', 'a', 't', 'u', 'r', 'e', '</s>'], ['<s>', 'c', 'a', 'r', 'i', 'c', 'a', 't', 'u', 'r', 'e', 'd', '</s>']), (['<s>', '<PRS.PTCP>', 'c', 'a', 'r', 'i', 'c', 'a', 't', 'u', 'r', 'e', '</s>'], ['<s>', 'c', 'a', 'r', 'i', 'c', 'a', 't', 'u', 'r', 'i', 'n', 'g', '</s>']), (['<s>', '<PST.PTCP>', 'c', 'a', 'r', 'i', 'c', 'a', 't', 'u', 'r', 'e', '</s>'], ['<s>', 'c', 'a', 'r', 'i', 'c', 'a', 't', 'u', 'r', 'e', 'd', '</s>'])]\n",
      "Test examples: [(['<s>', '<PRS>', 'z', 'o', 'o', 'm', 'o', 'r', 'p', 'h', 'i', 'z', 'e', '</s>'], ['<s>', 'z', 'o', 'o', 'm', 'o', 'r', 'p', 'h', 'i', 'z', 'e', '</s>']), (['<s>', '<3SG>', 'z', 'o', 'o', 'm', 'o', 'r', 'p', 'h', 'i', 'z', 'e', '</s>'], ['<s>', 'z', 'o', 'o', 'm', 'o', 'r', 'p', 'h', 'i', 'z', 'e', 's', '</s>']), (['<s>', '<PST>', 'z', 'o', 'o', 'm', 'o', 'r', 'p', 'h', 'i', 'z', 'e', '</s>'], ['<s>', 'z', 'o', 'o', 'm', 'o', 'r', 'p', 'h', 'i', 'z', 'e', 'd', '</s>']), (['<s>', '<PRS.PTCP>', 'z', 'o', 'o', 'm', 'o', 'r', 'p', 'h', 'i', 'z', 'e', '</s>'], ['<s>', 'z', 'o', 'o', 'm', 'o', 'r', 'p', 'h', 'i', 'z', 'i', 'n', 'g', '</s>']), (['<s>', '<PST.PTCP>', 'z', 'o', 'o', 'm', 'o', 'r', 'p', 'h', 'i', 'z', 'e', '</s>'], ['<s>', 'z', 'o', 'o', 'm', 'o', 'r', 'p', 'h', 'i', 'z', 'e', 'd', '</s>'])]\n",
      "Validation examples: [(['<s>', '<PRS>', 'c', 'o', 'u', 'n', 't', 'e', 'r', 'r', 'e', 'a', 'd', '</s>'], ['<s>', 'c', 'o', 'u', 'n', 't', 'e', 'r', 'r', 'e', 'a', 'd', '</s>']), (['<s>', '<3SG>', 'c', 'o', 'u', 'n', 't', 'e', 'r', 'r', 'e', 'a', 'd', '</s>'], ['<s>', 'c', 'o', 'u', 'n', 't', 'e', 'r', 'r', 'e', 'a', 'd', 's', '</s>']), (['<s>', '<PST>', 'c', 'o', 'u', 'n', 't', 'e', 'r', 'r', 'e', 'a', 'd', '</s>'], ['<s>', 'c', 'o', 'u', 'n', 't', 'e', 'r', 'r', 'e', 'a', 'd', '</s>']), (['<s>', '<PRS.PTCP>', 'c', 'o', 'u', 'n', 't', 'e', 'r', 'r', 'e', 'a', 'd', '</s>'], ['<s>', 'c', 'o', 'u', 'n', 't', 'e', 'r', 'r', 'e', 'a', 'd', 'i', 'n', 'g', '</s>']), (['<s>', '<PST.PTCP>', 'c', 'o', 'u', 'n', 't', 'e', 'r', 'r', 'e', 'a', 'd', '</s>'], ['<s>', 'c', 'o', 'u', 'n', 't', 'e', 'r', 'r', 'e', 'a', 'd', '</s>'])]\n"
     ]
    }
   ],
   "source": [
    "# Data preparation\n",
    "\n",
    "def prepare_data(data, val_size=1000, train_size=2000, test_ratio=0.4):\n",
    "    paradigms = list(data.items())\n",
    "    random.shuffle(paradigms)\n",
    "    \n",
    "    val_set = paradigms[:val_size]\n",
    "    train_test_set = paradigms[val_size:val_size + train_size]\n",
    "    \n",
    "    train_size = int((1 - test_ratio) * len(train_test_set))\n",
    "    train_set = train_test_set[:train_size]\n",
    "    test_set = train_test_set[train_size:]\n",
    "    \n",
    "    return train_set, test_set, val_set\n",
    "\n",
    "def generate_examples(paradigm):\n",
    "    lemma = list(paradigm[0])  # Convert lemma to list of characters\n",
    "    forms = paradigm[1]\n",
    "    examples = []\n",
    "    \n",
    "    for tag, form in forms.items():\n",
    "        src = ['<s>', f'<{tag}>'] + lemma + ['</s>']\n",
    "        tgt = ['<s>'] + list(form) + ['</s>']\n",
    "        examples.append((src, tgt))\n",
    "    \n",
    "    return examples\n",
    "\n",
    "# Load dataset\n",
    "with open(\"/home/minhk/Assignments/CSCI 5801/project/data/processed/eng_v.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "train_set, test_set, val_set = prepare_data(data, val_size=1000, train_size=1000, test_ratio=0.1)\n",
    "\n",
    "train_examples = [ex for paradigm in train_set for ex in generate_examples(paradigm)]\n",
    "test_examples = [ex for paradigm in test_set for ex in generate_examples(paradigm)]\n",
    "val_examples = [ex for paradigm in val_set for ex in generate_examples(paradigm)]\n",
    "\n",
    "print(\"Train examples:\", train_examples[:5])  # Show first 5 examples\n",
    "print(\"Test examples:\", test_examples[:5])\n",
    "print(\"Validation examples:\", val_examples[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from bisect import bisect_left\n",
    "def align_sequences(src, tgt):\n",
    "    # Scoring scheme\n",
    "    match_score = 1\n",
    "    mismatch_penalty = -3\n",
    "    gap_penalty = -2\n",
    "    \n",
    "    m, n = len(src), len(tgt)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    \n",
    "    # Initialize the DP table\n",
    "    for i in range(1, m + 1):\n",
    "        dp[i][0] = i * gap_penalty\n",
    "    for j in range(1, n + 1):\n",
    "        dp[0][j] = j * gap_penalty\n",
    "    \n",
    "    # Fill the DP table\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if src[i - 1] == tgt[j - 1]:\n",
    "                score = match_score\n",
    "            else:\n",
    "                score = mismatch_penalty\n",
    "            dp[i][j] = max(\n",
    "                dp[i - 1][j - 1] + score,  # Diagonal (match/mismatch)\n",
    "                dp[i - 1][j] + gap_penalty,  # Gap in target\n",
    "                dp[i][j - 1] + gap_penalty   # Gap in source\n",
    "            )\n",
    "    \n",
    "    # Backtrack to find the aligned sequences\n",
    "    aligned_src, aligned_tgt = [], []\n",
    "    i, j = m, n\n",
    "    while i > 0 and j > 0:\n",
    "        if src[i - 1] == tgt[j - 1] or dp[i][j] == dp[i - 1][j - 1] + mismatch_penalty:\n",
    "            aligned_src.append(src[i - 1])\n",
    "            aligned_tgt.append(tgt[j - 1])\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "        elif dp[i][j] == dp[i - 1][j] + gap_penalty:\n",
    "            aligned_src.append(src[i - 1])\n",
    "            aligned_tgt.append('#')\n",
    "            i -= 1\n",
    "        else:\n",
    "            aligned_src.append('#')\n",
    "            aligned_tgt.append(tgt[j - 1])\n",
    "            j -= 1\n",
    "    \n",
    "    # Add remaining characters\n",
    "    while i > 0:\n",
    "        aligned_src.append(src[i - 1])\n",
    "        aligned_tgt.append('#')\n",
    "        i -= 1\n",
    "    while j > 0:\n",
    "        aligned_src.append('#')\n",
    "        aligned_tgt.append(tgt[j - 1])\n",
    "        j -= 1\n",
    "    \n",
    "    # Reverse to get the correct order\n",
    "    aligned_src.reverse()\n",
    "    aligned_tgt.reverse()\n",
    "    \n",
    "    return aligned_src, aligned_tgt\n",
    "\n",
    "def hallucinate_trigram(src, tgt):\n",
    "    \"\"\"\n",
    "    Hallucinates an example from an existing one by replacing aligned trigrams with random characters.\n",
    "    \"\"\"\n",
    "    # Align the sequences first\n",
    "    tag = src[1]  # The morphological tag\n",
    "    src = src[2:-1]  # Remove <s>, morphological tag, and </s>\n",
    "    tgt = tgt[1:-1]  # Remove <s> and </s>\n",
    "    aligned_src, aligned_tgt = align_sequences(src, tgt)\n",
    "    n = len(aligned_src)\n",
    "\n",
    "    first_non_gap_src = max(\n",
    "        min(idx for idx, char in enumerate(aligned_src) if char != '#'),\n",
    "        min(idx for idx, char in enumerate(aligned_tgt) if char != '#')\n",
    "    )\n",
    "    last_non_gap_src = min(\n",
    "        max(idx for idx, char in enumerate(aligned_src) if char != '#'),\n",
    "        max(idx for idx, char in enumerate(aligned_tgt) if char != '#')\n",
    "    )\n",
    "    \n",
    "    # Find all valid trigrams (3 consecutive characters) that do not touch the ends\n",
    "    trigram_indices = []\n",
    "    for i in range(first_non_gap_src + 1, last_non_gap_src - 2):\n",
    "        if aligned_src[i] == aligned_tgt[i] and aligned_src[i + 1] == aligned_tgt[i + 1] and aligned_src[i + 2] == aligned_tgt[i + 2]:\n",
    "            trigram_indices.append(i)\n",
    "    # Shuffle the indices to randomize the hallucination\n",
    "    random.shuffle(trigram_indices)\n",
    "\n",
    "    # If alignment does not work, try to replace a single aligned character\n",
    "    if len(trigram_indices) == 0:\n",
    "        for i in range(first_non_gap_src, last_non_gap_src - 1):\n",
    "            if aligned_src[i] == aligned_tgt[i]:\n",
    "                vowels = set('aeiouy')\n",
    "                char = aligned_src[i]\n",
    "                if char in vowels:\n",
    "                    new_char = random.choice('aeiouy')\n",
    "                else:\n",
    "                    new_char = random.choice([c for c in string.ascii_lowercase if c not in 'aeiouy'])\n",
    "                aligned_src[i] = new_char\n",
    "                aligned_tgt[i] = new_char\n",
    "                break\n",
    "        else:\n",
    "            return None, None\n",
    "    \n",
    "    new_src = aligned_src\n",
    "    new_tgt = aligned_tgt\n",
    "    replaced = [False] * n\n",
    "    \n",
    "    # Randomly hallucinate some of these trigrams\n",
    "    for i in trigram_indices:\n",
    "        if not replaced[i] and not replaced[i + 1] and not replaced[i + 2]:\n",
    "            # Replace the trigram with random characters\n",
    "            for j in range(i, i + 3):\n",
    "                new_char = random.choice(string.ascii_lowercase)\n",
    "                new_src[j] = new_char\n",
    "                new_tgt[j] = new_char\n",
    "            replaced[i] = True\n",
    "            replaced[i + 1] = True\n",
    "            replaced[i + 2] = True\n",
    "    \n",
    "    new_src = ['<s>', tag] + [char for char in new_src if char != '#'] + ['</s>']\n",
    "    new_tgt = ['<s>'] + [char for char in new_tgt if char != '#'] + ['</s>']\n",
    "\n",
    "    return new_src, new_tgt\n",
    "\n",
    "orig_train_examples = copy.deepcopy(train_examples)\n",
    "def hallucinate_data(examples, hallucination_ratio=0.2, probs=None):\n",
    "    \"\"\"\n",
    "    Hallucinates data by replacing aligned trigrams with random characters.\n",
    "    Considers the probability of choosing examples for hallucination based on `probs`.\n",
    "    \"\"\"\n",
    "    new_examples = []\n",
    "    if probs is None:\n",
    "        probs = [1] * len(examples)  # Uniform probability if none provided\n",
    "    \n",
    "    # Normalize probabilities to sum to 1\n",
    "    total_prob = sum(probs) + 1\n",
    "    normalized_probs = [(p + 1 / len(probs)) / total_prob for p in probs]\n",
    "    \n",
    "    choices = random.choices(examples, weights=normalized_probs, k=int(len(examples) * hallucination_ratio * 1.1))\n",
    "    for example in choices:\n",
    "        if len(new_examples) >= len(examples) * hallucination_ratio:\n",
    "            break\n",
    "        # Choose an example based on the normalized probabilities\n",
    "        src, tgt = hallucinate_trigram(example[0], example[1])\n",
    "        if src is not None and tgt is not None:\n",
    "            new_examples.append((src, tgt))\n",
    "    return new_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: <s><3SG>reskim</s> <s>reskims</s>\n",
      "Hallucinated: <s><3SG>rgluim</s> <s>rgluims</s>\n",
      "\n",
      "Original: <s><PRS.PTCP>beshape</s> <s>beshaping</s>\n",
      "Hallucinated: <s><PRS.PTCP>bokmape</s> <s>bokmaping</s>\n",
      "\n",
      "Original: <s><PST.PTCP>cassate</s> <s>cassated</s>\n",
      "Hallucinated: <s><PST.PTCP>casooee</s> <s>casooeed</s>\n",
      "\n",
      "Original: <s><PST>aviate</s> <s>aviated</s>\n",
      "Hallucinated: <s><PST>avmswe</s> <s>avmswed</s>\n",
      "\n",
      "Original: <s><PST.PTCP>uncause</s> <s>uncaused</s>\n",
      "Hallucinated: <s><PST.PTCP>ujsquse</s> <s>ujsqused</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the hallucination function\n",
    "for _ in range(5):\n",
    "    # Randomly select an example from the training set\n",
    "    example = random.choice(orig_train_examples)\n",
    "    src, tgt = hallucinate_trigram(example[0], example[1])\n",
    "    print(\"Original:\", \"\".join(example[0]), \"\".join(example[1]))\n",
    "    if src is not None and tgt is not None:\n",
    "        print(\"Hallucinated:\", \"\".join(src), \"\".join(tgt))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InverseSquareLRWithWarmup(LRScheduler):\n",
    "    \"\"\"\n",
    "    Implements an inverse square learning rate scheduler with warmup steps.\n",
    "    \n",
    "    During warmup, the learning rate increases linearly from init_lr to max_lr.\n",
    "    After warmup, the learning rate decreases according to the inverse square of the step number:\n",
    "    lr = max_lr * (warmup_steps / step)^2 for step > warmup_steps\n",
    "    \n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        init_lr (float): Initial learning rate during warmup phase. Default: 0.0\n",
    "        max_lr (float): Maximum learning rate after warmup phase. Default: 0.1\n",
    "        warmup_steps (int): Number of warmup steps. Default: 1000\n",
    "        last_epoch (int): The index of the last epoch. Default: -1\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer, init_lr=0.0, max_lr=0.001, warmup_steps=1000, last_epoch=-1):\n",
    "        self.init_lr = init_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        super(InverseSquareLRWithWarmup, self).__init__(optimizer, last_epoch)\n",
    "    \n",
    "    def is_warmed_up(self):\n",
    "        return self.last_epoch >= self.warmup_steps\n",
    "        \n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\")\n",
    "        \n",
    "        if self.last_epoch < self.warmup_steps:\n",
    "            # Linear warmup phase\n",
    "            alpha = self.last_epoch / self.warmup_steps\n",
    "            return [self.init_lr + alpha * (self.max_lr - self.init_lr) for _ in self.base_lrs]\n",
    "        else:\n",
    "            # Inverse square decay phase\n",
    "            decay_factor = math.sqrt(self.warmup_steps / self.last_epoch)\n",
    "            return [self.max_lr * decay_factor for _ in self.base_lrs]\n",
    "            \n",
    "    def _get_closed_form_lr(self):\n",
    "        if self.last_epoch < self.warmup_steps:\n",
    "            # Linear warmup phase\n",
    "            alpha = self.last_epoch / self.warmup_steps\n",
    "            return [self.init_lr + alpha * (self.max_lr - self.init_lr) for _ in self.base_lrs]\n",
    "        else:\n",
    "            # Inverse square decay phase\n",
    "            decay_factor = (self.warmup_steps / self.last_epoch) ** 2\n",
    "            return [self.max_lr * decay_factor for _ in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampler for scheduled learning, gradually replaces ground truth (teacher forcing) with model input\n",
    "\n",
    "class ScheduledSampler():\n",
    "    def __init__(self, base_rate=0.5, warmup_steps=1000):\n",
    "        self.base_rate = base_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.step_count = 0        \n",
    "        self.sampling_rate = 1\n",
    "    \n",
    "    def step(self):\n",
    "        self.step_count += 1\n",
    "        if self.step_count > self.warmup_steps:\n",
    "            self.sampling_rate = self.base_rate + (1 - self.base_rate) * math.sqrt(self.warmup_steps / self.step_count)\n",
    "\n",
    "    def sample(self, logits, truth_ids):\n",
    "        \"\"\"\n",
    "        Selects truth_ids with probability `sampling_rate`, \n",
    "        otherwise samples using Gumbel noise.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, vocab_size = logits.shape\n",
    "        \n",
    "        # Decide per-token whether to take ground truth (1) or Gumbel sample (0)\n",
    "        mask = torch.bernoulli(torch.full((batch_size, seq_len), self.sampling_rate, device=logits.device, dtype=float)).bool()\n",
    "        \n",
    "        # Gumbel-sampled predictions\n",
    "        gumbel_preds = self._gumbel_sample(logits)\n",
    "        \n",
    "        # Use ground truth where mask == True, else use gumbel_preds\n",
    "        return torch.where(mask, truth_ids, gumbel_preds)\n",
    "    \n",
    "    def _gumbel_sample(self, logits):\n",
    "        gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits)))  # Generate Gumbel noise\n",
    "        return (logits + gumbel_noise).argmax(dim=-1)  # Apply Gumbel noise and take the argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'œ', '6', 'j', 'b', '*', 'k', 'g', '/', 'T', 'u', '<PST>', 'i', 'r', 't', '8', 'è', 's', '9', '<PRS.PTCP>', 'q', 'l', '4', ' ', 'w', 'é', 'v', 'æ', 'ä', '’', '<PRS>', 'o', '<3SG>', 'ö', 'û', 'n', '<PST.PTCP>', 'a', 'f', 'h', 'p', '-', 'c', 'R', \"'\", 'ê', 'ë', 'x', '0', '1', 'y', 'E', 'ï', 'd', 'e', 'z', 'm', 'U'}\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "def tokenize(sequence, char_to_idx):\n",
    "    return [char_to_idx[char] for char in sequence]\n",
    "\n",
    "# Build character vocabulary\n",
    "all_chars = set()\n",
    "for word, inflect in data.items():\n",
    "    all_chars.update(word)\n",
    "    for tag, forms in inflect.items():\n",
    "        all_chars.add(f\"<{tag}>\")\n",
    "        all_chars.update(forms)\n",
    "print(all_chars)\n",
    "all_alphabet_chars = {char for char in all_chars if not (char.startswith('<') and char.endswith('>'))}\n",
    "char_to_idx = {char: i for i, char in enumerate(sorted(all_chars), start=3)}  # Reserve 0, 1, 2 for special tokens\n",
    "char_to_idx['<pad>'] = 0\n",
    "char_to_idx['<s>'] = 1\n",
    "char_to_idx['</s>'] = 2\n",
    "idx_to_char = {\n",
    "    i: char for char, i in char_to_idx.items()\n",
    "}\n",
    "vocab_size = len(char_to_idx)\n",
    "max_len = 32\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CharTransformer(vocab_size, device=device, max_len=max_len)\n",
    "\n",
    "def pad_sequence(sequence, max_len, pad_token='<pad>'):\n",
    "    return sequence + [pad_token] * (max_len - len(sequence))\n",
    "\n",
    "def create_feature_mask(sequence):\n",
    "    \"\"\"Create a feature mask where tags (enclosed in < >) are marked as 1, else 0.\"\"\"\n",
    "    return torch.tensor([1 if char.startswith('<') and char.endswith('>') else 0 for char in sequence], device=device)\n",
    "\n",
    "def create_padding_mask(sequence, pad_token='<pad>'):\n",
    "    \"\"\"Create a padding mask where padding tokens are marked as True (to be ignored).\"\"\"\n",
    "    return (sequence == pad_token)\n",
    "\n",
    "def train_model(model, train_examples, test_examples, epochs=1000, batch_size=256, \n",
    "                patience=20, \n",
    "                hallucination_ratio=0.2, hallucination_refresh_rate=5, stop_hallucinating_after=0.9):\n",
    "    optimizer = optim.AdamW(model.parameters(), betas=(0.99, 0.98))\n",
    "    scheduler = InverseSquareLRWithWarmup(optimizer, init_lr=1e-5, max_lr=1e-3, warmup_steps=4000)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1, reduction='none')\n",
    "    # sampler = ScheduledSampler(base_rate=0.5, warmup_steps=4000)\n",
    "\n",
    "    orig_train_examples = copy.deepcopy(train_examples)\n",
    "    u_train_examples = orig_train_examples\n",
    "    order = [i for i in range(len(train_examples))]\n",
    "    # losses = [0] * len(train_examples)\n",
    "\n",
    "    pad_token = char_to_idx['<pad>']\n",
    "    best_test_loss = float('inf')  # Initialize the best test loss to a very large value\n",
    "    best_model_state = copy.deepcopy(model.state_dict())  # Store best model parameters\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        if epoch == int(stop_hallucinating_after * epochs):\n",
    "            # Stop hallucinating after a certain epoch\n",
    "            u_train_examples = orig_train_examples\n",
    "            order = [i for i in range(len(u_train_examples))]\n",
    "            #  = [0] * len(u_train_examples)\n",
    "        elif epoch % hallucination_refresh_rate == 0 and scheduler.is_warmed_up() and epoch < int(stop_hallucinating_after * epochs):\n",
    "            u_train_examples = orig_train_examples + hallucinate_data(orig_train_examples, hallucination_ratio) # losses[:len(orig_train_examples)])\n",
    "            order = [i for i in range(len(u_train_examples))]\n",
    "            # losses = [0] * len(u_train_examples)\n",
    "            \n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        random.shuffle(order)\n",
    "        train_examples = [u_train_examples[order[i]] for i in range(len(u_train_examples))]\n",
    "        print(len(u_train_examples))\n",
    "        \n",
    "        for i in range(0, len(train_examples), batch_size):\n",
    "            batch = train_examples[i:i+batch_size]\n",
    "            src_batch, tgt_batch = zip(*batch)\n",
    "\n",
    "            max_batch_len = max(max(len(s) for s in src_batch), max(len(t) for t in tgt_batch))\n",
    "            \n",
    "            # Pad sequences to the maximum length (max_len) in the batch\n",
    "            src_padded = [pad_sequence(seq, max_batch_len) for seq in src_batch]\n",
    "            tgt_padded = [pad_sequence(seq, max_batch_len) for seq in tgt_batch]\n",
    "            \n",
    "            # Convert padded sequences to tensors\n",
    "            src_tensor = torch.tensor([tokenize(seq, char_to_idx) for seq in src_padded], device=device)\n",
    "            tgt_tensor = torch.tensor([tokenize(seq, char_to_idx) for seq in tgt_padded], device=device)\n",
    "            \n",
    "            # Create the feature mask\n",
    "            feature_mask_src = torch.stack([create_feature_mask(seq) for seq in src_padded], dim=0)\n",
    "\n",
    "            # Shift target tensor for teacher forcing (the model will predict next token)\n",
    "            tgt_input = tgt_tensor[:, :-1]  # Remove the last token (it's not used as input)\n",
    "            tgt_expected = tgt_tensor[:, 1:]  # The target sequence for the loss is shifted by 1\n",
    "\n",
    "            # First round of predictions (using teacher forcing)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src_tensor, tgt_input, feature_mask_src, tgt_is_causal=True)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(output.reshape(-1, vocab_size), tgt_expected.reshape(-1))\n",
    "\n",
    "            # Apply padding mask to loss (ignores padded tokens)\n",
    "            tgt_mask = (tgt_input != pad_token).float().view(-1)\n",
    "            loss = loss * tgt_mask  # Element-wise multiply with the mask to ignore padding tokens\n",
    "\n",
    "            # Reshape loss and mask back to batch and sequence dimensions\n",
    "            loss = loss.view(-1, max_batch_len-1)\n",
    "            tgt_mask = tgt_mask.view(-1, max_batch_len-1)\n",
    "\n",
    "            # Calculate the average loss per word in the batch\n",
    "            word_loss = loss.sum(dim=1) / tgt_mask.sum(dim=1)  # Average loss per word\n",
    "            # if not torch.isnan(word_loss).any():\n",
    "            #     for j in range(len(batch)):\n",
    "            #        losses[order[i+j]] += word_loss[j].item()\n",
    "\n",
    "\n",
    "            # Normalize the total loss (average over non-padding tokens)\n",
    "            loss = loss.sum() / tgt_mask.sum()\n",
    "\n",
    "            # Apply loss\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            # sampler.step()  # Update the sampler (teacher forcing rate)\n",
    "\n",
    "            total_loss += loss.item() * len(batch)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_examples)}\")\n",
    "        \n",
    "        # Evaluate on the test set\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(test_examples), batch_size):\n",
    "                batch = test_examples[i:i+batch_size]\n",
    "                src_batch, tgt_batch = zip(*batch)\n",
    "\n",
    "                max_batch_len = max(max(len(s) for s in src_batch), max(len(t) for t in tgt_batch))\n",
    "\n",
    "                # Pad sequences to the maximum length (max_len) in the batch\n",
    "                src_padded = [pad_sequence(seq, max_batch_len) for seq in src_batch]\n",
    "                tgt_padded = [pad_sequence(seq, max_batch_len) for seq in tgt_batch]\n",
    "\n",
    "                # Convert padded sequences to tensors\n",
    "                src_tensor = torch.tensor([tokenize(seq, char_to_idx) for seq in src_padded], device=device)\n",
    "                tgt_tensor = torch.tensor([tokenize(seq, char_to_idx) for seq in tgt_padded], device=device)\n",
    "\n",
    "                # Create the feature mask\n",
    "                feature_mask_src = torch.stack([create_feature_mask(seq) for seq in src_padded], dim=0)\n",
    "\n",
    "                # Shift target tensor\n",
    "                tgt_input = tgt_tensor[:, :-1]\n",
    "                tgt_expected = tgt_tensor[:, 1:]\n",
    "\n",
    "                # Forward pass\n",
    "                output = model(src_tensor, tgt_input, feature_mask_src, tgt_is_causal=True)\n",
    "\n",
    "                # Compute the loss\n",
    "                loss = criterion(output.reshape(-1, vocab_size), tgt_expected.reshape(-1))\n",
    "\n",
    "                # Apply padding mask to loss\n",
    "                tgt_mask = (tgt_expected != pad_token).float().view(-1)\n",
    "                loss = loss * tgt_mask  # Element-wise multiply with the mask to ignore padding tokens\n",
    "                loss = loss.sum() / tgt_mask.sum()  # Normalize the loss (average over non-padding tokens)\n",
    "                \n",
    "                test_loss += loss.item() * len(batch)\n",
    "\n",
    "        test_loss = test_loss / len(test_examples)\n",
    "        print(f\"Test Loss after Epoch {epoch+1}: {test_loss}\")\n",
    "\n",
    "        # Early stopping based on test set loss\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            best_epoch = epoch\n",
    "            patience_count = 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())  # Save best model state\n",
    "        else:\n",
    "            patience_count += 1\n",
    "            if patience_count == patience:\n",
    "                patience_count = 0\n",
    "                patience = int(patience * math.sqrt(2))\n",
    "                # Rollback to best model state (undo last epoch update)\n",
    "                model.load_state_dict(best_model_state)\n",
    "                print(f\"Rolling back to best model from epoch {best_epoch + 1}\")\n",
    "                print(f\"Best test loss: {best_test_loss}\")\n",
    "        \n",
    "    model.load_state_dict(best_model_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set, val_set = prepare_data(data, val_size=100, train_size=5000, test_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_word(arr):\n",
    "    return ''.join(arr[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 4500 500\n",
      "4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhk/Assignments/CSCI 5801/project/.venv/lib/python3.12/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.067972715059916\n",
      "Test Loss after Epoch 1: 3.736744210243225\n",
      "4500\n",
      "Epoch 2, Loss: 3.7102280310524836\n",
      "Test Loss after Epoch 2: 3.5517073907852175\n",
      "4500\n",
      "Epoch 3, Loss: 3.5272643058564928\n",
      "Test Loss after Epoch 3: 3.445409442901611\n",
      "4500\n",
      "Epoch 4, Loss: 3.419533242755466\n",
      "Test Loss after Epoch 4: 3.3505896120071412\n",
      "4500\n",
      "Epoch 5, Loss: 3.348769915368822\n",
      "Test Loss after Epoch 5: 3.2837056665420534\n",
      "4500\n",
      "Epoch 6, Loss: 3.2970220947265627\n",
      "Test Loss after Epoch 6: 3.231209547042847\n",
      "4500\n",
      "Epoch 7, Loss: 3.246455425898234\n",
      "Test Loss after Epoch 7: 3.1726335897445677\n",
      "4500\n",
      "Epoch 8, Loss: 3.1757055989371406\n",
      "Test Loss after Epoch 8: 3.0838602142333986\n",
      "4500\n",
      "Epoch 9, Loss: 3.065431721581353\n",
      "Test Loss after Epoch 9: 2.9411994962692263\n",
      "4500\n",
      "Epoch 10, Loss: 2.9672621830834283\n",
      "Test Loss after Epoch 10: 2.8451403369903563\n",
      "4500\n",
      "Epoch 11, Loss: 2.8526019938786824\n",
      "Test Loss after Epoch 11: 2.8165315923690795\n",
      "4500\n",
      "Epoch 12, Loss: 2.769704253938463\n",
      "Test Loss after Epoch 12: 2.7295224609375\n",
      "4500\n",
      "Epoch 13, Loss: 2.6906890058517456\n",
      "Test Loss after Epoch 13: 2.6763406944274903\n",
      "4500\n",
      "Epoch 14, Loss: 2.619119060728285\n",
      "Test Loss after Epoch 14: 2.638262376785278\n",
      "4500\n",
      "Epoch 15, Loss: 2.561889758957757\n",
      "Test Loss after Epoch 15: 2.614437710762024\n",
      "4500\n",
      "Epoch 16, Loss: 2.504785224702623\n",
      "Test Loss after Epoch 16: 2.568110327720642\n",
      "4500\n",
      "Epoch 17, Loss: 2.456053403324551\n",
      "Test Loss after Epoch 17: 2.5377042894363404\n",
      "4500\n",
      "Epoch 18, Loss: 2.4065370615853205\n",
      "Test Loss after Epoch 18: 2.512779457092285\n",
      "4500\n",
      "Epoch 19, Loss: 2.3647130568822226\n",
      "Test Loss after Epoch 19: 2.4779648275375368\n",
      "4500\n",
      "Epoch 20, Loss: 2.3236482293870715\n",
      "Test Loss after Epoch 20: 2.4492188482284547\n",
      "4500\n",
      "Epoch 21, Loss: 2.2914488985273573\n",
      "Test Loss after Epoch 21: 2.4134834613800047\n",
      "4500\n",
      "Epoch 22, Loss: 2.261597144656711\n",
      "Test Loss after Epoch 22: 2.392451418876648\n",
      "4500\n",
      "Epoch 23, Loss: 2.2333248353534274\n",
      "Test Loss after Epoch 23: 2.3826259899139406\n",
      "4500\n",
      "Epoch 24, Loss: 2.2068809311125013\n",
      "Test Loss after Epoch 24: 2.334962357521057\n",
      "4500\n",
      "Epoch 25, Loss: 2.176501108063592\n",
      "Test Loss after Epoch 25: 2.3023471870422365\n",
      "4500\n",
      "Epoch 26, Loss: 2.1448646460639105\n",
      "Test Loss after Epoch 26: 2.2736056928634643\n",
      "4500\n",
      "Epoch 27, Loss: 2.1219695217344494\n",
      "Test Loss after Epoch 27: 2.2512601442337035\n",
      "4500\n",
      "Epoch 28, Loss: 2.096623040729099\n",
      "Test Loss after Epoch 28: 2.23100075340271\n",
      "4500\n",
      "Epoch 29, Loss: 2.0692581981023155\n",
      "Test Loss after Epoch 29: 2.2245702962875367\n",
      "4500\n",
      "Epoch 30, Loss: 2.0457570367919073\n",
      "Test Loss after Epoch 30: 2.1860127878189086\n",
      "4500\n",
      "Epoch 31, Loss: 2.020554359489017\n",
      "Test Loss after Epoch 31: 2.1696767930984495\n",
      "4500\n",
      "Epoch 32, Loss: 1.996768839624193\n",
      "Test Loss after Epoch 32: 2.1270919246673583\n",
      "4500\n",
      "Epoch 33, Loss: 1.969841338687473\n",
      "Test Loss after Epoch 33: 2.120088713645935\n",
      "4500\n",
      "Epoch 34, Loss: 1.944097474998898\n",
      "Test Loss after Epoch 34: 2.0902480940818786\n",
      "4500\n",
      "Epoch 35, Loss: 1.924150864813063\n",
      "Test Loss after Epoch 35: 2.0687183699607847\n",
      "4500\n",
      "Epoch 36, Loss: 1.9050447993808322\n",
      "Test Loss after Epoch 36: 2.0403554248809814\n",
      "4500\n",
      "Epoch 37, Loss: 1.8816156282954746\n",
      "Test Loss after Epoch 37: 2.0219099898338317\n",
      "4500\n",
      "Epoch 38, Loss: 1.8609162762959799\n",
      "Test Loss after Epoch 38: 1.9884223456382752\n",
      "4500\n",
      "Epoch 39, Loss: 1.8380723196135627\n",
      "Test Loss after Epoch 39: 1.9849662675857545\n",
      "4500\n",
      "Epoch 40, Loss: 1.8100125223795573\n",
      "Test Loss after Epoch 40: 1.9579036269187928\n",
      "4500\n",
      "Epoch 41, Loss: 1.7924803759257\n",
      "Test Loss after Epoch 41: 1.9658197250366212\n",
      "4500\n",
      "Epoch 42, Loss: 1.7713266905148823\n",
      "Test Loss after Epoch 42: 1.9461974711418153\n",
      "4500\n",
      "Epoch 43, Loss: 1.749791787677341\n",
      "Test Loss after Epoch 43: 1.9532372784614562\n",
      "4500\n",
      "Epoch 44, Loss: 1.72840184519026\n",
      "Test Loss after Epoch 44: 1.9363577842712403\n",
      "4500\n",
      "Epoch 45, Loss: 1.7086433209313288\n",
      "Test Loss after Epoch 45: 1.939458158493042\n",
      "4500\n",
      "Epoch 46, Loss: 1.6916713513268364\n",
      "Test Loss after Epoch 46: 1.914090181827545\n",
      "4500\n",
      "Epoch 47, Loss: 1.667137879530589\n",
      "Test Loss after Epoch 47: 1.9086347699165345\n",
      "4500\n",
      "Epoch 48, Loss: 1.6495275905927023\n",
      "Test Loss after Epoch 48: 1.8667108368873597\n",
      "4500\n",
      "Epoch 49, Loss: 1.6247135735617744\n",
      "Test Loss after Epoch 49: 1.861622663974762\n",
      "4500\n",
      "Epoch 50, Loss: 1.6045800059106614\n",
      "Test Loss after Epoch 50: 1.823946702003479\n",
      "4500\n",
      "Epoch 51, Loss: 1.5820398219426473\n",
      "Test Loss after Epoch 51: 1.8021325874328613\n",
      "4500\n",
      "Epoch 52, Loss: 1.5555544389088949\n",
      "Test Loss after Epoch 52: 1.7682379765510559\n",
      "4500\n",
      "Epoch 53, Loss: 1.5316210451126098\n",
      "Test Loss after Epoch 53: 1.7408939819335938\n",
      "4500\n",
      "Epoch 54, Loss: 1.5022688035964966\n",
      "Test Loss after Epoch 54: 1.7140375471115112\n",
      "4500\n",
      "Epoch 55, Loss: 1.477537453810374\n",
      "Test Loss after Epoch 55: 1.667318311214447\n",
      "4500\n",
      "Epoch 56, Loss: 1.4497596219380697\n",
      "Test Loss after Epoch 56: 1.6147134370803833\n",
      "4500\n",
      "Epoch 57, Loss: 1.4306582127147252\n",
      "Test Loss after Epoch 57: 1.5786936235427858\n",
      "4500\n",
      "Epoch 58, Loss: 1.4108624162144132\n",
      "Test Loss after Epoch 58: 1.5222181506156922\n",
      "4500\n",
      "Epoch 59, Loss: 1.381594725873735\n",
      "Test Loss after Epoch 59: 1.4748671927452088\n",
      "4500\n",
      "Epoch 60, Loss: 1.367562273343404\n",
      "Test Loss after Epoch 60: 1.4554480652809143\n",
      "4500\n",
      "Epoch 61, Loss: 1.3515918414857653\n",
      "Test Loss after Epoch 61: 1.4127657747268676\n",
      "4500\n",
      "Epoch 62, Loss: 1.3185680780410767\n",
      "Test Loss after Epoch 62: 1.385074357509613\n",
      "4500\n",
      "Epoch 63, Loss: 1.2992126908302306\n",
      "Test Loss after Epoch 63: 1.3426243829727174\n",
      "4500\n",
      "Epoch 64, Loss: 1.276657143804762\n",
      "Test Loss after Epoch 64: 1.3402228851318359\n",
      "4500\n",
      "Epoch 65, Loss: 1.2545318199793498\n",
      "Test Loss after Epoch 65: 1.299083885192871\n",
      "4500\n",
      "Epoch 66, Loss: 1.2401339356104533\n",
      "Test Loss after Epoch 66: 1.259852111339569\n",
      "4500\n",
      "Epoch 67, Loss: 1.2167627977265252\n",
      "Test Loss after Epoch 67: 1.2523399114608764\n",
      "4500\n",
      "Epoch 68, Loss: 1.2025033500459459\n",
      "Test Loss after Epoch 68: 1.2248489742279052\n",
      "4500\n",
      "Epoch 69, Loss: 1.1821873337957594\n",
      "Test Loss after Epoch 69: 1.2225152473449707\n",
      "4500\n",
      "Epoch 70, Loss: 1.1682726652887132\n",
      "Test Loss after Epoch 70: 1.1946732087135314\n",
      "4500\n",
      "Epoch 71, Loss: 1.1479908145798576\n",
      "Test Loss after Epoch 71: 1.195976167678833\n",
      "4500\n",
      "Epoch 72, Loss: 1.1348546146816678\n",
      "Test Loss after Epoch 72: 1.1642623739242555\n",
      "4500\n",
      "Epoch 73, Loss: 1.1191977756818137\n",
      "Test Loss after Epoch 73: 1.1604455146789552\n",
      "4500\n",
      "Epoch 74, Loss: 1.1046812799241807\n",
      "Test Loss after Epoch 74: 1.1324479222297668\n",
      "4500\n",
      "Epoch 75, Loss: 1.0874860983424717\n",
      "Test Loss after Epoch 75: 1.1300368785858155\n",
      "4500\n",
      "Epoch 76, Loss: 1.0800136881934272\n",
      "Test Loss after Epoch 76: 1.1172158789634705\n",
      "4500\n",
      "Epoch 77, Loss: 1.0634172534412807\n",
      "Test Loss after Epoch 77: 1.102632174015045\n",
      "4500\n",
      "Epoch 78, Loss: 1.0491840640174017\n",
      "Test Loss after Epoch 78: 1.0875725140571595\n",
      "4500\n",
      "Epoch 79, Loss: 1.0362722056176927\n",
      "Test Loss after Epoch 79: 1.0829507217407226\n",
      "4500\n",
      "Epoch 80, Loss: 1.0276315097279018\n",
      "Test Loss after Epoch 80: 1.0768488221168517\n",
      "4500\n",
      "Epoch 81, Loss: 1.0157619646125369\n",
      "Test Loss after Epoch 81: 1.0683806920051575\n",
      "4500\n",
      "Epoch 82, Loss: 1.0095636880927616\n",
      "Test Loss after Epoch 82: 1.0772440304756166\n",
      "4500\n",
      "Epoch 83, Loss: 0.9989475229051378\n",
      "Test Loss after Epoch 83: 1.0764845728874206\n",
      "4500\n",
      "Epoch 84, Loss: 0.9847831809785631\n",
      "Test Loss after Epoch 84: 1.0722975010871887\n",
      "4500\n",
      "Epoch 85, Loss: 0.9850758666462368\n",
      "Test Loss after Epoch 85: 1.0482068219184875\n",
      "4500\n",
      "Epoch 86, Loss: 0.9714265241622925\n",
      "Test Loss after Epoch 86: 1.0580088033676147\n",
      "4500\n",
      "Epoch 87, Loss: 0.9653578481674194\n",
      "Test Loss after Epoch 87: 1.052199677467346\n",
      "4500\n",
      "Epoch 88, Loss: 0.9578307454850938\n",
      "Test Loss after Epoch 88: 1.0486693186759948\n",
      "4500\n",
      "Epoch 89, Loss: 0.9453665195835961\n",
      "Test Loss after Epoch 89: 1.0269366364479064\n",
      "4500\n",
      "Epoch 90, Loss: 0.9378098635143703\n",
      "Test Loss after Epoch 90: 1.0108379473686218\n",
      "4500\n",
      "Epoch 91, Loss: 0.9307060354550679\n",
      "Test Loss after Epoch 91: 1.0094322080612184\n",
      "4500\n",
      "Epoch 92, Loss: 0.9257533850140042\n",
      "Test Loss after Epoch 92: 1.002051878452301\n",
      "4500\n",
      "Epoch 93, Loss: 0.9175699742370181\n",
      "Test Loss after Epoch 93: 0.996157066822052\n",
      "4500\n",
      "Epoch 94, Loss: 0.9173199615478516\n",
      "Test Loss after Epoch 94: 0.998953531742096\n",
      "4500\n",
      "Epoch 95, Loss: 0.9093772937456767\n",
      "Test Loss after Epoch 95: 0.9949629294872284\n",
      "4500\n",
      "Epoch 96, Loss: 0.9013626831902398\n",
      "Test Loss after Epoch 96: 0.9751013040542602\n",
      "4500\n",
      "Epoch 97, Loss: 0.8984580696953668\n",
      "Test Loss after Epoch 97: 0.9724343457221984\n",
      "4500\n",
      "Epoch 98, Loss: 0.8937222856150733\n",
      "Test Loss after Epoch 98: 0.9818505384922027\n",
      "4500\n",
      "Epoch 99, Loss: 0.8892438733047909\n",
      "Test Loss after Epoch 99: 0.9787331297397613\n",
      "4500\n",
      "Epoch 100, Loss: 0.8872196242279476\n",
      "Test Loss after Epoch 100: 0.9611792473793029\n",
      "4500\n",
      "Epoch 101, Loss: 0.8800608819590674\n",
      "Test Loss after Epoch 101: 0.9649091827869415\n",
      "4500\n",
      "Epoch 102, Loss: 0.8775329168902503\n",
      "Test Loss after Epoch 102: 0.9629427127838135\n",
      "4500\n",
      "Epoch 103, Loss: 0.8739461141162449\n",
      "Test Loss after Epoch 103: 0.9704618835449219\n",
      "4500\n",
      "Epoch 104, Loss: 0.8703387929068671\n",
      "Test Loss after Epoch 104: 0.9674481019973755\n",
      "4500\n",
      "Epoch 105, Loss: 0.8643515528043111\n",
      "Test Loss after Epoch 105: 0.9454052913188934\n",
      "4500\n",
      "Epoch 106, Loss: 0.8622922305795881\n",
      "Test Loss after Epoch 106: 0.940326984167099\n",
      "4500\n",
      "Epoch 107, Loss: 0.8604401700496673\n",
      "Test Loss after Epoch 107: 0.9465468029975891\n",
      "4500\n",
      "Epoch 108, Loss: 0.8594875072373285\n",
      "Test Loss after Epoch 108: 0.9449985313415528\n",
      "4500\n",
      "Epoch 109, Loss: 0.8513515393733978\n",
      "Test Loss after Epoch 109: 0.9426502516269684\n",
      "4500\n",
      "Epoch 110, Loss: 0.8521033019224803\n",
      "Test Loss after Epoch 110: 0.9346927292346955\n",
      "4500\n",
      "Epoch 111, Loss: 0.8495331127378676\n",
      "Test Loss after Epoch 111: 0.9434848656654358\n",
      "4500\n",
      "Epoch 112, Loss: 0.8441029391288757\n",
      "Test Loss after Epoch 112: 0.9404793803691864\n",
      "4500\n",
      "Epoch 113, Loss: 0.8421969628334045\n",
      "Test Loss after Epoch 113: 0.9442043375968933\n",
      "4500\n",
      "Epoch 114, Loss: 0.8389767649968465\n",
      "Test Loss after Epoch 114: 0.9264258925914765\n",
      "4500\n",
      "Epoch 115, Loss: 0.8375267946455214\n",
      "Test Loss after Epoch 115: 0.9237581982612609\n",
      "4500\n",
      "Epoch 116, Loss: 0.8310366174909803\n",
      "Test Loss after Epoch 116: 0.9299670946598053\n",
      "4500\n",
      "Epoch 117, Loss: 0.832258504735099\n",
      "Test Loss after Epoch 117: 0.9363909299373627\n",
      "4500\n",
      "Epoch 118, Loss: 0.8299232219854991\n",
      "Test Loss after Epoch 118: 0.9163641579151154\n",
      "4500\n",
      "Epoch 119, Loss: 0.8281522099441953\n",
      "Test Loss after Epoch 119: 0.9127697923183441\n",
      "4500\n",
      "Epoch 120, Loss: 0.8241637335883246\n",
      "Test Loss after Epoch 120: 0.9136031637191773\n",
      "4500\n",
      "Epoch 121, Loss: 0.8255252315998077\n",
      "Test Loss after Epoch 121: 0.9105783619880676\n",
      "4500\n",
      "Epoch 122, Loss: 0.8190576366583506\n",
      "Test Loss after Epoch 122: 0.9094665741920471\n",
      "4500\n",
      "Epoch 123, Loss: 0.8207318659623464\n",
      "Test Loss after Epoch 123: 0.9116836187839508\n",
      "4500\n",
      "Epoch 124, Loss: 0.8195189961062538\n",
      "Test Loss after Epoch 124: 0.920273686170578\n",
      "4500\n",
      "Epoch 125, Loss: 0.8199549684524536\n",
      "Test Loss after Epoch 125: 0.9146804268360138\n",
      "4500\n",
      "Epoch 126, Loss: 0.815957720067766\n",
      "Test Loss after Epoch 126: 0.9128582813739776\n",
      "4500\n",
      "Epoch 127, Loss: 0.814881805340449\n",
      "Test Loss after Epoch 127: 0.915369270324707\n",
      "4500\n",
      "Epoch 128, Loss: 0.8112167719205221\n",
      "Test Loss after Epoch 128: 0.9236516368389129\n",
      "4500\n",
      "Epoch 129, Loss: 0.8098294905821483\n",
      "Test Loss after Epoch 129: 0.9119984169006348\n",
      "4500\n",
      "Epoch 130, Loss: 0.8089005536238353\n",
      "Test Loss after Epoch 130: 0.9053184571266174\n",
      "4500\n",
      "Epoch 131, Loss: 0.8108710808753967\n",
      "Test Loss after Epoch 131: 0.8994717285633087\n",
      "4500\n",
      "Epoch 132, Loss: 0.806918269528283\n",
      "Test Loss after Epoch 132: 0.9035941917896271\n",
      "4500\n",
      "Epoch 133, Loss: 0.8050482155217065\n",
      "Test Loss after Epoch 133: 0.9037816524505615\n",
      "4500\n",
      "Epoch 134, Loss: 0.8060372029410469\n",
      "Test Loss after Epoch 134: 0.8993036787509918\n",
      "4500\n",
      "Epoch 135, Loss: 0.8035954704549577\n",
      "Test Loss after Epoch 135: 0.9089554405212402\n",
      "4500\n",
      "Epoch 136, Loss: 0.8046540067990621\n",
      "Test Loss after Epoch 136: 0.9123232975006104\n",
      "4500\n",
      "Epoch 137, Loss: 0.8061684317853716\n",
      "Test Loss after Epoch 137: 0.9132489864826202\n",
      "4500\n",
      "Epoch 138, Loss: 0.8045600831243727\n",
      "Test Loss after Epoch 138: 0.907844051361084\n",
      "4500\n",
      "Epoch 139, Loss: 0.8031334915426043\n",
      "Test Loss after Epoch 139: 0.9102420451641082\n",
      "4500\n",
      "Epoch 140, Loss: 0.801840542183982\n",
      "Test Loss after Epoch 140: 0.9001837828159333\n",
      "4500\n",
      "Epoch 141, Loss: 0.79889074283176\n",
      "Test Loss after Epoch 141: 0.902413083076477\n",
      "4500\n",
      "Epoch 142, Loss: 0.7982865190770891\n",
      "Test Loss after Epoch 142: 0.901738070487976\n",
      "4500\n",
      "Epoch 143, Loss: 0.8004637829727597\n",
      "Test Loss after Epoch 143: 0.9043852949142456\n",
      "4500\n",
      "Epoch 144, Loss: 0.7956217039691077\n",
      "Test Loss after Epoch 144: 0.8966481211185455\n",
      "4500\n",
      "Epoch 145, Loss: 0.7962692715326944\n",
      "Test Loss after Epoch 145: 0.9029014060497284\n",
      "4500\n",
      "Epoch 146, Loss: 0.7945912109745873\n",
      "Test Loss after Epoch 146: 0.892683066368103\n",
      "4500\n",
      "Epoch 147, Loss: 0.7921918253898621\n",
      "Test Loss after Epoch 147: 0.8872819230556488\n",
      "4500\n",
      "Epoch 148, Loss: 0.7955169517993927\n",
      "Test Loss after Epoch 148: 0.8979352142810821\n",
      "4500\n",
      "Epoch 149, Loss: 0.7929447333547804\n",
      "Test Loss after Epoch 149: 0.89510702252388\n",
      "4500\n",
      "Epoch 150, Loss: 0.7918978601826562\n",
      "Test Loss after Epoch 150: 0.8906747450828553\n",
      "4500\n",
      "Epoch 151, Loss: 0.7909233215120104\n",
      "Test Loss after Epoch 151: 0.8947312955856324\n",
      "4500\n",
      "Epoch 152, Loss: 0.7905400217374166\n",
      "Test Loss after Epoch 152: 0.9014112005233764\n",
      "4500\n",
      "Epoch 153, Loss: 0.7884613214598761\n",
      "Test Loss after Epoch 153: 0.8876391339302063\n",
      "4500\n",
      "Epoch 154, Loss: 0.789930609146754\n",
      "Test Loss after Epoch 154: 0.885963140964508\n",
      "4500\n",
      "Epoch 155, Loss: 0.7875446868207719\n",
      "Test Loss after Epoch 155: 0.8968489277362823\n",
      "4500\n",
      "Epoch 156, Loss: 0.785074005206426\n",
      "Test Loss after Epoch 156: 0.8977260072231292\n",
      "4500\n",
      "Epoch 157, Loss: 0.7846747634675768\n",
      "Test Loss after Epoch 157: 0.8939879112243653\n",
      "4500\n",
      "Epoch 158, Loss: 0.7885730760627323\n",
      "Test Loss after Epoch 158: 0.8979523375034332\n",
      "4500\n",
      "Epoch 159, Loss: 0.7887113968531291\n",
      "Test Loss after Epoch 159: 0.8954409852027893\n",
      "4500\n",
      "Epoch 160, Loss: 0.7873396916654375\n",
      "Test Loss after Epoch 160: 0.8923883428573608\n",
      "4500\n",
      "Epoch 161, Loss: 0.7878573742177751\n",
      "Test Loss after Epoch 161: 0.8981892745494843\n",
      "4500\n",
      "Epoch 162, Loss: 0.7869784949355655\n",
      "Test Loss after Epoch 162: 0.8957604978084565\n",
      "4500\n",
      "Epoch 163, Loss: 0.7857310092714098\n",
      "Test Loss after Epoch 163: 0.8853100388050079\n",
      "4500\n",
      "Epoch 164, Loss: 0.7861238392723932\n",
      "Test Loss after Epoch 164: 0.8771327035427093\n",
      "4500\n",
      "Epoch 165, Loss: 0.7838099539015029\n",
      "Test Loss after Epoch 165: 0.89125119805336\n",
      "4500\n",
      "Epoch 166, Loss: 0.7805648828877343\n",
      "Test Loss after Epoch 166: 0.8840913181304931\n",
      "4500\n",
      "Epoch 167, Loss: 0.7821699136098226\n",
      "Test Loss after Epoch 167: 0.8912888510227204\n",
      "4500\n",
      "Epoch 168, Loss: 0.7799639231893751\n",
      "Test Loss after Epoch 168: 0.891335435628891\n",
      "4500\n",
      "Epoch 169, Loss: 0.780438317961163\n",
      "Test Loss after Epoch 169: 0.8871692309379577\n",
      "4500\n",
      "Epoch 170, Loss: 0.7850254966947767\n",
      "Test Loss after Epoch 170: 0.8864769382476807\n",
      "4500\n",
      "Epoch 171, Loss: 0.7784097280502319\n",
      "Test Loss after Epoch 171: 0.8755738005638123\n",
      "4500\n",
      "Epoch 172, Loss: 0.7758688524564107\n",
      "Test Loss after Epoch 172: 0.8824934515953063\n",
      "4500\n",
      "Epoch 173, Loss: 0.779197923262914\n",
      "Test Loss after Epoch 173: 0.8866770296096802\n",
      "4500\n",
      "Epoch 174, Loss: 0.7779888747268253\n",
      "Test Loss after Epoch 174: 0.8760698311328888\n",
      "4500\n",
      "Epoch 175, Loss: 0.7807316353056166\n",
      "Test Loss after Epoch 175: 0.8783473544120789\n",
      "4500\n",
      "Epoch 176, Loss: 0.7793327042526669\n",
      "Test Loss after Epoch 176: 0.8889033277034759\n",
      "4500\n",
      "Epoch 177, Loss: 0.7793555550045437\n",
      "Test Loss after Epoch 177: 0.8863493208885193\n",
      "4500\n",
      "Epoch 178, Loss: 0.7806395732296838\n",
      "Test Loss after Epoch 178: 0.8699586431980133\n",
      "4500\n",
      "Epoch 179, Loss: 0.7768504101435344\n",
      "Test Loss after Epoch 179: 0.8688866715431214\n",
      "4500\n",
      "Epoch 180, Loss: 0.7786022940476736\n",
      "Test Loss after Epoch 180: 0.8875178630352021\n",
      "4500\n",
      "Epoch 181, Loss: 0.7759035252730052\n",
      "Test Loss after Epoch 181: 0.8828440401554107\n",
      "4500\n",
      "Epoch 182, Loss: 0.7793780309624142\n",
      "Test Loss after Epoch 182: 0.8888510363101959\n",
      "4500\n",
      "Epoch 183, Loss: 0.7765499396059248\n",
      "Test Loss after Epoch 183: 0.8939934678077698\n",
      "4500\n",
      "Epoch 184, Loss: 0.7749917176299626\n",
      "Test Loss after Epoch 184: 0.8859550926685333\n",
      "4500\n",
      "Epoch 185, Loss: 0.7766640688843197\n",
      "Test Loss after Epoch 185: 0.9036614527702331\n",
      "4500\n",
      "Epoch 186, Loss: 0.7785397914250691\n",
      "Test Loss after Epoch 186: 0.9008995385169983\n",
      "4500\n",
      "Epoch 187, Loss: 0.7746411872704824\n",
      "Test Loss after Epoch 187: 0.896333752155304\n",
      "4500\n",
      "Epoch 188, Loss: 0.77450619702869\n",
      "Test Loss after Epoch 188: 0.890415367603302\n",
      "4500\n",
      "Epoch 189, Loss: 0.774907873445087\n",
      "Test Loss after Epoch 189: 0.8774633026123047\n",
      "4500\n",
      "Epoch 190, Loss: 0.7731475797494253\n",
      "Test Loss after Epoch 190: 0.8840784759521484\n",
      "4500\n",
      "Epoch 191, Loss: 0.7739973747730255\n",
      "Test Loss after Epoch 191: 0.8691078443527221\n",
      "4500\n",
      "Epoch 192, Loss: 0.7711483849949307\n",
      "Test Loss after Epoch 192: 0.8758945548534394\n",
      "4500\n",
      "Epoch 193, Loss: 0.7750799441072675\n",
      "Test Loss after Epoch 193: 0.8914542572498322\n",
      "4500\n",
      "Epoch 194, Loss: 0.7744328850640191\n",
      "Test Loss after Epoch 194: 0.8805245230197907\n",
      "4500\n",
      "Epoch 195, Loss: 0.7740650716092852\n",
      "Test Loss after Epoch 195: 0.8774364230632782\n",
      "4500\n",
      "Epoch 196, Loss: 0.7720283774799771\n",
      "Test Loss after Epoch 196: 0.8840923125743866\n",
      "4500\n",
      "Epoch 197, Loss: 0.770456382089191\n",
      "Test Loss after Epoch 197: 0.885258505821228\n",
      "4500\n",
      "Epoch 198, Loss: 0.7729850078688727\n",
      "Test Loss after Epoch 198: 0.8734484958648682\n",
      "4500\n",
      "Epoch 199, Loss: 0.774012254635493\n",
      "Test Loss after Epoch 199: 0.875823175907135\n",
      "Rolling back to best model from epoch 179\n",
      "Best test loss: 0.8688866715431214\n",
      "4500\n",
      "Epoch 200, Loss: 0.778294838560952\n",
      "Test Loss after Epoch 200: 0.8714131391048432\n",
      "4500\n",
      "Epoch 201, Loss: 0.773600585963991\n",
      "Test Loss after Epoch 201: 0.8770443651676177\n",
      "4500\n",
      "Epoch 202, Loss: 0.7745178140269385\n",
      "Test Loss after Epoch 202: 0.8805310928821564\n",
      "4500\n",
      "Epoch 203, Loss: 0.7758187584877014\n",
      "Test Loss after Epoch 203: 0.8812133572101593\n",
      "4500\n",
      "Epoch 204, Loss: 0.7732806061638726\n",
      "Test Loss after Epoch 204: 0.8783837833404541\n",
      "4500\n",
      "Epoch 205, Loss: 0.7688084380096859\n",
      "Test Loss after Epoch 205: 0.88948095536232\n",
      "4500\n",
      "Epoch 206, Loss: 0.7717124260001712\n",
      "Test Loss after Epoch 206: 0.882058482170105\n",
      "4500\n",
      "Epoch 207, Loss: 0.7721397624810536\n",
      "Test Loss after Epoch 207: 0.8842326374053955\n",
      "4500\n",
      "Epoch 208, Loss: 0.7709316234853533\n",
      "Test Loss after Epoch 208: 0.8800074801445007\n",
      "4500\n",
      "Epoch 209, Loss: 0.7722081024911669\n",
      "Test Loss after Epoch 209: 0.8839511775970459\n",
      "4500\n",
      "Epoch 210, Loss: 0.7700975911882189\n",
      "Test Loss after Epoch 210: 0.8777308876514435\n",
      "4500\n",
      "Epoch 211, Loss: 0.7756180606153277\n",
      "Test Loss after Epoch 211: 0.8754208314418793\n",
      "4500\n",
      "Epoch 212, Loss: 0.7743920010195838\n",
      "Test Loss after Epoch 212: 0.8806067440509796\n",
      "4500\n",
      "Epoch 213, Loss: 0.7744480255444844\n",
      "Test Loss after Epoch 213: 0.8674530210494995\n",
      "4500\n",
      "Epoch 214, Loss: 0.7723058271408081\n",
      "Test Loss after Epoch 214: 0.8779882323741913\n",
      "4500\n",
      "Epoch 215, Loss: 0.7695376641485426\n",
      "Test Loss after Epoch 215: 0.8846202948093415\n",
      "4500\n",
      "Epoch 216, Loss: 0.7693215278519524\n",
      "Test Loss after Epoch 216: 0.8868866484165192\n",
      "4500\n",
      "Epoch 217, Loss: 0.7727108201185863\n",
      "Test Loss after Epoch 217: 0.8833695554733276\n",
      "4500\n",
      "Epoch 218, Loss: 0.7677489720715417\n",
      "Test Loss after Epoch 218: 0.8857272884845734\n",
      "4500\n",
      "Epoch 219, Loss: 0.7706102153460185\n",
      "Test Loss after Epoch 219: 0.8809948983192444\n",
      "4500\n",
      "Epoch 220, Loss: 0.7692979442808363\n",
      "Test Loss after Epoch 220: 0.8769267632961273\n",
      "4500\n",
      "Epoch 221, Loss: 0.7686807902124193\n",
      "Test Loss after Epoch 221: 0.8757970397472381\n",
      "4500\n",
      "Epoch 222, Loss: 0.7718162942727407\n",
      "Test Loss after Epoch 222: 0.8796289367675781\n",
      "4500\n",
      "Epoch 223, Loss: 0.7698433038393656\n",
      "Test Loss after Epoch 223: 0.8715817320346833\n",
      "4500\n",
      "Epoch 224, Loss: 0.7691036406622993\n",
      "Test Loss after Epoch 224: 0.8728079867362976\n",
      "4500\n",
      "Epoch 225, Loss: 0.7683518625100454\n",
      "Test Loss after Epoch 225: 0.8823337430953979\n",
      "4500\n",
      "Epoch 226, Loss: 0.7722739097542233\n",
      "Test Loss after Epoch 226: 0.8830645003318787\n",
      "4500\n",
      "Epoch 227, Loss: 0.768956976890564\n",
      "Test Loss after Epoch 227: 0.8765676293373108\n",
      "4500\n",
      "Epoch 228, Loss: 0.7713487780623965\n",
      "Test Loss after Epoch 228: 0.8959271433353424\n",
      "4500\n",
      "Epoch 229, Loss: 0.7678908154964447\n",
      "Test Loss after Epoch 229: 0.8916535506248474\n",
      "4500\n",
      "Epoch 230, Loss: 0.7669857563442654\n",
      "Test Loss after Epoch 230: 0.878470983505249\n",
      "4500\n",
      "Epoch 231, Loss: 0.7678753259181976\n",
      "Test Loss after Epoch 231: 0.8741572523117065\n",
      "4500\n",
      "Epoch 232, Loss: 0.765771435101827\n",
      "Test Loss after Epoch 232: 0.8706442985534668\n",
      "4500\n",
      "Epoch 233, Loss: 0.7671541876792908\n",
      "Test Loss after Epoch 233: 0.8785511565208435\n",
      "4500\n",
      "Epoch 234, Loss: 0.7660788026385837\n",
      "Test Loss after Epoch 234: 0.8731732375621796\n",
      "4500\n",
      "Epoch 235, Loss: 0.7686685961352454\n",
      "Test Loss after Epoch 235: 0.8715486805438996\n",
      "4500\n",
      "Epoch 236, Loss: 0.7679333969751994\n",
      "Test Loss after Epoch 236: 0.8807696769237519\n",
      "4500\n",
      "Epoch 237, Loss: 0.7690681474473742\n",
      "Test Loss after Epoch 237: 0.8746604554653168\n",
      "4500\n",
      "Epoch 238, Loss: 0.7677436273362902\n",
      "Test Loss after Epoch 238: 0.8709328234195709\n",
      "4500\n",
      "Epoch 239, Loss: 0.7664863437546624\n",
      "Test Loss after Epoch 239: 0.8709060156345367\n",
      "4500\n",
      "Epoch 240, Loss: 0.7667114180723826\n",
      "Test Loss after Epoch 240: 0.8713039059638977\n",
      "4500\n",
      "Epoch 241, Loss: 0.7678319981627995\n",
      "Test Loss after Epoch 241: 0.8757727222442627\n",
      "Rolling back to best model from epoch 213\n",
      "Best test loss: 0.8674530210494995\n",
      "4500\n",
      "Epoch 242, Loss: 0.7703864216009776\n",
      "Test Loss after Epoch 242: 0.8755769982337952\n",
      "4500\n",
      "Epoch 243, Loss: 0.7668681122726865\n",
      "Test Loss after Epoch 243: 0.8801913187503815\n",
      "4500\n",
      "Epoch 244, Loss: 0.7701098159154256\n",
      "Test Loss after Epoch 244: 0.8809055500030517\n",
      "4500\n",
      "Epoch 245, Loss: 0.7663036732673645\n",
      "Test Loss after Epoch 245: 0.8774462780952453\n",
      "4500\n",
      "Epoch 246, Loss: 0.7655987261136373\n",
      "Test Loss after Epoch 246: 0.8706202001571656\n",
      "4500\n",
      "Epoch 247, Loss: 0.7676622349156274\n",
      "Test Loss after Epoch 247: 0.8757425968647004\n",
      "4500\n",
      "Epoch 248, Loss: 0.7672444313102298\n",
      "Test Loss after Epoch 248: 0.8765168344974518\n",
      "4500\n",
      "Epoch 249, Loss: 0.7676498159567515\n",
      "Test Loss after Epoch 249: 0.8704522323608398\n",
      "4500\n",
      "Epoch 250, Loss: 0.7663369076251983\n",
      "Test Loss after Epoch 250: 0.866372156381607\n",
      "4500\n",
      "Epoch 251, Loss: 0.7683507211208344\n",
      "Test Loss after Epoch 251: 0.8754082524776459\n",
      "4500\n",
      "Epoch 252, Loss: 0.7669425004058414\n",
      "Test Loss after Epoch 252: 0.877869042634964\n",
      "4500\n",
      "Epoch 253, Loss: 0.7697176201608446\n",
      "Test Loss after Epoch 253: 0.869029158115387\n",
      "4500\n",
      "Epoch 254, Loss: 0.7673280211819543\n",
      "Test Loss after Epoch 254: 0.8715807814598083\n",
      "4500\n",
      "Epoch 255, Loss: 0.7714131532245212\n",
      "Test Loss after Epoch 255: 0.8721166586875916\n",
      "4500\n",
      "Epoch 256, Loss: 0.7690317319764032\n",
      "Test Loss after Epoch 256: 0.8640969462394714\n",
      "4500\n",
      "Epoch 257, Loss: 0.7665263381799062\n",
      "Test Loss after Epoch 257: 0.8532632410526275\n",
      "4500\n",
      "Epoch 258, Loss: 0.7681812176174587\n",
      "Test Loss after Epoch 258: 0.8519109256267547\n",
      "4500\n",
      "Epoch 259, Loss: 0.7710397076076931\n",
      "Test Loss after Epoch 259: 0.8584020707607269\n",
      "4500\n",
      "Epoch 260, Loss: 0.7711257952054341\n",
      "Test Loss after Epoch 260: 0.8582180299758911\n",
      "4500\n",
      "Epoch 261, Loss: 0.7689931745529175\n",
      "Test Loss after Epoch 261: 0.8646418471336365\n",
      "4500\n",
      "Epoch 262, Loss: 0.772067820681466\n",
      "Test Loss after Epoch 262: 0.8612579238414765\n",
      "4500\n",
      "Epoch 263, Loss: 0.7680779458416833\n",
      "Test Loss after Epoch 263: 0.8643789184093476\n",
      "4500\n",
      "Epoch 264, Loss: 0.7703979350195991\n",
      "Test Loss after Epoch 264: 0.8757412481307983\n",
      "4500\n",
      "Epoch 265, Loss: 0.7677044115861257\n",
      "Test Loss after Epoch 265: 0.8668340661525726\n",
      "4500\n",
      "Epoch 266, Loss: 0.7674990419811673\n",
      "Test Loss after Epoch 266: 0.874032888174057\n",
      "4500\n",
      "Epoch 267, Loss: 0.7680971290270487\n",
      "Test Loss after Epoch 267: 0.8782334389686585\n",
      "4500\n",
      "Epoch 268, Loss: 0.7675163845486112\n",
      "Test Loss after Epoch 268: 0.8738693680763244\n",
      "4500\n",
      "Epoch 269, Loss: 0.7705294544431899\n",
      "Test Loss after Epoch 269: 0.8723583853244782\n",
      "4500\n",
      "Epoch 270, Loss: 0.7682648534244961\n",
      "Test Loss after Epoch 270: 0.8777556140422821\n",
      "4500\n",
      "Epoch 271, Loss: 0.7703337374263339\n",
      "Test Loss after Epoch 271: 0.8840860435962677\n",
      "4500\n",
      "Epoch 272, Loss: 0.7690114521980286\n",
      "Test Loss after Epoch 272: 0.8894265491962433\n",
      "4500\n",
      "Epoch 273, Loss: 0.7722285380628374\n",
      "Test Loss after Epoch 273: 0.900564968585968\n",
      "4500\n",
      "Epoch 274, Loss: 0.7658377122879029\n",
      "Test Loss after Epoch 274: 0.8798461213111878\n",
      "4500\n",
      "Epoch 275, Loss: 0.7717852114571465\n",
      "Test Loss after Epoch 275: 0.8822196588516236\n",
      "4500\n",
      "Epoch 276, Loss: 0.7661576357152727\n",
      "Test Loss after Epoch 276: 0.8858530702590942\n",
      "4500\n",
      "Epoch 277, Loss: 0.7683106480174594\n",
      "Test Loss after Epoch 277: 0.8802292790412903\n",
      "4500\n",
      "Epoch 278, Loss: 0.770034424384435\n",
      "Test Loss after Epoch 278: 0.8824668657779694\n",
      "4500\n",
      "Epoch 279, Loss: 0.7670589745574528\n",
      "Test Loss after Epoch 279: 0.8834266197681427\n",
      "4500\n",
      "Epoch 280, Loss: 0.7683288956218296\n",
      "Test Loss after Epoch 280: 0.8891346108913422\n",
      "4500\n",
      "Epoch 281, Loss: 0.7693071992927127\n",
      "Test Loss after Epoch 281: 0.884098849773407\n",
      "4500\n",
      "Epoch 282, Loss: 0.7691191032197741\n",
      "Test Loss after Epoch 282: 0.887422178030014\n",
      "4500\n",
      "Epoch 283, Loss: 0.7690871041350894\n",
      "Test Loss after Epoch 283: 0.884173070192337\n",
      "4500\n",
      "Epoch 284, Loss: 0.7688705215189192\n",
      "Test Loss after Epoch 284: 0.890603812456131\n",
      "4500\n",
      "Epoch 285, Loss: 0.7647275762822893\n",
      "Test Loss after Epoch 285: 0.8880067586898803\n",
      "4500\n",
      "Epoch 286, Loss: 0.7683458516332838\n",
      "Test Loss after Epoch 286: 0.8809645383358001\n",
      "4500\n",
      "Epoch 287, Loss: 0.7678560965326098\n",
      "Test Loss after Epoch 287: 0.8783122613430023\n",
      "4500\n",
      "Epoch 288, Loss: 0.7680191654894087\n",
      "Test Loss after Epoch 288: 0.8786702508926392\n",
      "4500\n",
      "Epoch 289, Loss: 0.7656179900964101\n",
      "Test Loss after Epoch 289: 0.8831616139411926\n",
      "4500\n",
      "Epoch 290, Loss: 0.7644694586594899\n",
      "Test Loss after Epoch 290: 0.8989557013511658\n",
      "4500\n",
      "Epoch 291, Loss: 0.7687450802326202\n",
      "Test Loss after Epoch 291: 0.8821458704471589\n",
      "4500\n",
      "Epoch 292, Loss: 0.76665773179796\n",
      "Test Loss after Epoch 292: 0.884834547996521\n",
      "4500\n",
      "Epoch 293, Loss: 0.7687010485596127\n",
      "Test Loss after Epoch 293: 0.8749380714893341\n",
      "4500\n",
      "Epoch 294, Loss: 0.7663458862039778\n",
      "Test Loss after Epoch 294: 0.8843490645885468\n",
      "4500\n",
      "Epoch 295, Loss: 0.7637755956119961\n",
      "Test Loss after Epoch 295: 0.8869528329372406\n",
      "4500\n",
      "Epoch 296, Loss: 0.7659268188476562\n",
      "Test Loss after Epoch 296: 0.8889641921520233\n",
      "4500\n",
      "Epoch 297, Loss: 0.7682523920271132\n",
      "Test Loss after Epoch 297: 0.870829495191574\n",
      "Rolling back to best model from epoch 258\n",
      "Best test loss: 0.8519109256267547\n",
      "4500\n",
      "Epoch 298, Loss: 0.7690031500127581\n",
      "Test Loss after Epoch 298: 0.8537752113342285\n",
      "4500\n",
      "Epoch 299, Loss: 0.7641295519404941\n",
      "Test Loss after Epoch 299: 0.8604659888744354\n",
      "4500\n",
      "Epoch 300, Loss: 0.7651256758371989\n",
      "Test Loss after Epoch 300: 0.8643852214813232\n",
      "4500\n",
      "Epoch 301, Loss: 0.7648451877435049\n",
      "Test Loss after Epoch 301: 0.8638794057369232\n",
      "4500\n",
      "Epoch 302, Loss: 0.7680050835609437\n",
      "Test Loss after Epoch 302: 0.8813181872367859\n",
      "4500\n",
      "Epoch 303, Loss: 0.7688933750523461\n",
      "Test Loss after Epoch 303: 0.877041204214096\n",
      "4500\n",
      "Epoch 304, Loss: 0.7714169791804419\n",
      "Test Loss after Epoch 304: 0.8564536142349243\n",
      "4500\n",
      "Epoch 305, Loss: 0.7704801386727227\n",
      "Test Loss after Epoch 305: 0.8782127459049225\n",
      "4500\n",
      "Epoch 306, Loss: 0.7724121849271987\n",
      "Test Loss after Epoch 306: 0.8704346356391907\n",
      "4500\n",
      "Epoch 307, Loss: 0.774598079389996\n",
      "Test Loss after Epoch 307: 0.8693674347400665\n",
      "4500\n",
      "Epoch 308, Loss: 0.7672266913255056\n",
      "Test Loss after Epoch 308: 0.8748338043689727\n",
      "4500\n",
      "Epoch 309, Loss: 0.774166415002611\n",
      "Test Loss after Epoch 309: 0.8686867723464966\n",
      "4500\n",
      "Epoch 310, Loss: 0.7712855266465081\n",
      "Test Loss after Epoch 310: 0.8756738817691803\n",
      "4500\n",
      "Epoch 311, Loss: 0.772440510696835\n",
      "Test Loss after Epoch 311: 0.8634432067871094\n",
      "4500\n",
      "Epoch 312, Loss: 0.7737320364316305\n",
      "Test Loss after Epoch 312: 0.877060950756073\n",
      "4500\n",
      "Epoch 313, Loss: 0.7721563977665371\n",
      "Test Loss after Epoch 313: 0.8879715528488159\n",
      "4500\n",
      "Epoch 314, Loss: 0.7723209606011708\n",
      "Test Loss after Epoch 314: 0.8736985757350921\n",
      "4500\n",
      "Epoch 315, Loss: 0.7715103261735704\n",
      "Test Loss after Epoch 315: 0.87025701212883\n",
      "4500\n",
      "Epoch 316, Loss: 0.7701565052403344\n",
      "Test Loss after Epoch 316: 0.8932146468162536\n",
      "4500\n",
      "Epoch 317, Loss: 0.7686644554668003\n",
      "Test Loss after Epoch 317: 0.8796364390850067\n",
      "4500\n",
      "Epoch 318, Loss: 0.7700110521051619\n",
      "Test Loss after Epoch 318: 0.8963038065433502\n",
      "4500\n",
      "Epoch 319, Loss: 0.7676032324632008\n",
      "Test Loss after Epoch 319: 0.8711251304149628\n",
      "4500\n",
      "Epoch 320, Loss: 0.7702872975137499\n",
      "Test Loss after Epoch 320: 0.8608413214683532\n",
      "4500\n",
      "Epoch 321, Loss: 0.7705426197052002\n",
      "Test Loss after Epoch 321: 0.8624456961154938\n",
      "4500\n",
      "Epoch 322, Loss: 0.772378113190333\n",
      "Test Loss after Epoch 322: 0.8756265873908997\n",
      "4500\n",
      "Epoch 323, Loss: 0.76820473546452\n",
      "Test Loss after Epoch 323: 0.8827972915172577\n",
      "4500\n",
      "Epoch 324, Loss: 0.7720944817860921\n",
      "Test Loss after Epoch 324: 0.8789957959651947\n",
      "4500\n",
      "Epoch 325, Loss: 0.7731520594755809\n",
      "Test Loss after Epoch 325: 0.8665319766998291\n",
      "4500\n",
      "Epoch 326, Loss: 0.7728229860199822\n",
      "Test Loss after Epoch 326: 0.8608684928417206\n",
      "4500\n",
      "Epoch 327, Loss: 0.7689056104024251\n",
      "Test Loss after Epoch 327: 0.8723902325630188\n",
      "4500\n",
      "Epoch 328, Loss: 0.7720976750320858\n",
      "Test Loss after Epoch 328: 0.886012579202652\n",
      "4500\n",
      "Epoch 329, Loss: 0.773238984213935\n",
      "Test Loss after Epoch 329: 0.888775890827179\n",
      "4500\n",
      "Epoch 330, Loss: 0.7698198574648963\n",
      "Test Loss after Epoch 330: 0.8925186250209808\n",
      "4500\n",
      "Epoch 331, Loss: 0.770337153673172\n",
      "Test Loss after Epoch 331: 0.8819627590179443\n",
      "4500\n",
      "Epoch 332, Loss: 0.7728976512485081\n",
      "Test Loss after Epoch 332: 0.8707005372047424\n",
      "4500\n",
      "Epoch 333, Loss: 0.7678866480191548\n",
      "Test Loss after Epoch 333: 0.8746385259628295\n",
      "4500\n",
      "Epoch 334, Loss: 0.772185898648368\n",
      "Test Loss after Epoch 334: 0.8855258371829987\n",
      "4500\n",
      "Epoch 335, Loss: 0.7671933064460754\n",
      "Test Loss after Epoch 335: 0.8766664659976959\n",
      "5400\n",
      "Epoch 336, Loss: 0.8523647699311927\n",
      "Test Loss after Epoch 336: 0.8415525364875793\n",
      "5400\n",
      "Epoch 337, Loss: 0.8298380748872404\n",
      "Test Loss after Epoch 337: 0.8340931684970856\n",
      "5400\n",
      "Epoch 338, Loss: 0.8203493839281577\n",
      "Test Loss after Epoch 338: 0.8557940695285797\n",
      "5400\n",
      "Epoch 339, Loss: 0.8175114086159954\n",
      "Test Loss after Epoch 339: 0.8528618078231811\n",
      "5400\n",
      "Epoch 340, Loss: 0.8072912450189944\n",
      "Test Loss after Epoch 340: 0.8291204123497009\n",
      "5400\n",
      "Epoch 341, Loss: 0.80663263992027\n",
      "Test Loss after Epoch 341: 0.8385462176799774\n",
      "5400\n",
      "Epoch 342, Loss: 0.8056652526502256\n",
      "Test Loss after Epoch 342: 0.8443422660827636\n",
      "5400\n",
      "Epoch 343, Loss: 0.7972891911312386\n",
      "Test Loss after Epoch 343: 0.83115247631073\n",
      "5400\n",
      "Epoch 344, Loss: 0.7974990522640723\n",
      "Test Loss after Epoch 344: 0.8313120458126068\n",
      "5400\n",
      "Epoch 345, Loss: 0.7949857898773971\n",
      "Test Loss after Epoch 345: 0.8397457454204559\n",
      "5400\n",
      "Epoch 346, Loss: 0.7998167900244395\n",
      "Test Loss after Epoch 346: 0.8419091911315918\n",
      "5400\n",
      "Epoch 347, Loss: 0.7999367592511354\n",
      "Test Loss after Epoch 347: 0.840450273513794\n",
      "5400\n",
      "Epoch 348, Loss: 0.7951143378019333\n",
      "Test Loss after Epoch 348: 0.8447931699752808\n",
      "5400\n",
      "Epoch 349, Loss: 0.7961062549220191\n",
      "Test Loss after Epoch 349: 0.8428488013744354\n",
      "5400\n",
      "Epoch 350, Loss: 0.794419353140725\n",
      "Test Loss after Epoch 350: 0.8477258870601654\n",
      "5400\n",
      "Epoch 351, Loss: 0.7941949620953312\n",
      "Test Loss after Epoch 351: 0.8448579597473145\n",
      "5400\n",
      "Epoch 352, Loss: 0.7889725452661515\n",
      "Test Loss after Epoch 352: 0.8396930034160615\n",
      "5400\n",
      "Epoch 353, Loss: 0.7902336775594287\n",
      "Test Loss after Epoch 353: 0.8439479389190674\n",
      "5400\n",
      "Epoch 354, Loss: 0.7843370068956305\n",
      "Test Loss after Epoch 354: 0.8366925852298737\n",
      "5400\n",
      "Epoch 355, Loss: 0.7874219848491527\n",
      "Test Loss after Epoch 355: 0.8437595989704132\n",
      "5400\n",
      "Epoch 356, Loss: 0.7898010458107347\n",
      "Test Loss after Epoch 356: 0.8379792497158051\n",
      "5400\n",
      "Epoch 357, Loss: 0.7861107413636313\n",
      "Test Loss after Epoch 357: 0.8336505346298217\n",
      "5400\n",
      "Epoch 358, Loss: 0.785977649865327\n",
      "Test Loss after Epoch 358: 0.8423225655555725\n",
      "5400\n",
      "Epoch 359, Loss: 0.7847364526104044\n",
      "Test Loss after Epoch 359: 0.8467570939064026\n",
      "5400\n",
      "Epoch 360, Loss: 0.7838874298996396\n",
      "Test Loss after Epoch 360: 0.8369615786075592\n",
      "5400\n",
      "Epoch 361, Loss: 0.7806450887521108\n",
      "Test Loss after Epoch 361: 0.8319387364387513\n",
      "5400\n",
      "Epoch 362, Loss: 0.786351332642414\n",
      "Test Loss after Epoch 362: 0.8364363949298859\n",
      "5400\n",
      "Epoch 363, Loss: 0.7816512282027138\n",
      "Test Loss after Epoch 363: 0.83398144364357\n",
      "5400\n",
      "Epoch 364, Loss: 0.7803797689190617\n",
      "Test Loss after Epoch 364: 0.8384143648147583\n",
      "5400\n",
      "Epoch 365, Loss: 0.7781268785176454\n",
      "Test Loss after Epoch 365: 0.8333960032463074\n",
      "5400\n",
      "Epoch 366, Loss: 0.7794848069879744\n",
      "Test Loss after Epoch 366: 0.8273843832015991\n",
      "5400\n",
      "Epoch 367, Loss: 0.7774569418695237\n",
      "Test Loss after Epoch 367: 0.8277681877613068\n",
      "5400\n",
      "Epoch 368, Loss: 0.7738148668077257\n",
      "Test Loss after Epoch 368: 0.8266029274463653\n",
      "5400\n",
      "Epoch 369, Loss: 0.7760136577818129\n",
      "Test Loss after Epoch 369: 0.8303823330402375\n",
      "5400\n",
      "Epoch 370, Loss: 0.773423099871035\n",
      "Test Loss after Epoch 370: 0.8393562977313995\n",
      "5400\n",
      "Epoch 371, Loss: 0.7790232753753662\n",
      "Test Loss after Epoch 371: 0.8369819645881653\n",
      "5400\n",
      "Epoch 372, Loss: 0.7769604053982982\n",
      "Test Loss after Epoch 372: 0.8429306135177612\n",
      "5400\n",
      "Epoch 373, Loss: 0.7768715280294418\n",
      "Test Loss after Epoch 373: 0.8441875751018524\n",
      "5400\n",
      "Epoch 374, Loss: 0.7736439385899792\n",
      "Test Loss after Epoch 374: 0.8351860599517822\n",
      "5400\n",
      "Epoch 375, Loss: 0.7735489132448479\n",
      "Test Loss after Epoch 375: 0.8330643124580384\n",
      "5400\n",
      "Epoch 376, Loss: 0.7745235786614595\n",
      "Test Loss after Epoch 376: 0.8408239979743958\n",
      "5400\n",
      "Epoch 377, Loss: 0.7756729109419717\n",
      "Test Loss after Epoch 377: 0.838250066280365\n",
      "5400\n",
      "Epoch 378, Loss: 0.7715043195088704\n",
      "Test Loss after Epoch 378: 0.8300406394004822\n",
      "5400\n",
      "Epoch 379, Loss: 0.7730804933662768\n",
      "Test Loss after Epoch 379: 0.8334000017642975\n",
      "5400\n",
      "Epoch 380, Loss: 0.7709960934409389\n",
      "Test Loss after Epoch 380: 0.8312655980587006\n",
      "5400\n",
      "Epoch 381, Loss: 0.7733274003090682\n",
      "Test Loss after Epoch 381: 0.8288642168045044\n",
      "5400\n",
      "Epoch 382, Loss: 0.7692283223734961\n",
      "Test Loss after Epoch 382: 0.8280252842903137\n",
      "5400\n",
      "Epoch 383, Loss: 0.7699392210995709\n",
      "Test Loss after Epoch 383: 0.8339792673587799\n",
      "5400\n",
      "Epoch 384, Loss: 0.7706547671114957\n",
      "Test Loss after Epoch 384: 0.8332137179374695\n",
      "5400\n",
      "Epoch 385, Loss: 0.7712653512204135\n",
      "Test Loss after Epoch 385: 0.8294187774658203\n",
      "5400\n",
      "Epoch 386, Loss: 0.7711154992271353\n",
      "Test Loss after Epoch 386: 0.8305615570545196\n",
      "5400\n",
      "Epoch 387, Loss: 0.771935407033673\n",
      "Test Loss after Epoch 387: 0.8386910142898559\n",
      "5400\n",
      "Epoch 388, Loss: 0.7734668985781846\n",
      "Test Loss after Epoch 388: 0.8456807861328125\n",
      "5400\n",
      "Epoch 389, Loss: 0.769675148261918\n",
      "Test Loss after Epoch 389: 0.8429702203273773\n",
      "5400\n",
      "Epoch 390, Loss: 0.7704121040414881\n",
      "Test Loss after Epoch 390: 0.8363664076328278\n",
      "5400\n",
      "Epoch 391, Loss: 0.7687213106729366\n",
      "Test Loss after Epoch 391: 0.8315596120357513\n",
      "5400\n",
      "Epoch 392, Loss: 0.7648498995436562\n",
      "Test Loss after Epoch 392: 0.8364564917087555\n",
      "5400\n",
      "Epoch 393, Loss: 0.7645685502334877\n",
      "Test Loss after Epoch 393: 0.8392392041683197\n",
      "5400\n",
      "Epoch 394, Loss: 0.7666005563735961\n",
      "Test Loss after Epoch 394: 0.8320467028617858\n",
      "5400\n",
      "Epoch 395, Loss: 0.7658293210356324\n",
      "Test Loss after Epoch 395: 0.8340257337093353\n",
      "5400\n",
      "Epoch 396, Loss: 0.7641237791600051\n",
      "Test Loss after Epoch 396: 0.8324322504997254\n",
      "5400\n",
      "Epoch 397, Loss: 0.7649667975858406\n",
      "Test Loss after Epoch 397: 0.8316873276233673\n",
      "5400\n",
      "Epoch 398, Loss: 0.7647557421966835\n",
      "Test Loss after Epoch 398: 0.8289990549087525\n",
      "5400\n",
      "Epoch 399, Loss: 0.7636776482838171\n",
      "Test Loss after Epoch 399: 0.8254787018299102\n",
      "5400\n",
      "Epoch 400, Loss: 0.7657166668220803\n",
      "Test Loss after Epoch 400: 0.8272536072731018\n",
      "5400\n",
      "Epoch 401, Loss: 0.7681352451774809\n",
      "Test Loss after Epoch 401: 0.8251322443485261\n",
      "5400\n",
      "Epoch 402, Loss: 0.7657894732333995\n",
      "Test Loss after Epoch 402: 0.8228883950710296\n",
      "5400\n",
      "Epoch 403, Loss: 0.7641626823389972\n",
      "Test Loss after Epoch 403: 0.8273701720237732\n",
      "5400\n",
      "Epoch 404, Loss: 0.7619391445539616\n",
      "Test Loss after Epoch 404: 0.8294384725093842\n",
      "5400\n",
      "Epoch 405, Loss: 0.762233898418921\n",
      "Test Loss after Epoch 405: 0.8284550166130066\n",
      "5400\n",
      "Epoch 406, Loss: 0.7630690760303427\n",
      "Test Loss after Epoch 406: 0.8367046022415161\n",
      "5400\n",
      "Epoch 407, Loss: 0.7633199187561318\n",
      "Test Loss after Epoch 407: 0.8299455137252808\n",
      "5400\n",
      "Epoch 408, Loss: 0.7630951546077375\n",
      "Test Loss after Epoch 408: 0.8298052322864532\n",
      "5400\n",
      "Epoch 409, Loss: 0.7630213230186038\n",
      "Test Loss after Epoch 409: 0.8338172357082367\n",
      "5400\n",
      "Epoch 410, Loss: 0.7629982690237187\n",
      "Test Loss after Epoch 410: 0.8296121788024903\n",
      "5400\n",
      "Epoch 411, Loss: 0.7628626912169987\n",
      "Test Loss after Epoch 411: 0.8286635797023774\n",
      "5400\n",
      "Epoch 412, Loss: 0.7638626760906644\n",
      "Test Loss after Epoch 412: 0.8363516428470612\n",
      "5400\n",
      "Epoch 413, Loss: 0.7626049987033562\n",
      "Test Loss after Epoch 413: 0.8373383457660675\n",
      "5400\n",
      "Epoch 414, Loss: 0.7649750321441227\n",
      "Test Loss after Epoch 414: 0.8355847671031952\n",
      "5400\n",
      "Epoch 415, Loss: 0.7623796896581296\n",
      "Test Loss after Epoch 415: 0.8317909190654754\n",
      "5400\n",
      "Epoch 416, Loss: 0.7631396620582651\n",
      "Test Loss after Epoch 416: 0.828532570362091\n",
      "5400\n",
      "Epoch 417, Loss: 0.7620422152015898\n",
      "Test Loss after Epoch 417: 0.8326051037311554\n",
      "5400\n",
      "Epoch 418, Loss: 0.7615210029151704\n",
      "Test Loss after Epoch 418: 0.8327360525131225\n",
      "5400\n",
      "Epoch 419, Loss: 0.7607198744791526\n",
      "Test Loss after Epoch 419: 0.8346067719459533\n",
      "5400\n",
      "Epoch 420, Loss: 0.7601907064738097\n",
      "Test Loss after Epoch 420: 0.8366979002952576\n",
      "5400\n",
      "Epoch 421, Loss: 0.7596844716204537\n",
      "Test Loss after Epoch 421: 0.8362462606430053\n",
      "5400\n",
      "Epoch 422, Loss: 0.7618864447761465\n",
      "Test Loss after Epoch 422: 0.8357559609413147\n",
      "5400\n",
      "Epoch 423, Loss: 0.7615555215985687\n",
      "Test Loss after Epoch 423: 0.8315304250717163\n",
      "5400\n",
      "Epoch 424, Loss: 0.7579506718229364\n",
      "Test Loss after Epoch 424: 0.8296710681915284\n",
      "5400\n",
      "Epoch 425, Loss: 0.7556214542300613\n",
      "Test Loss after Epoch 425: 0.8329468874931335\n",
      "5400\n",
      "Epoch 426, Loss: 0.7590708132364132\n",
      "Test Loss after Epoch 426: 0.8328999140262604\n",
      "5400\n",
      "Epoch 427, Loss: 0.75930105165199\n",
      "Test Loss after Epoch 427: 0.8297021551132202\n",
      "5400\n",
      "Epoch 428, Loss: 0.7569242203014869\n",
      "Test Loss after Epoch 428: 0.8299742488861084\n",
      "5400\n",
      "Epoch 429, Loss: 0.7564631712657434\n",
      "Test Loss after Epoch 429: 0.8312511100769043\n",
      "5400\n",
      "Epoch 430, Loss: 0.7617473126340796\n",
      "Test Loss after Epoch 430: 0.8352821447849273\n",
      "5400\n",
      "Epoch 431, Loss: 0.7587423708262266\n",
      "Test Loss after Epoch 431: 0.8281469063758851\n",
      "5400\n",
      "Epoch 432, Loss: 0.7586623679046277\n",
      "Test Loss after Epoch 432: 0.8263443491458893\n",
      "5400\n",
      "Epoch 433, Loss: 0.7571399806384687\n",
      "Test Loss after Epoch 433: 0.8330749745368957\n",
      "5400\n",
      "Epoch 434, Loss: 0.757841159414362\n",
      "Test Loss after Epoch 434: 0.8381842696666717\n",
      "5400\n",
      "Epoch 435, Loss: 0.7569861003425387\n",
      "Test Loss after Epoch 435: 0.8345347118377685\n",
      "5400\n",
      "Epoch 436, Loss: 0.7605809171332253\n",
      "Test Loss after Epoch 436: 0.8312615957260132\n",
      "5400\n",
      "Epoch 437, Loss: 0.7592630900939306\n",
      "Test Loss after Epoch 437: 0.8281837325096131\n",
      "5400\n",
      "Epoch 438, Loss: 0.7598095979734704\n",
      "Test Loss after Epoch 438: 0.8258601794242859\n",
      "5400\n",
      "Epoch 439, Loss: 0.7576428134353073\n",
      "Test Loss after Epoch 439: 0.8258127644062042\n",
      "5400\n",
      "Epoch 440, Loss: 0.7555719412035412\n",
      "Test Loss after Epoch 440: 0.8284393107891083\n",
      "5400\n",
      "Epoch 441, Loss: 0.7574259958002303\n",
      "Test Loss after Epoch 441: 0.8292779448032379\n",
      "5400\n",
      "Epoch 442, Loss: 0.7562009726851074\n",
      "Test Loss after Epoch 442: 0.8275916273593903\n",
      "5400\n",
      "Epoch 443, Loss: 0.7550319938968729\n",
      "Test Loss after Epoch 443: 0.8228360476493836\n",
      "5400\n",
      "Epoch 444, Loss: 0.7561829892132017\n",
      "Test Loss after Epoch 444: 0.8263603179454804\n",
      "5400\n",
      "Epoch 445, Loss: 0.755987673357681\n",
      "Test Loss after Epoch 445: 0.8355770797729493\n",
      "5400\n",
      "Epoch 446, Loss: 0.7591971989472707\n",
      "Test Loss after Epoch 446: 0.8293849906921387\n",
      "5400\n",
      "Epoch 447, Loss: 0.7557939895877132\n",
      "Test Loss after Epoch 447: 0.8245995354652405\n",
      "5400\n",
      "Epoch 448, Loss: 0.757391568797606\n",
      "Test Loss after Epoch 448: 0.831656031370163\n",
      "5400\n",
      "Epoch 449, Loss: 0.7558134688050658\n",
      "Test Loss after Epoch 449: 0.8345628056526184\n",
      "5400\n",
      "Epoch 450, Loss: 0.7539942185525541\n",
      "Test Loss after Epoch 450: 0.8280286281108856\n",
      "5400\n",
      "Epoch 451, Loss: 0.7571616288688447\n",
      "Test Loss after Epoch 451: 0.8254970192909241\n",
      "5400\n",
      "Epoch 452, Loss: 0.7554909996853935\n",
      "Test Loss after Epoch 452: 0.8302977747917175\n",
      "5400\n",
      "Epoch 453, Loss: 0.7557940831890813\n",
      "Test Loss after Epoch 453: 0.8306578600406647\n",
      "5400\n",
      "Epoch 454, Loss: 0.7540246034993066\n",
      "Test Loss after Epoch 454: 0.8270364437103271\n",
      "5400\n",
      "Epoch 455, Loss: 0.7535319341112067\n",
      "Test Loss after Epoch 455: 0.8346907341480255\n",
      "5400\n",
      "Epoch 456, Loss: 0.7503761376716472\n",
      "Test Loss after Epoch 456: 0.8327268013954162\n",
      "5400\n",
      "Epoch 457, Loss: 0.7544990225853744\n",
      "Test Loss after Epoch 457: 0.8332295560836792\n",
      "5400\n",
      "Epoch 458, Loss: 0.7513785644372304\n",
      "Test Loss after Epoch 458: 0.8414738893508911\n",
      "5400\n",
      "Epoch 459, Loss: 0.7534132213945742\n",
      "Test Loss after Epoch 459: 0.8398362097740173\n",
      "5400\n",
      "Epoch 460, Loss: 0.7505501908505404\n",
      "Test Loss after Epoch 460: 0.8412095980644226\n",
      "5400\n",
      "Epoch 461, Loss: 0.7528791942419829\n",
      "Test Loss after Epoch 461: 0.8361962463855743\n",
      "5400\n",
      "Epoch 462, Loss: 0.7512392899062899\n",
      "Test Loss after Epoch 462: 0.836056919336319\n",
      "5400\n",
      "Epoch 463, Loss: 0.75161811278926\n",
      "Test Loss after Epoch 463: 0.8382146158218384\n",
      "5400\n",
      "Epoch 464, Loss: 0.7516313790171235\n",
      "Test Loss after Epoch 464: 0.8362331898212433\n",
      "5400\n",
      "Epoch 465, Loss: 0.748749696413676\n",
      "Test Loss after Epoch 465: 0.8322845678329468\n",
      "5400\n",
      "Epoch 466, Loss: 0.752797549366951\n",
      "Test Loss after Epoch 466: 0.8258986959457397\n",
      "5400\n",
      "Epoch 467, Loss: 0.7515364664572256\n",
      "Test Loss after Epoch 467: 0.8292967059612274\n",
      "5400\n",
      "Epoch 468, Loss: 0.7508950783146753\n",
      "Test Loss after Epoch 468: 0.8279734773635864\n",
      "5400\n",
      "Epoch 469, Loss: 0.7500107389688492\n",
      "Test Loss after Epoch 469: 0.8246226007938385\n",
      "5400\n",
      "Epoch 470, Loss: 0.748282716759929\n",
      "Test Loss after Epoch 470: 0.8282397530078888\n",
      "5400\n",
      "Epoch 471, Loss: 0.7509949020986204\n",
      "Test Loss after Epoch 471: 0.8310704636573791\n",
      "5400\n",
      "Epoch 472, Loss: 0.7510868401880617\n",
      "Test Loss after Epoch 472: 0.8320782146453858\n",
      "5400\n",
      "Epoch 473, Loss: 0.7497206128526617\n",
      "Test Loss after Epoch 473: 0.8391874568462372\n",
      "5400\n",
      "Epoch 474, Loss: 0.7502884634115078\n",
      "Test Loss after Epoch 474: 0.8372563459873199\n",
      "5400\n",
      "Epoch 475, Loss: 0.7494682334087513\n",
      "Test Loss after Epoch 475: 0.8369375560283661\n",
      "5400\n",
      "Epoch 476, Loss: 0.7532458778884675\n",
      "Test Loss after Epoch 476: 0.8293096103668213\n",
      "5400\n",
      "Epoch 477, Loss: 0.7520313074632927\n",
      "Test Loss after Epoch 477: 0.8314863111972809\n",
      "5400\n",
      "Epoch 478, Loss: 0.7514047842555576\n",
      "Test Loss after Epoch 478: 0.8336384773254395\n",
      "5400\n",
      "Epoch 479, Loss: 0.7510889667934841\n",
      "Test Loss after Epoch 479: 0.8282009902000427\n",
      "5400\n",
      "Epoch 480, Loss: 0.7491410025843868\n",
      "Test Loss after Epoch 480: 0.8240762507915497\n",
      "5400\n",
      "Epoch 481, Loss: 0.7490906848730864\n",
      "Test Loss after Epoch 481: 0.8242175350189209\n",
      "5400\n",
      "Epoch 482, Loss: 0.7502185638745625\n",
      "Test Loss after Epoch 482: 0.8258198294639587\n",
      "5400\n",
      "Epoch 483, Loss: 0.7477902285699491\n",
      "Test Loss after Epoch 483: 0.8250964651107788\n",
      "5400\n",
      "Epoch 484, Loss: 0.748343969738042\n",
      "Test Loss after Epoch 484: 0.8250637276172638\n",
      "5400\n",
      "Epoch 485, Loss: 0.7480791430120115\n",
      "Test Loss after Epoch 485: 0.8242723441123962\n",
      "5400\n",
      "Epoch 486, Loss: 0.7463596577997561\n",
      "Test Loss after Epoch 486: 0.8252266547679901\n",
      "5400\n",
      "Epoch 487, Loss: 0.7485575443726998\n",
      "Test Loss after Epoch 487: 0.8284788112640381\n",
      "5400\n",
      "Epoch 488, Loss: 0.7491311499586811\n",
      "Test Loss after Epoch 488: 0.8249448676109314\n",
      "5400\n",
      "Epoch 489, Loss: 0.7481834703904611\n",
      "Test Loss after Epoch 489: 0.8236258537769318\n",
      "5400\n",
      "Epoch 490, Loss: 0.7478196882318567\n",
      "Test Loss after Epoch 490: 0.8248852071762085\n",
      "5400\n",
      "Epoch 491, Loss: 0.7495877568368559\n",
      "Test Loss after Epoch 491: 0.8221816339492798\n",
      "5400\n",
      "Epoch 492, Loss: 0.7468647923292937\n",
      "Test Loss after Epoch 492: 0.823243010044098\n",
      "5400\n",
      "Epoch 493, Loss: 0.7485044621317475\n",
      "Test Loss after Epoch 493: 0.8245115611553192\n",
      "5400\n",
      "Epoch 494, Loss: 0.7484118459622066\n",
      "Test Loss after Epoch 494: 0.8236536304950715\n",
      "5400\n",
      "Epoch 495, Loss: 0.7480205116890095\n",
      "Test Loss after Epoch 495: 0.8303045670986176\n",
      "5400\n",
      "Epoch 496, Loss: 0.7483874742852317\n",
      "Test Loss after Epoch 496: 0.8275064008235932\n",
      "5400\n",
      "Epoch 497, Loss: 0.7488305344625755\n",
      "Test Loss after Epoch 497: 0.8235252821445465\n",
      "5400\n",
      "Epoch 498, Loss: 0.7479766403524964\n",
      "Test Loss after Epoch 498: 0.8257679870128631\n",
      "5400\n",
      "Epoch 499, Loss: 0.7479995537466473\n",
      "Test Loss after Epoch 499: 0.8310523710250854\n",
      "5400\n",
      "Epoch 500, Loss: 0.7476170014893567\n",
      "Test Loss after Epoch 500: 0.8319736166000367\n",
      "5400\n",
      "Epoch 501, Loss: 0.7482773093824033\n",
      "Test Loss after Epoch 501: 0.835807916879654\n",
      "5400\n",
      "Epoch 502, Loss: 0.7492973933175758\n",
      "Test Loss after Epoch 502: 0.834787058353424\n",
      "5400\n",
      "Epoch 503, Loss: 0.7475511640972561\n",
      "Test Loss after Epoch 503: 0.8307367279529572\n",
      "5400\n",
      "Epoch 504, Loss: 0.7478803869309248\n",
      "Test Loss after Epoch 504: 0.8343912470340729\n",
      "5400\n",
      "Epoch 505, Loss: 0.7458083425627814\n",
      "Test Loss after Epoch 505: 0.8345474176406861\n",
      "5400\n",
      "Epoch 506, Loss: 0.7470504745068374\n",
      "Test Loss after Epoch 506: 0.8293450999259949\n",
      "5400\n",
      "Epoch 507, Loss: 0.7474128391566099\n",
      "Test Loss after Epoch 507: 0.8301983120441436\n",
      "5400\n",
      "Epoch 508, Loss: 0.7461472920576732\n",
      "Test Loss after Epoch 508: 0.832890962600708\n",
      "5400\n",
      "Epoch 509, Loss: 0.7468113611583357\n",
      "Test Loss after Epoch 509: 0.8321577401161194\n",
      "5400\n",
      "Epoch 510, Loss: 0.7466889832637928\n",
      "Test Loss after Epoch 510: 0.8288999729156494\n",
      "5400\n",
      "Epoch 511, Loss: 0.7465252172063899\n",
      "Test Loss after Epoch 511: 0.8297333974838257\n",
      "5400\n",
      "Epoch 512, Loss: 0.7452923632109607\n",
      "Test Loss after Epoch 512: 0.8327470276355743\n",
      "5400\n",
      "Epoch 513, Loss: 0.7460663289935501\n",
      "Test Loss after Epoch 513: 0.8282986474037171\n",
      "5400\n",
      "Epoch 514, Loss: 0.7447663639651404\n",
      "Test Loss after Epoch 514: 0.8293065695762635\n",
      "5400\n",
      "Epoch 515, Loss: 0.7456800562143325\n",
      "Test Loss after Epoch 515: 0.8312527992725373\n",
      "5400\n",
      "Epoch 516, Loss: 0.7457699926473477\n",
      "Test Loss after Epoch 516: 0.8306882159709931\n",
      "5400\n",
      "Epoch 517, Loss: 0.746044526387144\n",
      "Test Loss after Epoch 517: 0.8301495947837829\n",
      "5400\n",
      "Epoch 518, Loss: 0.7464587116903729\n",
      "Test Loss after Epoch 518: 0.8318369479179383\n",
      "5400\n",
      "Epoch 519, Loss: 0.7445999421234484\n",
      "Test Loss after Epoch 519: 0.8290450839996338\n",
      "5400\n",
      "Epoch 520, Loss: 0.7468101908100976\n",
      "Test Loss after Epoch 520: 0.8275271081924438\n",
      "5400\n",
      "Epoch 521, Loss: 0.7457955057311941\n",
      "Test Loss after Epoch 521: 0.8254607973098755\n",
      "5400\n",
      "Epoch 522, Loss: 0.7466347596601204\n",
      "Test Loss after Epoch 522: 0.830604548215866\n",
      "5400\n",
      "Epoch 523, Loss: 0.7440650823160454\n",
      "Test Loss after Epoch 523: 0.8339757363796234\n",
      "5400\n",
      "Epoch 524, Loss: 0.7441978440461335\n",
      "Test Loss after Epoch 524: 0.8311006369590759\n",
      "5400\n",
      "Epoch 525, Loss: 0.7455417312736864\n",
      "Test Loss after Epoch 525: 0.8331159656047821\n",
      "5400\n",
      "Epoch 526, Loss: 0.7438131754707407\n",
      "Test Loss after Epoch 526: 0.836536491394043\n",
      "5400\n",
      "Epoch 527, Loss: 0.7440576541865314\n",
      "Test Loss after Epoch 527: 0.8331438915729523\n",
      "5400\n",
      "Epoch 528, Loss: 0.7464368026565622\n",
      "Test Loss after Epoch 528: 0.8340029091835022\n",
      "5400\n",
      "Epoch 529, Loss: 0.7432730595050034\n",
      "Test Loss after Epoch 529: 0.8365860750675201\n",
      "5400\n",
      "Epoch 530, Loss: 0.7436646228807944\n",
      "Test Loss after Epoch 530: 0.8349046030044556\n",
      "5400\n",
      "Epoch 531, Loss: 0.7458986847709727\n",
      "Test Loss after Epoch 531: 0.8346566605567932\n",
      "5400\n",
      "Epoch 532, Loss: 0.7437505584955215\n",
      "Test Loss after Epoch 532: 0.8339787182807923\n",
      "5400\n",
      "Epoch 533, Loss: 0.7437604479657279\n",
      "Test Loss after Epoch 533: 0.8314509389400482\n",
      "5400\n",
      "Epoch 534, Loss: 0.7466946836974886\n",
      "Test Loss after Epoch 534: 0.8308788414001465\n",
      "5400\n",
      "Epoch 535, Loss: 0.7437676626885379\n",
      "Test Loss after Epoch 535: 0.8342842476367951\n",
      "5400\n",
      "Epoch 536, Loss: 0.743492187416112\n",
      "Test Loss after Epoch 536: 0.8361172068119049\n",
      "5400\n",
      "Epoch 537, Loss: 0.7466974252683145\n",
      "Test Loss after Epoch 537: 0.8351665081977844\n",
      "5400\n",
      "Epoch 538, Loss: 0.7437875766003573\n",
      "Test Loss after Epoch 538: 0.8398698904514312\n",
      "5400\n",
      "Epoch 539, Loss: 0.7452731859242474\n",
      "Test Loss after Epoch 539: 0.8443091106414795\n",
      "5400\n",
      "Epoch 540, Loss: 0.7452949085942021\n",
      "Test Loss after Epoch 540: 0.842057260274887\n",
      "5400\n",
      "Epoch 541, Loss: 0.7429197022870735\n",
      "Test Loss after Epoch 541: 0.8331807692050934\n",
      "5400\n",
      "Epoch 542, Loss: 0.7425328477444472\n",
      "Test Loss after Epoch 542: 0.8304614682197571\n",
      "5400\n",
      "Epoch 543, Loss: 0.7406564803918203\n",
      "Test Loss after Epoch 543: 0.8288987193107605\n",
      "5400\n",
      "Epoch 544, Loss: 0.7435289565942905\n",
      "Test Loss after Epoch 544: 0.8285916991233826\n",
      "5400\n",
      "Epoch 545, Loss: 0.7419015308883455\n",
      "Test Loss after Epoch 545: 0.8277761096954346\n",
      "5400\n",
      "Epoch 546, Loss: 0.7430653080675337\n",
      "Test Loss after Epoch 546: 0.8252769699096679\n",
      "Rolling back to best model from epoch 491\n",
      "Best test loss: 0.8221816339492798\n",
      "5400\n",
      "Epoch 547, Loss: 0.7484239838962202\n",
      "Test Loss after Epoch 547: 0.8216257755756378\n",
      "5400\n",
      "Epoch 548, Loss: 0.7450841141630102\n",
      "Test Loss after Epoch 548: 0.8253964309692383\n",
      "5400\n",
      "Epoch 549, Loss: 0.7441394148049532\n",
      "Test Loss after Epoch 549: 0.8214754085540772\n",
      "5400\n",
      "Epoch 550, Loss: 0.7435259609310715\n",
      "Test Loss after Epoch 550: 0.8214656507968903\n",
      "5400\n",
      "Epoch 551, Loss: 0.7463147149924879\n",
      "Test Loss after Epoch 551: 0.8265752577781678\n",
      "5400\n",
      "Epoch 552, Loss: 0.7463635249711849\n",
      "Test Loss after Epoch 552: 0.8253048760890961\n",
      "5400\n",
      "Epoch 553, Loss: 0.7466813142432107\n",
      "Test Loss after Epoch 553: 0.8275865225791931\n",
      "5400\n",
      "Epoch 554, Loss: 0.7458967876213568\n",
      "Test Loss after Epoch 554: 0.8322451524734497\n",
      "5400\n",
      "Epoch 555, Loss: 0.7451743584209019\n",
      "Test Loss after Epoch 555: 0.8294392833709717\n",
      "5400\n",
      "Epoch 556, Loss: 0.74539701976158\n",
      "Test Loss after Epoch 556: 0.828120849609375\n",
      "5400\n",
      "Epoch 557, Loss: 0.7450847485109612\n",
      "Test Loss after Epoch 557: 0.8269126057624817\n",
      "5400\n",
      "Epoch 558, Loss: 0.7453086388773388\n",
      "Test Loss after Epoch 558: 0.8263116190433503\n",
      "5400\n",
      "Epoch 559, Loss: 0.7458950780497657\n",
      "Test Loss after Epoch 559: 0.8260069074630737\n",
      "5400\n",
      "Epoch 560, Loss: 0.7426549826727973\n",
      "Test Loss after Epoch 560: 0.8285794839859009\n",
      "5400\n",
      "Epoch 561, Loss: 0.7480577111023444\n",
      "Test Loss after Epoch 561: 0.8271168704032898\n",
      "5400\n",
      "Epoch 562, Loss: 0.744274423872983\n",
      "Test Loss after Epoch 562: 0.828462626695633\n",
      "5400\n",
      "Epoch 563, Loss: 0.7449144204457601\n",
      "Test Loss after Epoch 563: 0.8305076923370361\n",
      "5400\n",
      "Epoch 564, Loss: 0.7448047107016599\n",
      "Test Loss after Epoch 564: 0.83197580742836\n",
      "5400\n",
      "Epoch 565, Loss: 0.7458519111297749\n",
      "Test Loss after Epoch 565: 0.8287961957454681\n",
      "5400\n",
      "Epoch 566, Loss: 0.7464350438117981\n",
      "Test Loss after Epoch 566: 0.831071626663208\n",
      "5400\n",
      "Epoch 567, Loss: 0.745260050340935\n",
      "Test Loss after Epoch 567: 0.8268568894863129\n",
      "5400\n",
      "Epoch 568, Loss: 0.7461441481775708\n",
      "Test Loss after Epoch 568: 0.8303691744804382\n",
      "5400\n",
      "Epoch 569, Loss: 0.7452995857706777\n",
      "Test Loss after Epoch 569: 0.8364776868820191\n",
      "5400\n",
      "Epoch 570, Loss: 0.7432533958664647\n",
      "Test Loss after Epoch 570: 0.8362127573490142\n",
      "5400\n",
      "Epoch 571, Loss: 0.7440634389939131\n",
      "Test Loss after Epoch 571: 0.8345042026042938\n",
      "5400\n",
      "Epoch 572, Loss: 0.7435937195795553\n",
      "Test Loss after Epoch 572: 0.8310552928447723\n",
      "5400\n",
      "Epoch 573, Loss: 0.744271254407035\n",
      "Test Loss after Epoch 573: 0.8311217231750488\n",
      "5400\n",
      "Epoch 574, Loss: 0.7441323409257111\n",
      "Test Loss after Epoch 574: 0.8311434710025787\n",
      "5400\n",
      "Epoch 575, Loss: 0.7428481123182509\n",
      "Test Loss after Epoch 575: 0.8383200604915619\n",
      "5400\n",
      "Epoch 576, Loss: 0.7436781941740601\n",
      "Test Loss after Epoch 576: 0.8352966177463531\n",
      "5400\n",
      "Epoch 577, Loss: 0.7440457751353582\n",
      "Test Loss after Epoch 577: 0.8330963776111603\n",
      "5400\n",
      "Epoch 578, Loss: 0.7454530108195764\n",
      "Test Loss after Epoch 578: 0.8337521698474885\n",
      "5400\n",
      "Epoch 579, Loss: 0.7450860723080458\n",
      "Test Loss after Epoch 579: 0.8338976294994355\n",
      "5400\n",
      "Epoch 580, Loss: 0.7442880750364728\n",
      "Test Loss after Epoch 580: 0.8321601538658142\n",
      "5400\n",
      "Epoch 581, Loss: 0.7466584406296413\n",
      "Test Loss after Epoch 581: 0.8313953025341034\n",
      "5400\n",
      "Epoch 582, Loss: 0.7461282464972249\n",
      "Test Loss after Epoch 582: 0.8290443892478943\n",
      "5400\n",
      "Epoch 583, Loss: 0.7427877794813227\n",
      "Test Loss after Epoch 583: 0.8310695807933808\n",
      "5400\n",
      "Epoch 584, Loss: 0.7428950789901946\n",
      "Test Loss after Epoch 584: 0.8267229540348053\n",
      "5400\n",
      "Epoch 585, Loss: 0.7451984404634546\n",
      "Test Loss after Epoch 585: 0.8256447982788085\n",
      "5400\n",
      "Epoch 586, Loss: 0.7421421382603822\n",
      "Test Loss after Epoch 586: 0.8287124710083008\n",
      "5400\n",
      "Epoch 587, Loss: 0.7414057923687829\n",
      "Test Loss after Epoch 587: 0.8295018393993377\n",
      "5400\n",
      "Epoch 588, Loss: 0.74275217294693\n",
      "Test Loss after Epoch 588: 0.8264803194999695\n",
      "5400\n",
      "Epoch 589, Loss: 0.7409373833515026\n",
      "Test Loss after Epoch 589: 0.8319011497497558\n",
      "5400\n",
      "Epoch 590, Loss: 0.7448332418998083\n",
      "Test Loss after Epoch 590: 0.8340221755504608\n",
      "5400\n",
      "Epoch 591, Loss: 0.7436628885622377\n",
      "Test Loss after Epoch 591: 0.8307262027263641\n",
      "5400\n",
      "Epoch 592, Loss: 0.742543092612867\n",
      "Test Loss after Epoch 592: 0.8295855107307434\n",
      "5400\n",
      "Epoch 593, Loss: 0.7410305156751915\n",
      "Test Loss after Epoch 593: 0.8306578931808472\n",
      "5400\n",
      "Epoch 594, Loss: 0.7429743379133719\n",
      "Test Loss after Epoch 594: 0.8328178133964539\n",
      "5400\n",
      "Epoch 595, Loss: 0.7426510326288365\n",
      "Test Loss after Epoch 595: 0.8344475119113922\n",
      "5400\n",
      "Epoch 596, Loss: 0.7410411481945604\n",
      "Test Loss after Epoch 596: 0.8305062055587769\n",
      "5400\n",
      "Epoch 597, Loss: 0.7420882521735297\n",
      "Test Loss after Epoch 597: 0.8301048920154571\n",
      "5400\n",
      "Epoch 598, Loss: 0.7421656238149713\n",
      "Test Loss after Epoch 598: 0.8266947288513183\n",
      "5400\n",
      "Epoch 599, Loss: 0.741128039293819\n",
      "Test Loss after Epoch 599: 0.826625587940216\n",
      "5400\n",
      "Epoch 600, Loss: 0.7402535259723664\n",
      "Test Loss after Epoch 600: 0.8239965968132019\n",
      "5400\n",
      "Epoch 601, Loss: 0.7424263104906789\n",
      "Test Loss after Epoch 601: 0.8236311361789703\n",
      "5400\n",
      "Epoch 602, Loss: 0.7436676610178418\n",
      "Test Loss after Epoch 602: 0.8233111746311188\n",
      "5400\n",
      "Epoch 603, Loss: 0.7433011138880694\n",
      "Test Loss after Epoch 603: 0.8243477263450623\n",
      "5400\n",
      "Epoch 604, Loss: 0.7414501394165887\n",
      "Test Loss after Epoch 604: 0.8288825840950013\n",
      "5400\n",
      "Epoch 605, Loss: 0.7411808379049655\n",
      "Test Loss after Epoch 605: 0.8317994794845581\n",
      "5400\n",
      "Epoch 606, Loss: 0.7425281146279088\n",
      "Test Loss after Epoch 606: 0.8280676074028015\n",
      "5400\n",
      "Epoch 607, Loss: 0.7420301688600469\n",
      "Test Loss after Epoch 607: 0.8297578961849212\n",
      "5400\n",
      "Epoch 608, Loss: 0.7418741193082597\n",
      "Test Loss after Epoch 608: 0.8320387938022613\n",
      "5400\n",
      "Epoch 609, Loss: 0.741712153421508\n",
      "Test Loss after Epoch 609: 0.8330386040210724\n",
      "5400\n",
      "Epoch 610, Loss: 0.7419678746770929\n",
      "Test Loss after Epoch 610: 0.8310675649642945\n",
      "5400\n",
      "Epoch 611, Loss: 0.7427041805232013\n",
      "Test Loss after Epoch 611: 0.8281012969017029\n",
      "5400\n",
      "Epoch 612, Loss: 0.7434717206822501\n",
      "Test Loss after Epoch 612: 0.8315820307731628\n",
      "5400\n",
      "Epoch 613, Loss: 0.7390712201816064\n",
      "Test Loss after Epoch 613: 0.8348807692527771\n",
      "5400\n",
      "Epoch 614, Loss: 0.7413753020984155\n",
      "Test Loss after Epoch 614: 0.8329715585708618\n",
      "5400\n",
      "Epoch 615, Loss: 0.7414060999729015\n",
      "Test Loss after Epoch 615: 0.8308142559528351\n",
      "5400\n",
      "Epoch 616, Loss: 0.7417932166655858\n",
      "Test Loss after Epoch 616: 0.8372514827251434\n",
      "5400\n",
      "Epoch 617, Loss: 0.7439527427929419\n",
      "Test Loss after Epoch 617: 0.836605064868927\n",
      "5400\n",
      "Epoch 618, Loss: 0.7417128224946834\n",
      "Test Loss after Epoch 618: 0.8351027996540069\n",
      "5400\n",
      "Epoch 619, Loss: 0.7414846880126883\n",
      "Test Loss after Epoch 619: 0.8330341601371765\n",
      "5400\n",
      "Epoch 620, Loss: 0.7421043695343865\n",
      "Test Loss after Epoch 620: 0.8321727092266082\n",
      "5400\n",
      "Epoch 621, Loss: 0.7428068405389786\n",
      "Test Loss after Epoch 621: 0.8320651140213012\n",
      "5400\n",
      "Epoch 622, Loss: 0.7422539670599831\n",
      "Test Loss after Epoch 622: 0.8299124820232391\n",
      "5400\n",
      "Epoch 623, Loss: 0.7401187419449842\n",
      "Test Loss after Epoch 623: 0.8263024597167968\n",
      "5400\n",
      "Epoch 624, Loss: 0.7394205752125492\n",
      "Test Loss after Epoch 624: 0.8287024006843567\n",
      "5400\n",
      "Epoch 625, Loss: 0.7409900572785625\n",
      "Test Loss after Epoch 625: 0.8310218665599823\n",
      "5400\n",
      "Epoch 626, Loss: 0.7391815555978705\n",
      "Test Loss after Epoch 626: 0.8323540201187134\n",
      "5400\n",
      "Epoch 627, Loss: 0.7405329356811665\n",
      "Test Loss after Epoch 627: 0.8302647819519043\n",
      "Rolling back to best model from epoch 550\n",
      "Best test loss: 0.8214656507968903\n",
      "5400\n",
      "Epoch 628, Loss: 0.7430328882402844\n",
      "Test Loss after Epoch 628: 0.8211075274944305\n",
      "5400\n",
      "Epoch 629, Loss: 0.7409149010976156\n",
      "Test Loss after Epoch 629: 0.8236174304485321\n",
      "5400\n",
      "Epoch 630, Loss: 0.7410621609731957\n",
      "Test Loss after Epoch 630: 0.8237166697978974\n",
      "5400\n",
      "Epoch 631, Loss: 0.7422037017345429\n",
      "Test Loss after Epoch 631: 0.8288130266666413\n",
      "5400\n",
      "Epoch 632, Loss: 0.7439141522513496\n",
      "Test Loss after Epoch 632: 0.8306488933563232\n",
      "5400\n",
      "Epoch 633, Loss: 0.7412227969920193\n",
      "Test Loss after Epoch 633: 0.8269078876972199\n",
      "5400\n",
      "Epoch 634, Loss: 0.7416644786243085\n",
      "Test Loss after Epoch 634: 0.827537440776825\n",
      "5400\n",
      "Epoch 635, Loss: 0.743444195566354\n",
      "Test Loss after Epoch 635: 0.8312948763370513\n",
      "5400\n",
      "Epoch 636, Loss: 0.7444318409981551\n",
      "Test Loss after Epoch 636: 0.8387519941329956\n",
      "5400\n",
      "Epoch 637, Loss: 0.7451291278777299\n",
      "Test Loss after Epoch 637: 0.8346409511566162\n",
      "5400\n",
      "Epoch 638, Loss: 0.7437194893315986\n",
      "Test Loss after Epoch 638: 0.8326572322845459\n",
      "5400\n",
      "Epoch 639, Loss: 0.7446856668922637\n",
      "Test Loss after Epoch 639: 0.8347575526237487\n",
      "5400\n",
      "Epoch 640, Loss: 0.7420270639657974\n",
      "Test Loss after Epoch 640: 0.8353609960079194\n",
      "5400\n",
      "Epoch 641, Loss: 0.7442472748844712\n",
      "Test Loss after Epoch 641: 0.830686500787735\n",
      "5400\n",
      "Epoch 642, Loss: 0.7443484675442731\n",
      "Test Loss after Epoch 642: 0.8251883742809296\n",
      "5400\n",
      "Epoch 643, Loss: 0.7436791610717773\n",
      "Test Loss after Epoch 643: 0.8244741702079773\n",
      "5400\n",
      "Epoch 644, Loss: 0.741813538979601\n",
      "Test Loss after Epoch 644: 0.826518490076065\n",
      "5400\n",
      "Epoch 645, Loss: 0.7418571935097377\n",
      "Test Loss after Epoch 645: 0.8358040604591369\n",
      "5400\n",
      "Epoch 646, Loss: 0.7444479445174889\n",
      "Test Loss after Epoch 646: 0.8377052519321442\n",
      "5400\n",
      "Epoch 647, Loss: 0.7439463707473543\n",
      "Test Loss after Epoch 647: 0.840525310754776\n",
      "5400\n",
      "Epoch 648, Loss: 0.7439383818705877\n",
      "Test Loss after Epoch 648: 0.8400790948867798\n",
      "5400\n",
      "Epoch 649, Loss: 0.7408899648542757\n",
      "Test Loss after Epoch 649: 0.8308958072662354\n",
      "5400\n",
      "Epoch 650, Loss: 0.7430040655754231\n",
      "Test Loss after Epoch 650: 0.8318339385986329\n",
      "5400\n",
      "Epoch 651, Loss: 0.7458348393881763\n",
      "Test Loss after Epoch 651: 0.8341056251525879\n",
      "5400\n",
      "Epoch 652, Loss: 0.7434654721728078\n",
      "Test Loss after Epoch 652: 0.8284368464946746\n",
      "5400\n",
      "Epoch 653, Loss: 0.7421895071974507\n",
      "Test Loss after Epoch 653: 0.8294393603801727\n",
      "5400\n",
      "Epoch 654, Loss: 0.7404639815621906\n",
      "Test Loss after Epoch 654: 0.8306339399814606\n",
      "5400\n",
      "Epoch 655, Loss: 0.7414087396639365\n",
      "Test Loss after Epoch 655: 0.8285907227993011\n",
      "5400\n",
      "Epoch 656, Loss: 0.7431284298940941\n",
      "Test Loss after Epoch 656: 0.8258898589611053\n",
      "5400\n",
      "Epoch 657, Loss: 0.7407743830371786\n",
      "Test Loss after Epoch 657: 0.8226293094158172\n",
      "5400\n",
      "Epoch 658, Loss: 0.7396945258202376\n",
      "Test Loss after Epoch 658: 0.8224199390411377\n",
      "5400\n",
      "Epoch 659, Loss: 0.7417967053033687\n",
      "Test Loss after Epoch 659: 0.8238888258934021\n",
      "5400\n",
      "Epoch 660, Loss: 0.7417323926863847\n",
      "Test Loss after Epoch 660: 0.8269965598583221\n",
      "5400\n",
      "Epoch 661, Loss: 0.7456296903778006\n",
      "Test Loss after Epoch 661: 0.8294930176734925\n",
      "5400\n",
      "Epoch 662, Loss: 0.7435899924128144\n",
      "Test Loss after Epoch 662: 0.8299513108730316\n",
      "5400\n",
      "Epoch 663, Loss: 0.7431158056082549\n",
      "Test Loss after Epoch 663: 0.8280662944316864\n",
      "5400\n",
      "Epoch 664, Loss: 0.7416121925910314\n",
      "Test Loss after Epoch 664: 0.8263797955513\n",
      "5400\n",
      "Epoch 665, Loss: 0.7408162443284635\n",
      "Test Loss after Epoch 665: 0.83471200299263\n",
      "5400\n",
      "Epoch 666, Loss: 0.7405248430260906\n",
      "Test Loss after Epoch 666: 0.8331542701721192\n",
      "5400\n",
      "Epoch 667, Loss: 0.7410656077994241\n",
      "Test Loss after Epoch 667: 0.8314622964859009\n",
      "5400\n",
      "Epoch 668, Loss: 0.7417363646295335\n",
      "Test Loss after Epoch 668: 0.8310521230697632\n",
      "5400\n",
      "Epoch 669, Loss: 0.7402158589495553\n",
      "Test Loss after Epoch 669: 0.8301902658939362\n",
      "5400\n",
      "Epoch 670, Loss: 0.7393135260211097\n",
      "Test Loss after Epoch 670: 0.8303076422214508\n",
      "5400\n",
      "Epoch 671, Loss: 0.741098360353046\n",
      "Test Loss after Epoch 671: 0.8316826498508454\n",
      "5400\n",
      "Epoch 672, Loss: 0.7422445665244702\n",
      "Test Loss after Epoch 672: 0.8286333072185517\n",
      "5400\n",
      "Epoch 673, Loss: 0.7415405241869114\n",
      "Test Loss after Epoch 673: 0.8254385204315186\n",
      "5400\n",
      "Epoch 674, Loss: 0.7403038914336099\n",
      "Test Loss after Epoch 674: 0.826097627401352\n",
      "5400\n",
      "Epoch 675, Loss: 0.7418062155776554\n",
      "Test Loss after Epoch 675: 0.82762731051445\n",
      "5400\n",
      "Epoch 676, Loss: 0.742906318947121\n",
      "Test Loss after Epoch 676: 0.8253809328079224\n",
      "5400\n",
      "Epoch 677, Loss: 0.741683144569397\n",
      "Test Loss after Epoch 677: 0.8310626063346863\n",
      "5400\n",
      "Epoch 678, Loss: 0.7417176064738521\n",
      "Test Loss after Epoch 678: 0.8301303181648254\n",
      "5400\n",
      "Epoch 679, Loss: 0.7435966284849026\n",
      "Test Loss after Epoch 679: 0.8314333090782166\n",
      "5400\n",
      "Epoch 680, Loss: 0.7415297091007232\n",
      "Test Loss after Epoch 680: 0.8341412127017975\n",
      "5400\n",
      "Epoch 681, Loss: 0.7418851536291617\n",
      "Test Loss after Epoch 681: 0.8318331451416016\n",
      "5400\n",
      "Epoch 682, Loss: 0.7409693106898555\n",
      "Test Loss after Epoch 682: 0.8283515617847442\n",
      "5400\n",
      "Epoch 683, Loss: 0.7410784200165007\n",
      "Test Loss after Epoch 683: 0.8299684183597564\n",
      "5400\n",
      "Epoch 684, Loss: 0.7401241112196887\n",
      "Test Loss after Epoch 684: 0.8272103815078735\n",
      "5400\n",
      "Epoch 685, Loss: 0.7412339746510541\n",
      "Test Loss after Epoch 685: 0.8230642340183258\n",
      "5400\n",
      "Epoch 686, Loss: 0.7414105393268444\n",
      "Test Loss after Epoch 686: 0.8254999186992645\n",
      "5400\n",
      "Epoch 687, Loss: 0.7419853199852837\n",
      "Test Loss after Epoch 687: 0.8315678496360779\n",
      "5400\n",
      "Epoch 688, Loss: 0.740656841132376\n",
      "Test Loss after Epoch 688: 0.83215500497818\n",
      "5400\n",
      "Epoch 689, Loss: 0.739486481723962\n",
      "Test Loss after Epoch 689: 0.8342255454063415\n",
      "5400\n",
      "Epoch 690, Loss: 0.7395812309450573\n",
      "Test Loss after Epoch 690: 0.8337912085056305\n",
      "5400\n",
      "Epoch 691, Loss: 0.740576781100697\n",
      "Test Loss after Epoch 691: 0.8312225303649903\n",
      "5400\n",
      "Epoch 692, Loss: 0.7400823560246715\n",
      "Test Loss after Epoch 692: 0.831242553472519\n",
      "5400\n",
      "Epoch 693, Loss: 0.7384390406476127\n",
      "Test Loss after Epoch 693: 0.8327769596576691\n",
      "5400\n",
      "Epoch 694, Loss: 0.7396532271967994\n",
      "Test Loss after Epoch 694: 0.836512912273407\n",
      "5400\n",
      "Epoch 695, Loss: 0.7395701396023786\n",
      "Test Loss after Epoch 695: 0.8301473231315613\n",
      "5400\n",
      "Epoch 696, Loss: 0.7412447200218837\n",
      "Test Loss after Epoch 696: 0.8286102843284607\n",
      "5400\n",
      "Epoch 697, Loss: 0.7403738185432223\n",
      "Test Loss after Epoch 697: 0.8309904797077179\n",
      "5400\n",
      "Epoch 698, Loss: 0.7407324406394252\n",
      "Test Loss after Epoch 698: 0.8277632489204406\n",
      "5400\n",
      "Epoch 699, Loss: 0.7400515620796769\n",
      "Test Loss after Epoch 699: 0.8291656630039215\n",
      "5400\n",
      "Epoch 700, Loss: 0.7416277606840487\n",
      "Test Loss after Epoch 700: 0.8328294284343719\n",
      "5400\n",
      "Epoch 701, Loss: 0.7386686061488258\n",
      "Test Loss after Epoch 701: 0.830480537891388\n",
      "5400\n",
      "Epoch 702, Loss: 0.7411782846406654\n",
      "Test Loss after Epoch 702: 0.8304271111488343\n",
      "5400\n",
      "Epoch 703, Loss: 0.7389768708856017\n",
      "Test Loss after Epoch 703: 0.8341145310401916\n",
      "5400\n",
      "Epoch 704, Loss: 0.7395373540012925\n",
      "Test Loss after Epoch 704: 0.8349640994071961\n",
      "5400\n",
      "Epoch 705, Loss: 0.7394710662409111\n",
      "Test Loss after Epoch 705: 0.8413712296485901\n",
      "5400\n",
      "Epoch 706, Loss: 0.7403266922853611\n",
      "Test Loss after Epoch 706: 0.841433084487915\n",
      "5400\n",
      "Epoch 707, Loss: 0.7398112908336851\n",
      "Test Loss after Epoch 707: 0.8331847167015076\n",
      "5400\n",
      "Epoch 708, Loss: 0.7384948266435553\n",
      "Test Loss after Epoch 708: 0.8282496700286865\n",
      "5400\n",
      "Epoch 709, Loss: 0.7404407072729534\n",
      "Test Loss after Epoch 709: 0.8277170414924622\n",
      "5400\n",
      "Epoch 710, Loss: 0.7381354062866281\n",
      "Test Loss after Epoch 710: 0.8277626066207886\n",
      "5400\n",
      "Epoch 711, Loss: 0.7391474327996925\n",
      "Test Loss after Epoch 711: 0.8297923753261566\n",
      "5400\n",
      "Epoch 712, Loss: 0.7401889439203121\n",
      "Test Loss after Epoch 712: 0.8303666594028473\n",
      "5400\n",
      "Epoch 713, Loss: 0.7394798317220476\n",
      "Test Loss after Epoch 713: 0.8267447497844697\n",
      "5400\n",
      "Epoch 714, Loss: 0.7388595321222587\n",
      "Test Loss after Epoch 714: 0.830760754108429\n",
      "5400\n",
      "Epoch 715, Loss: 0.7393840404130795\n",
      "Test Loss after Epoch 715: 0.8315911526679993\n",
      "5400\n",
      "Epoch 716, Loss: 0.738762730801547\n",
      "Test Loss after Epoch 716: 0.8291705236434936\n",
      "5400\n",
      "Epoch 717, Loss: 0.738544446406541\n",
      "Test Loss after Epoch 717: 0.8297210245132446\n",
      "5400\n",
      "Epoch 718, Loss: 0.7385391929856053\n",
      "Test Loss after Epoch 718: 0.8285242671966553\n",
      "5400\n",
      "Epoch 719, Loss: 0.7369066797362434\n",
      "Test Loss after Epoch 719: 0.8268861703872681\n",
      "5400\n",
      "Epoch 720, Loss: 0.7385770304997762\n",
      "Test Loss after Epoch 720: 0.828761292219162\n",
      "5400\n",
      "Epoch 721, Loss: 0.7407790446943707\n",
      "Test Loss after Epoch 721: 0.8317310683727265\n",
      "5400\n",
      "Epoch 722, Loss: 0.7409201252018964\n",
      "Test Loss after Epoch 722: 0.833328547000885\n",
      "5400\n",
      "Epoch 723, Loss: 0.7409214735251886\n",
      "Test Loss after Epoch 723: 0.8348446850776672\n",
      "5400\n",
      "Epoch 724, Loss: 0.7382410332008644\n",
      "Test Loss after Epoch 724: 0.8389119732379914\n",
      "5400\n",
      "Epoch 725, Loss: 0.7402712964128565\n",
      "Test Loss after Epoch 725: 0.8350184369087219\n",
      "5400\n",
      "Epoch 726, Loss: 0.740458604031139\n",
      "Test Loss after Epoch 726: 0.8364458158016205\n",
      "5400\n",
      "Epoch 727, Loss: 0.7398074666438279\n",
      "Test Loss after Epoch 727: 0.8424926316738128\n",
      "5400\n",
      "Epoch 728, Loss: 0.7399634096578316\n",
      "Test Loss after Epoch 728: 0.8397026767730713\n",
      "5400\n",
      "Epoch 729, Loss: 0.73822786476877\n",
      "Test Loss after Epoch 729: 0.8367347524166108\n",
      "5400\n",
      "Epoch 730, Loss: 0.7371187113391029\n",
      "Test Loss after Epoch 730: 0.8357414710521698\n",
      "5400\n",
      "Epoch 731, Loss: 0.7391933537854088\n",
      "Test Loss after Epoch 731: 0.8366826179027558\n",
      "5400\n",
      "Epoch 732, Loss: 0.7386231090404369\n",
      "Test Loss after Epoch 732: 0.8314597365856171\n",
      "5400\n",
      "Epoch 733, Loss: 0.7388715174683819\n",
      "Test Loss after Epoch 733: 0.8263808705806732\n",
      "5400\n",
      "Epoch 734, Loss: 0.7389491903119617\n",
      "Test Loss after Epoch 734: 0.8315310580730438\n",
      "5400\n",
      "Epoch 735, Loss: 0.7393239481360824\n",
      "Test Loss after Epoch 735: 0.8312484455108643\n",
      "5400\n",
      "Epoch 736, Loss: 0.739976384109921\n",
      "Test Loss after Epoch 736: 0.833426346540451\n",
      "Rolling back to best model from epoch 628\n",
      "Best test loss: 0.8211075274944305\n",
      "5400\n",
      "Epoch 737, Loss: 0.7405402601869018\n",
      "Test Loss after Epoch 737: 0.8208053126335144\n",
      "5400\n",
      "Epoch 738, Loss: 0.7419979267870939\n",
      "Test Loss after Epoch 738: 0.824179408788681\n",
      "5400\n",
      "Epoch 739, Loss: 0.7420140464438333\n",
      "Test Loss after Epoch 739: 0.8289571297168732\n",
      "5400\n",
      "Epoch 740, Loss: 0.7402104920811123\n",
      "Test Loss after Epoch 740: 0.8287845399379731\n",
      "5400\n",
      "Epoch 741, Loss: 0.7421384106079737\n",
      "Test Loss after Epoch 741: 0.8248328931331634\n",
      "5400\n",
      "Epoch 742, Loss: 0.7419003424379561\n",
      "Test Loss after Epoch 742: 0.822798714876175\n",
      "5400\n",
      "Epoch 743, Loss: 0.7425270333334252\n",
      "Test Loss after Epoch 743: 0.8242098970413207\n",
      "5400\n",
      "Epoch 744, Loss: 0.7421321082115173\n",
      "Test Loss after Epoch 744: 0.8226686980724335\n",
      "5400\n",
      "Epoch 745, Loss: 0.7433317790208039\n",
      "Test Loss after Epoch 745: 0.8251355571746826\n",
      "5400\n",
      "Epoch 746, Loss: 0.7428694780005349\n",
      "Test Loss after Epoch 746: 0.8281323065757752\n",
      "5400\n",
      "Epoch 747, Loss: 0.7432769326368968\n",
      "Test Loss after Epoch 747: 0.8323752110004425\n",
      "5400\n",
      "Epoch 748, Loss: 0.7420790357722177\n",
      "Test Loss after Epoch 748: 0.837752674818039\n",
      "5400\n",
      "Epoch 749, Loss: 0.7399876694546805\n",
      "Test Loss after Epoch 749: 0.8315522549152374\n",
      "5400\n",
      "Epoch 750, Loss: 0.7417923711626618\n",
      "Test Loss after Epoch 750: 0.8291557660102844\n",
      "5400\n",
      "Epoch 751, Loss: 0.7445412406214962\n",
      "Test Loss after Epoch 751: 0.8309663832187653\n",
      "5400\n",
      "Epoch 752, Loss: 0.7438826324763121\n",
      "Test Loss after Epoch 752: 0.8338162305355072\n",
      "5400\n",
      "Epoch 753, Loss: 0.7400259632313693\n",
      "Test Loss after Epoch 753: 0.8360453171730041\n",
      "5400\n",
      "Epoch 754, Loss: 0.7427534691492717\n",
      "Test Loss after Epoch 754: 0.8367715008258819\n",
      "5400\n",
      "Epoch 755, Loss: 0.7423970269494586\n",
      "Test Loss after Epoch 755: 0.8352739114761353\n",
      "5400\n",
      "Epoch 756, Loss: 0.7430928349494934\n",
      "Test Loss after Epoch 756: 0.8323410642147064\n",
      "5400\n",
      "Epoch 757, Loss: 0.7419623692609646\n",
      "Test Loss after Epoch 757: 0.8326856315135955\n",
      "5400\n",
      "Epoch 758, Loss: 0.7409091565344069\n",
      "Test Loss after Epoch 758: 0.829791533946991\n",
      "5400\n",
      "Epoch 759, Loss: 0.7413621099348422\n",
      "Test Loss after Epoch 759: 0.8279877877235413\n",
      "5400\n",
      "Epoch 760, Loss: 0.7423243390189277\n",
      "Test Loss after Epoch 760: 0.8278981504440308\n",
      "5400\n",
      "Epoch 761, Loss: 0.7430253108342488\n",
      "Test Loss after Epoch 761: 0.8272977919578552\n",
      "5400\n",
      "Epoch 762, Loss: 0.7421934383666073\n",
      "Test Loss after Epoch 762: 0.8271818876266479\n",
      "5400\n",
      "Epoch 763, Loss: 0.7431424718874472\n",
      "Test Loss after Epoch 763: 0.8263100278377533\n",
      "5400\n",
      "Epoch 764, Loss: 0.7423802177773582\n",
      "Test Loss after Epoch 764: 0.8256220963001252\n",
      "5400\n",
      "Epoch 765, Loss: 0.7413647345922612\n",
      "Test Loss after Epoch 765: 0.8277150955200195\n",
      "5400\n",
      "Epoch 766, Loss: 0.7410933977586251\n",
      "Test Loss after Epoch 766: 0.8315077669620514\n",
      "5400\n",
      "Epoch 767, Loss: 0.7413528981032195\n",
      "Test Loss after Epoch 767: 0.8309705340862275\n",
      "5400\n",
      "Epoch 768, Loss: 0.7411757591256389\n",
      "Test Loss after Epoch 768: 0.8298758409023285\n",
      "5400\n",
      "Epoch 769, Loss: 0.7415900361317176\n",
      "Test Loss after Epoch 769: 0.8293395330905914\n",
      "5400\n",
      "Epoch 770, Loss: 0.7411981393672802\n",
      "Test Loss after Epoch 770: 0.8302111327648163\n",
      "5400\n",
      "Epoch 771, Loss: 0.7421911947815506\n",
      "Test Loss after Epoch 771: 0.8320015983581543\n",
      "5400\n",
      "Epoch 772, Loss: 0.7401714007722007\n",
      "Test Loss after Epoch 772: 0.8305706660747528\n",
      "5400\n",
      "Epoch 773, Loss: 0.7392300670235246\n",
      "Test Loss after Epoch 773: 0.8316784145832061\n",
      "5400\n",
      "Epoch 774, Loss: 0.7424162914134838\n",
      "Test Loss after Epoch 774: 0.8369978568553924\n",
      "5400\n",
      "Epoch 775, Loss: 0.7434113391902711\n",
      "Test Loss after Epoch 775: 0.8359380843639374\n",
      "5400\n",
      "Epoch 776, Loss: 0.7420792914099164\n",
      "Test Loss after Epoch 776: 0.8352093319892884\n",
      "5400\n",
      "Epoch 777, Loss: 0.7411338391789684\n",
      "Test Loss after Epoch 777: 0.8328167245388031\n",
      "5400\n",
      "Epoch 778, Loss: 0.7425550010027708\n",
      "Test Loss after Epoch 778: 0.8322417192459106\n",
      "5400\n",
      "Epoch 779, Loss: 0.7398931420070154\n",
      "Test Loss after Epoch 779: 0.8352181975841522\n",
      "5400\n",
      "Epoch 780, Loss: 0.7383565398057302\n",
      "Test Loss after Epoch 780: 0.8358974812030793\n",
      "5400\n",
      "Epoch 781, Loss: 0.7411165145370695\n",
      "Test Loss after Epoch 781: 0.8319047665596009\n",
      "5400\n",
      "Epoch 782, Loss: 0.7387142226651863\n",
      "Test Loss after Epoch 782: 0.835775978565216\n",
      "5400\n",
      "Epoch 783, Loss: 0.7413975468829826\n",
      "Test Loss after Epoch 783: 0.8363962292671203\n",
      "5400\n",
      "Epoch 784, Loss: 0.7409600320348033\n",
      "Test Loss after Epoch 784: 0.8298265452384949\n",
      "5400\n",
      "Epoch 785, Loss: 0.7413246077299118\n",
      "Test Loss after Epoch 785: 0.8304277338981628\n",
      "5400\n",
      "Epoch 786, Loss: 0.7393486928719062\n",
      "Test Loss after Epoch 786: 0.8312279534339905\n",
      "5400\n",
      "Epoch 787, Loss: 0.7403691066856738\n",
      "Test Loss after Epoch 787: 0.825851623058319\n",
      "5400\n",
      "Epoch 788, Loss: 0.7397663980943185\n",
      "Test Loss after Epoch 788: 0.8283208239078522\n",
      "5400\n",
      "Epoch 789, Loss: 0.7381992682704219\n",
      "Test Loss after Epoch 789: 0.8278061792850494\n",
      "5400\n",
      "Epoch 790, Loss: 0.7403439587575418\n",
      "Test Loss after Epoch 790: 0.8213375322818756\n",
      "5400\n",
      "Epoch 791, Loss: 0.7387652541089941\n",
      "Test Loss after Epoch 791: 0.8232478110790252\n",
      "5400\n",
      "Epoch 792, Loss: 0.7412887375884586\n",
      "Test Loss after Epoch 792: 0.8250926861763\n",
      "5400\n",
      "Epoch 793, Loss: 0.7381270384346997\n",
      "Test Loss after Epoch 793: 0.8244604864120484\n",
      "5400\n",
      "Epoch 794, Loss: 0.738923824010072\n",
      "Test Loss after Epoch 794: 0.8233422095775604\n",
      "5400\n",
      "Epoch 795, Loss: 0.7373467844945413\n",
      "Test Loss after Epoch 795: 0.8236602110862732\n",
      "5400\n",
      "Epoch 796, Loss: 0.7385288196581381\n",
      "Test Loss after Epoch 796: 0.8228720977306366\n",
      "5400\n",
      "Epoch 797, Loss: 0.7398463459588863\n",
      "Test Loss after Epoch 797: 0.8232314169406891\n",
      "5400\n",
      "Epoch 798, Loss: 0.7386587089741672\n",
      "Test Loss after Epoch 798: 0.8269343495368957\n",
      "5400\n",
      "Epoch 799, Loss: 0.7380475368323149\n",
      "Test Loss after Epoch 799: 0.8285755598545075\n",
      "5400\n",
      "Epoch 800, Loss: 0.7383667874777758\n",
      "Test Loss after Epoch 800: 0.8280483701229095\n",
      "5400\n",
      "Epoch 801, Loss: 0.7392133491127579\n",
      "Test Loss after Epoch 801: 0.8274763216972351\n",
      "5400\n",
      "Epoch 802, Loss: 0.7388255293060232\n",
      "Test Loss after Epoch 802: 0.8284223952293396\n",
      "5400\n",
      "Epoch 803, Loss: 0.7395303367906146\n",
      "Test Loss after Epoch 803: 0.8308074736595153\n",
      "5400\n",
      "Epoch 804, Loss: 0.7380834360917409\n",
      "Test Loss after Epoch 804: 0.8309977655410766\n",
      "5400\n",
      "Epoch 805, Loss: 0.7381472622685962\n",
      "Test Loss after Epoch 805: 0.8294811582565308\n",
      "5400\n",
      "Epoch 806, Loss: 0.7382667465342416\n",
      "Test Loss after Epoch 806: 0.8256091485023499\n",
      "5400\n",
      "Epoch 807, Loss: 0.7392915073368285\n",
      "Test Loss after Epoch 807: 0.8294218208789825\n",
      "5400\n",
      "Epoch 808, Loss: 0.7386700804145248\n",
      "Test Loss after Epoch 808: 0.8347502608299255\n",
      "5400\n",
      "Epoch 809, Loss: 0.7375380863966765\n",
      "Test Loss after Epoch 809: 0.832621618270874\n",
      "5400\n",
      "Epoch 810, Loss: 0.7387570286459393\n",
      "Test Loss after Epoch 810: 0.8301193478107453\n",
      "5400\n",
      "Epoch 811, Loss: 0.7378668640278003\n",
      "Test Loss after Epoch 811: 0.829141776561737\n",
      "5400\n",
      "Epoch 812, Loss: 0.7381751660947447\n",
      "Test Loss after Epoch 812: 0.8276354670524597\n",
      "5400\n",
      "Epoch 813, Loss: 0.7395250008503595\n",
      "Test Loss after Epoch 813: 0.8292164237499237\n",
      "5400\n",
      "Epoch 814, Loss: 0.73799184984631\n",
      "Test Loss after Epoch 814: 0.8303923482894897\n",
      "5400\n",
      "Epoch 815, Loss: 0.7380230279984298\n",
      "Test Loss after Epoch 815: 0.8331002516746521\n",
      "5400\n",
      "Epoch 816, Loss: 0.7390250138000206\n",
      "Test Loss after Epoch 816: 0.8318186810016632\n",
      "5400\n",
      "Epoch 817, Loss: 0.73816077472987\n",
      "Test Loss after Epoch 817: 0.832344496011734\n",
      "5400\n",
      "Epoch 818, Loss: 0.7378068583762204\n",
      "Test Loss after Epoch 818: 0.8349627432823181\n",
      "5400\n",
      "Epoch 819, Loss: 0.7390953652947038\n",
      "Test Loss after Epoch 819: 0.8341760792732239\n",
      "5400\n",
      "Epoch 820, Loss: 0.737933267377041\n",
      "Test Loss after Epoch 820: 0.8338670475482941\n",
      "5400\n",
      "Epoch 821, Loss: 0.7390270519918866\n",
      "Test Loss after Epoch 821: 0.8316058280467987\n",
      "5400\n",
      "Epoch 822, Loss: 0.7383230826810554\n",
      "Test Loss after Epoch 822: 0.8293843138217926\n",
      "5400\n",
      "Epoch 823, Loss: 0.739028581932739\n",
      "Test Loss after Epoch 823: 0.8257133362293243\n",
      "5400\n",
      "Epoch 824, Loss: 0.7386923254640014\n",
      "Test Loss after Epoch 824: 0.8259242405891418\n",
      "5400\n",
      "Epoch 825, Loss: 0.7382937845918868\n",
      "Test Loss after Epoch 825: 0.8232722253799438\n",
      "5400\n",
      "Epoch 826, Loss: 0.7375975373056199\n",
      "Test Loss after Epoch 826: 0.8247971720695496\n",
      "5400\n",
      "Epoch 827, Loss: 0.7386083537560922\n",
      "Test Loss after Epoch 827: 0.8319447793960572\n",
      "5400\n",
      "Epoch 828, Loss: 0.7398686568604576\n",
      "Test Loss after Epoch 828: 0.8318518476486206\n",
      "5400\n",
      "Epoch 829, Loss: 0.7385687807533476\n",
      "Test Loss after Epoch 829: 0.8291124265193939\n",
      "5400\n",
      "Epoch 830, Loss: 0.7368156657174781\n",
      "Test Loss after Epoch 830: 0.8244482634067536\n",
      "5400\n",
      "Epoch 831, Loss: 0.7380322034712191\n",
      "Test Loss after Epoch 831: 0.8239004175662994\n",
      "5400\n",
      "Epoch 832, Loss: 0.7364372469319238\n",
      "Test Loss after Epoch 832: 0.8309345552921296\n",
      "5400\n",
      "Epoch 833, Loss: 0.7369782627732665\n",
      "Test Loss after Epoch 833: 0.8391565387248993\n",
      "5400\n",
      "Epoch 834, Loss: 0.7365421277505381\n",
      "Test Loss after Epoch 834: 0.837732914686203\n",
      "5400\n",
      "Epoch 835, Loss: 0.7372165594277559\n",
      "Test Loss after Epoch 835: 0.8358286578655243\n",
      "5400\n",
      "Epoch 836, Loss: 0.738024514361664\n",
      "Test Loss after Epoch 836: 0.8322707254886628\n",
      "5400\n",
      "Epoch 837, Loss: 0.7372143703478354\n",
      "Test Loss after Epoch 837: 0.8285526254177094\n",
      "5400\n",
      "Epoch 838, Loss: 0.7377081264389885\n",
      "Test Loss after Epoch 838: 0.8317190630435943\n",
      "5400\n",
      "Epoch 839, Loss: 0.7381671990509386\n",
      "Test Loss after Epoch 839: 0.8330398008823395\n",
      "5400\n",
      "Epoch 840, Loss: 0.7362567310200797\n",
      "Test Loss after Epoch 840: 0.8304638907909393\n",
      "5400\n",
      "Epoch 841, Loss: 0.7364730621488006\n",
      "Test Loss after Epoch 841: 0.8310603561401367\n",
      "5400\n",
      "Epoch 842, Loss: 0.7376843925317128\n",
      "Test Loss after Epoch 842: 0.8336648013591766\n",
      "5400\n",
      "Epoch 843, Loss: 0.7370392442632605\n",
      "Test Loss after Epoch 843: 0.8337281141281128\n",
      "5400\n",
      "Epoch 844, Loss: 0.7365101794401805\n",
      "Test Loss after Epoch 844: 0.8362512791156769\n",
      "5400\n",
      "Epoch 845, Loss: 0.7371108044297607\n",
      "Test Loss after Epoch 845: 0.8369426782131195\n",
      "5400\n",
      "Epoch 846, Loss: 0.7371012120555949\n",
      "Test Loss after Epoch 846: 0.8327204420566559\n",
      "5400\n",
      "Epoch 847, Loss: 0.7367126959783059\n",
      "Test Loss after Epoch 847: 0.8299128081798554\n",
      "5400\n",
      "Epoch 848, Loss: 0.7375135612487793\n",
      "Test Loss after Epoch 848: 0.8293640694618225\n",
      "5400\n",
      "Epoch 849, Loss: 0.7369746185011334\n",
      "Test Loss after Epoch 849: 0.8291168749332428\n",
      "5400\n",
      "Epoch 850, Loss: 0.737928425780049\n",
      "Test Loss after Epoch 850: 0.8312399084568024\n",
      "5400\n",
      "Epoch 851, Loss: 0.7374952029519611\n",
      "Test Loss after Epoch 851: 0.8323968212604522\n",
      "5400\n",
      "Epoch 852, Loss: 0.7362539322950222\n",
      "Test Loss after Epoch 852: 0.8303225374221802\n",
      "5400\n",
      "Epoch 853, Loss: 0.7362321901983685\n",
      "Test Loss after Epoch 853: 0.8267059435844422\n",
      "5400\n",
      "Epoch 854, Loss: 0.7374463646941715\n",
      "Test Loss after Epoch 854: 0.8278491981029511\n",
      "5400\n",
      "Epoch 855, Loss: 0.7360997049676048\n",
      "Test Loss after Epoch 855: 0.8285158040523529\n",
      "5400\n",
      "Epoch 856, Loss: 0.73763898882601\n",
      "Test Loss after Epoch 856: 0.8302522385120392\n",
      "5400\n",
      "Epoch 857, Loss: 0.7359664096214154\n",
      "Test Loss after Epoch 857: 0.8313436806201935\n",
      "5400\n",
      "Epoch 858, Loss: 0.7369455105287057\n",
      "Test Loss after Epoch 858: 0.8310442872047424\n",
      "5400\n",
      "Epoch 859, Loss: 0.7361491896708806\n",
      "Test Loss after Epoch 859: 0.8315025870800018\n",
      "5400\n",
      "Epoch 860, Loss: 0.736365340926029\n",
      "Test Loss after Epoch 860: 0.8301822779178619\n",
      "5400\n",
      "Epoch 861, Loss: 0.7383607040511238\n",
      "Test Loss after Epoch 861: 0.8315064551830291\n",
      "5400\n",
      "Epoch 862, Loss: 0.7371613578443174\n",
      "Test Loss after Epoch 862: 0.8335470623970032\n",
      "5400\n",
      "Epoch 863, Loss: 0.7363006833306065\n",
      "Test Loss after Epoch 863: 0.8334556789398193\n",
      "5400\n",
      "Epoch 864, Loss: 0.738118649257554\n",
      "Test Loss after Epoch 864: 0.8286628720760345\n",
      "5400\n",
      "Epoch 865, Loss: 0.7365228628670728\n",
      "Test Loss after Epoch 865: 0.8270512216091156\n",
      "5400\n",
      "Epoch 866, Loss: 0.7373214116802922\n",
      "Test Loss after Epoch 866: 0.8310244591236114\n",
      "5400\n",
      "Epoch 867, Loss: 0.7367235133383009\n",
      "Test Loss after Epoch 867: 0.831321384191513\n",
      "5400\n",
      "Epoch 868, Loss: 0.7388153548593874\n",
      "Test Loss after Epoch 868: 0.829113436460495\n",
      "5400\n",
      "Epoch 869, Loss: 0.7357536090965624\n",
      "Test Loss after Epoch 869: 0.8322392249107361\n",
      "5400\n",
      "Epoch 870, Loss: 0.7364563511698334\n",
      "Test Loss after Epoch 870: 0.8324357295036315\n",
      "5400\n",
      "Epoch 871, Loss: 0.7360245042597806\n",
      "Test Loss after Epoch 871: 0.8319065172672272\n",
      "5400\n",
      "Epoch 872, Loss: 0.7375248127954978\n",
      "Test Loss after Epoch 872: 0.8364488906860351\n",
      "5400\n",
      "Epoch 873, Loss: 0.7355816220133393\n",
      "Test Loss after Epoch 873: 0.8353828501701355\n",
      "5400\n",
      "Epoch 874, Loss: 0.736966724925571\n",
      "Test Loss after Epoch 874: 0.8303704969882965\n",
      "5400\n",
      "Epoch 875, Loss: 0.7345556185863636\n",
      "Test Loss after Epoch 875: 0.8301328327655793\n",
      "5400\n",
      "Epoch 876, Loss: 0.7379507284915006\n",
      "Test Loss after Epoch 876: 0.8332149858474731\n",
      "5400\n",
      "Epoch 877, Loss: 0.7359400488049895\n",
      "Test Loss after Epoch 877: 0.8334037244319916\n",
      "5400\n",
      "Epoch 878, Loss: 0.7370472888372562\n",
      "Test Loss after Epoch 878: 0.8317705144882203\n",
      "5400\n",
      "Epoch 879, Loss: 0.7354746536413829\n",
      "Test Loss after Epoch 879: 0.8305418133735657\n",
      "5400\n",
      "Epoch 880, Loss: 0.7362295736648419\n",
      "Test Loss after Epoch 880: 0.8312229213714599\n",
      "5400\n",
      "Epoch 881, Loss: 0.7361152141624027\n",
      "Test Loss after Epoch 881: 0.8329133665561677\n",
      "5400\n",
      "Epoch 882, Loss: 0.7363175273162347\n",
      "Test Loss after Epoch 882: 0.8323328523635865\n",
      "5400\n",
      "Epoch 883, Loss: 0.7344770993789037\n",
      "Test Loss after Epoch 883: 0.8294059436321258\n",
      "5400\n",
      "Epoch 884, Loss: 0.7369895586702558\n",
      "Test Loss after Epoch 884: 0.8315624632835388\n",
      "5400\n",
      "Epoch 885, Loss: 0.7348435258865357\n",
      "Test Loss after Epoch 885: 0.8336606805324555\n",
      "5400\n",
      "Epoch 886, Loss: 0.7360198172816523\n",
      "Test Loss after Epoch 886: 0.8308855929374694\n",
      "5400\n",
      "Epoch 887, Loss: 0.735758846424244\n",
      "Test Loss after Epoch 887: 0.8326418948173523\n",
      "5400\n",
      "Epoch 888, Loss: 0.7362618046336704\n",
      "Test Loss after Epoch 888: 0.8302900612354278\n",
      "5400\n",
      "Epoch 889, Loss: 0.7358756192525228\n",
      "Test Loss after Epoch 889: 0.8265685663223267\n",
      "Rolling back to best model from epoch 737\n",
      "Best test loss: 0.8208053126335144\n",
      "5400\n",
      "Epoch 890, Loss: 0.7421178844460735\n",
      "Test Loss after Epoch 890: 0.821518850326538\n",
      "5400\n",
      "Epoch 891, Loss: 0.7406364610018553\n",
      "Test Loss after Epoch 891: 0.8218903892040252\n",
      "5400\n",
      "Epoch 892, Loss: 0.7400890000881972\n",
      "Test Loss after Epoch 892: 0.8215825517177582\n",
      "5400\n",
      "Epoch 893, Loss: 0.7391043418866616\n",
      "Test Loss after Epoch 893: 0.8204747023582458\n",
      "5400\n",
      "Epoch 894, Loss: 0.7417181728945839\n",
      "Test Loss after Epoch 894: 0.8215833525657654\n",
      "5400\n",
      "Epoch 895, Loss: 0.7399513280391693\n",
      "Test Loss after Epoch 895: 0.8223382167816162\n",
      "5400\n",
      "Epoch 896, Loss: 0.7417428212033378\n",
      "Test Loss after Epoch 896: 0.8238721315860749\n",
      "5400\n",
      "Epoch 897, Loss: 0.7415437090617639\n",
      "Test Loss after Epoch 897: 0.8238564920425415\n",
      "5400\n",
      "Epoch 898, Loss: 0.7412901187605329\n",
      "Test Loss after Epoch 898: 0.8268502435684204\n",
      "5400\n",
      "Epoch 899, Loss: 0.7412714671426349\n",
      "Test Loss after Epoch 899: 0.8267782633304596\n",
      "5400\n",
      "Epoch 900, Loss: 0.741625908502826\n",
      "Test Loss after Epoch 900: 0.8295644946098327\n",
      "5400\n",
      "Epoch 901, Loss: 0.7428838794981992\n",
      "Test Loss after Epoch 901: 0.8231377513408661\n",
      "5400\n",
      "Epoch 902, Loss: 0.7422971897875821\n",
      "Test Loss after Epoch 902: 0.825173816204071\n",
      "5400\n",
      "Epoch 903, Loss: 0.7413693029571462\n",
      "Test Loss after Epoch 903: 0.8269688646793365\n",
      "5400\n",
      "Epoch 904, Loss: 0.7408300297348588\n",
      "Test Loss after Epoch 904: 0.8279645466804504\n",
      "5400\n",
      "Epoch 905, Loss: 0.7404332656992806\n",
      "Test Loss after Epoch 905: 0.8335168943405151\n",
      "5400\n",
      "Epoch 906, Loss: 0.7406055925510547\n",
      "Test Loss after Epoch 906: 0.8337394556999207\n",
      "5400\n",
      "Epoch 907, Loss: 0.7404937361346351\n",
      "Test Loss after Epoch 907: 0.8324914150238037\n",
      "5400\n",
      "Epoch 908, Loss: 0.739645971832452\n",
      "Test Loss after Epoch 908: 0.8354549186229706\n",
      "5400\n",
      "Epoch 909, Loss: 0.7410868230793212\n",
      "Test Loss after Epoch 909: 0.8359445385932922\n",
      "5400\n",
      "Epoch 910, Loss: 0.7394388550961459\n",
      "Test Loss after Epoch 910: 0.8355822041034698\n",
      "5400\n",
      "Epoch 911, Loss: 0.741251379692996\n",
      "Test Loss after Epoch 911: 0.8382176146507263\n",
      "5400\n",
      "Epoch 912, Loss: 0.7405500083720242\n",
      "Test Loss after Epoch 912: 0.8416596703529358\n",
      "5400\n",
      "Epoch 913, Loss: 0.7402998161978191\n",
      "Test Loss after Epoch 913: 0.8388747353553772\n",
      "5400\n",
      "Epoch 914, Loss: 0.7406386982953107\n",
      "Test Loss after Epoch 914: 0.8332708880901337\n",
      "5400\n",
      "Epoch 915, Loss: 0.7415096604824066\n",
      "Test Loss after Epoch 915: 0.8304918129444122\n",
      "5400\n",
      "Epoch 916, Loss: 0.739992992259838\n",
      "Test Loss after Epoch 916: 0.831299650669098\n",
      "5400\n",
      "Epoch 917, Loss: 0.7392289461471416\n",
      "Test Loss after Epoch 917: 0.8301586673259735\n",
      "5400\n",
      "Epoch 918, Loss: 0.7389324870595225\n",
      "Test Loss after Epoch 918: 0.8288922796249389\n",
      "5400\n",
      "Epoch 919, Loss: 0.7384441898928749\n",
      "Test Loss after Epoch 919: 0.8323038878440857\n",
      "5400\n",
      "Epoch 920, Loss: 0.7388762537417588\n",
      "Test Loss after Epoch 920: 0.834314288854599\n",
      "5400\n",
      "Epoch 921, Loss: 0.7400139582157135\n",
      "Test Loss after Epoch 921: 0.8330078768730164\n",
      "5400\n",
      "Epoch 922, Loss: 0.7411765087754638\n",
      "Test Loss after Epoch 922: 0.830482272863388\n",
      "5400\n",
      "Epoch 923, Loss: 0.7396198746230868\n",
      "Test Loss after Epoch 923: 0.8254876465797424\n",
      "5400\n",
      "Epoch 924, Loss: 0.7394217019390177\n",
      "Test Loss after Epoch 924: 0.8234123873710633\n",
      "5400\n",
      "Epoch 925, Loss: 0.7380318355339545\n",
      "Test Loss after Epoch 925: 0.824834822177887\n",
      "5400\n",
      "Epoch 926, Loss: 0.7402508097445524\n",
      "Test Loss after Epoch 926: 0.8230678491592407\n",
      "5400\n",
      "Epoch 927, Loss: 0.7421824671162499\n",
      "Test Loss after Epoch 927: 0.8216495175361633\n",
      "5400\n",
      "Epoch 928, Loss: 0.7394633176591662\n",
      "Test Loss after Epoch 928: 0.8249318971633911\n",
      "5400\n",
      "Epoch 929, Loss: 0.7390113967657089\n",
      "Test Loss after Epoch 929: 0.8266715185642243\n",
      "5400\n",
      "Epoch 930, Loss: 0.7392258056004842\n",
      "Test Loss after Epoch 930: 0.8259434134960174\n",
      "5400\n",
      "Epoch 931, Loss: 0.7413702105813557\n",
      "Test Loss after Epoch 931: 0.8287725086212159\n",
      "5400\n",
      "Epoch 932, Loss: 0.7401804098155763\n",
      "Test Loss after Epoch 932: 0.8288271541595459\n",
      "5400\n",
      "Epoch 933, Loss: 0.7394772722544494\n",
      "Test Loss after Epoch 933: 0.8287142028808594\n",
      "5400\n",
      "Epoch 934, Loss: 0.7395949079151507\n",
      "Test Loss after Epoch 934: 0.8295002343654633\n",
      "5400\n",
      "Epoch 935, Loss: 0.7401322488873093\n",
      "Test Loss after Epoch 935: 0.8300122618675232\n",
      "5400\n",
      "Epoch 936, Loss: 0.7398309542514659\n",
      "Test Loss after Epoch 936: 0.8311488938331604\n",
      "5400\n",
      "Epoch 937, Loss: 0.7396953341254482\n",
      "Test Loss after Epoch 937: 0.8305540368556976\n",
      "5400\n",
      "Epoch 938, Loss: 0.7384014111095004\n",
      "Test Loss after Epoch 938: 0.8327908217906952\n",
      "5400\n",
      "Epoch 939, Loss: 0.7398214755234895\n",
      "Test Loss after Epoch 939: 0.8322064442634582\n",
      "5400\n",
      "Epoch 940, Loss: 0.7376269165895604\n",
      "Test Loss after Epoch 940: 0.8309610214233398\n",
      "5400\n",
      "Epoch 941, Loss: 0.7406165349262732\n",
      "Test Loss after Epoch 941: 0.8294162538051605\n",
      "5400\n",
      "Epoch 942, Loss: 0.739045566143813\n",
      "Test Loss after Epoch 942: 0.8281808714866639\n",
      "5400\n",
      "Epoch 943, Loss: 0.7393780552678638\n",
      "Test Loss after Epoch 943: 0.8290972483158111\n",
      "5400\n",
      "Epoch 944, Loss: 0.7384620374220389\n",
      "Test Loss after Epoch 944: 0.8282524349689484\n",
      "5400\n",
      "Epoch 945, Loss: 0.7389756897423002\n",
      "Test Loss after Epoch 945: 0.8272708814144134\n",
      "5400\n",
      "Epoch 946, Loss: 0.7391629314422608\n",
      "Test Loss after Epoch 946: 0.8273534893989563\n",
      "5400\n",
      "Epoch 947, Loss: 0.7383011253895583\n",
      "Test Loss after Epoch 947: 0.8274853551387786\n",
      "5400\n",
      "Epoch 948, Loss: 0.7393898306511066\n",
      "Test Loss after Epoch 948: 0.8286121754646302\n",
      "5400\n",
      "Epoch 949, Loss: 0.7392667437262005\n",
      "Test Loss after Epoch 949: 0.8246729800701141\n",
      "5400\n",
      "Epoch 950, Loss: 0.7381653539118943\n",
      "Test Loss after Epoch 950: 0.8244015414714814\n",
      "5400\n",
      "Epoch 951, Loss: 0.7402286657801381\n",
      "Test Loss after Epoch 951: 0.8262314758300782\n",
      "5400\n",
      "Epoch 952, Loss: 0.7393058147695329\n",
      "Test Loss after Epoch 952: 0.8305120992660523\n",
      "5400\n",
      "Epoch 953, Loss: 0.7383159995299798\n",
      "Test Loss after Epoch 953: 0.8355854580402374\n",
      "5400\n",
      "Epoch 954, Loss: 0.7366928656675198\n",
      "Test Loss after Epoch 954: 0.8371599931716919\n",
      "5400\n",
      "Epoch 955, Loss: 0.7402266711438144\n",
      "Test Loss after Epoch 955: 0.8340380601882934\n",
      "5400\n",
      "Epoch 956, Loss: 0.73874058842659\n",
      "Test Loss after Epoch 956: 0.8305411746501923\n",
      "5400\n",
      "Epoch 957, Loss: 0.7378563282445625\n",
      "Test Loss after Epoch 957: 0.8296521301269532\n",
      "5400\n",
      "Epoch 958, Loss: 0.7382021897810477\n",
      "Test Loss after Epoch 958: 0.8312231891155243\n",
      "5400\n",
      "Epoch 959, Loss: 0.7371210777097278\n",
      "Test Loss after Epoch 959: 0.8337198441028595\n",
      "5400\n",
      "Epoch 960, Loss: 0.7373278760027002\n",
      "Test Loss after Epoch 960: 0.8367308204174042\n",
      "5400\n",
      "Epoch 961, Loss: 0.7396661257081562\n",
      "Test Loss after Epoch 961: 0.8314270007610322\n",
      "5400\n",
      "Epoch 962, Loss: 0.7376707001968666\n",
      "Test Loss after Epoch 962: 0.8266674880981445\n",
      "5400\n",
      "Epoch 963, Loss: 0.7384635672745882\n",
      "Test Loss after Epoch 963: 0.8270447824001312\n",
      "5400\n",
      "Epoch 964, Loss: 0.7363392396547176\n",
      "Test Loss after Epoch 964: 0.8269794661998748\n",
      "5400\n",
      "Epoch 965, Loss: 0.7369282690463242\n",
      "Test Loss after Epoch 965: 0.8280801267623902\n",
      "5400\n",
      "Epoch 966, Loss: 0.738403389012372\n",
      "Test Loss after Epoch 966: 0.830310955286026\n",
      "5400\n",
      "Epoch 967, Loss: 0.7374841453631719\n",
      "Test Loss after Epoch 967: 0.8281925139427185\n",
      "5400\n",
      "Epoch 968, Loss: 0.7387065428053892\n",
      "Test Loss after Epoch 968: 0.8271722233295441\n",
      "5400\n",
      "Epoch 969, Loss: 0.7369240605389631\n",
      "Test Loss after Epoch 969: 0.8287084763050079\n",
      "5400\n",
      "Epoch 970, Loss: 0.7369416864492275\n",
      "Test Loss after Epoch 970: 0.8330837202072143\n",
      "5400\n",
      "Epoch 971, Loss: 0.737999143158948\n",
      "Test Loss after Epoch 971: 0.8343073344230652\n",
      "5400\n",
      "Epoch 972, Loss: 0.7371835676166746\n",
      "Test Loss after Epoch 972: 0.8330688219070435\n",
      "5400\n",
      "Epoch 973, Loss: 0.7366970321646443\n",
      "Test Loss after Epoch 973: 0.8336501564979554\n",
      "5400\n",
      "Epoch 974, Loss: 0.7374310023917092\n",
      "Test Loss after Epoch 974: 0.8368799381256103\n",
      "5400\n",
      "Epoch 975, Loss: 0.736779746722292\n",
      "Test Loss after Epoch 975: 0.8381331651210785\n",
      "5400\n",
      "Epoch 976, Loss: 0.737759474515915\n",
      "Test Loss after Epoch 976: 0.8320966980457306\n",
      "5400\n",
      "Epoch 977, Loss: 0.7380600619095343\n",
      "Test Loss after Epoch 977: 0.8311169986724853\n",
      "5400\n",
      "Epoch 978, Loss: 0.7381348621403729\n",
      "Test Loss after Epoch 978: 0.8303039681911468\n",
      "5400\n",
      "Epoch 979, Loss: 0.736090004157137\n",
      "Test Loss after Epoch 979: 0.8285827150344849\n",
      "5400\n",
      "Epoch 980, Loss: 0.7355255971793775\n",
      "Test Loss after Epoch 980: 0.8299189846515655\n",
      "5400\n",
      "Epoch 981, Loss: 0.7357076862564793\n",
      "Test Loss after Epoch 981: 0.8312874572277069\n",
      "5400\n",
      "Epoch 982, Loss: 0.7371179187960095\n",
      "Test Loss after Epoch 982: 0.831567177772522\n",
      "5400\n",
      "Epoch 983, Loss: 0.7351387344687074\n",
      "Test Loss after Epoch 983: 0.831804676771164\n",
      "5400\n",
      "Epoch 984, Loss: 0.736417335514669\n",
      "Test Loss after Epoch 984: 0.8341467599868775\n",
      "5400\n",
      "Epoch 985, Loss: 0.7357748981096126\n",
      "Test Loss after Epoch 985: 0.8333915972709656\n",
      "5400\n",
      "Epoch 986, Loss: 0.7377551486315551\n",
      "Test Loss after Epoch 986: 0.8268653583526612\n",
      "5400\n",
      "Epoch 987, Loss: 0.7368785369175451\n",
      "Test Loss after Epoch 987: 0.826577523469925\n",
      "5400\n",
      "Epoch 988, Loss: 0.7364436160193549\n",
      "Test Loss after Epoch 988: 0.8319038708209991\n",
      "5400\n",
      "Epoch 989, Loss: 0.7356441033548778\n",
      "Test Loss after Epoch 989: 0.8320940763950347\n",
      "5400\n",
      "Epoch 990, Loss: 0.7341928738576394\n",
      "Test Loss after Epoch 990: 0.8273467681407929\n",
      "5400\n",
      "Epoch 991, Loss: 0.7370908444236826\n",
      "Test Loss after Epoch 991: 0.8268346309661865\n",
      "5400\n",
      "Epoch 992, Loss: 0.7381589098771413\n",
      "Test Loss after Epoch 992: 0.8291724507808685\n",
      "5400\n",
      "Epoch 993, Loss: 0.737731556671637\n",
      "Test Loss after Epoch 993: 0.8330323538780212\n",
      "5400\n",
      "Epoch 994, Loss: 0.7367090841796663\n",
      "Test Loss after Epoch 994: 0.8369912927150727\n",
      "5400\n",
      "Epoch 995, Loss: 0.7372210717421991\n",
      "Test Loss after Epoch 995: 0.8373366029262542\n",
      "5400\n",
      "Epoch 996, Loss: 0.7366046698446627\n",
      "Test Loss after Epoch 996: 0.8340407428741455\n",
      "5400\n",
      "Epoch 997, Loss: 0.7373970203487962\n",
      "Test Loss after Epoch 997: 0.83272905087471\n",
      "5400\n",
      "Epoch 998, Loss: 0.7363211774605292\n",
      "Test Loss after Epoch 998: 0.8344637656211853\n",
      "5400\n",
      "Epoch 999, Loss: 0.7355576152271694\n",
      "Test Loss after Epoch 999: 0.8351771590709687\n",
      "5400\n",
      "Epoch 1000, Loss: 0.7361681025116532\n",
      "Test Loss after Epoch 1000: 0.8369106125831604\n",
      "5400\n",
      "Epoch 1001, Loss: 0.7373746747220004\n",
      "Test Loss after Epoch 1001: 0.8369353427886963\n",
      "5400\n",
      "Epoch 1002, Loss: 0.7365180761284298\n",
      "Test Loss after Epoch 1002: 0.8339490070343017\n",
      "5400\n",
      "Epoch 1003, Loss: 0.7369249926673042\n",
      "Test Loss after Epoch 1003: 0.8339093565940857\n",
      "5400\n",
      "Epoch 1004, Loss: 0.737257547488919\n",
      "Test Loss after Epoch 1004: 0.8329506800174713\n",
      "5400\n",
      "Epoch 1005, Loss: 0.7358631267150243\n",
      "Test Loss after Epoch 1005: 0.8342467317581177\n",
      "5400\n",
      "Epoch 1006, Loss: 0.7355050837552106\n",
      "Test Loss after Epoch 1006: 0.8335743234157562\n",
      "5400\n",
      "Epoch 1007, Loss: 0.7341320361472943\n",
      "Test Loss after Epoch 1007: 0.8276564502716064\n",
      "5400\n",
      "Epoch 1008, Loss: 0.7351054805296439\n",
      "Test Loss after Epoch 1008: 0.8242898840904236\n",
      "5400\n",
      "Epoch 1009, Loss: 0.734200970309752\n",
      "Test Loss after Epoch 1009: 0.8261634051799774\n",
      "5400\n",
      "Epoch 1010, Loss: 0.7346057486975635\n",
      "Test Loss after Epoch 1010: 0.8287648997306823\n",
      "5400\n",
      "Epoch 1011, Loss: 0.7365040300510548\n",
      "Test Loss after Epoch 1011: 0.8301347336769104\n",
      "5400\n",
      "Epoch 1012, Loss: 0.7361743518820515\n",
      "Test Loss after Epoch 1012: 0.8321822159290314\n",
      "5400\n",
      "Epoch 1013, Loss: 0.7351788187689251\n",
      "Test Loss after Epoch 1013: 0.830861001253128\n",
      "5400\n",
      "Epoch 1014, Loss: 0.7338925694094763\n",
      "Test Loss after Epoch 1014: 0.828613401889801\n",
      "5400\n",
      "Epoch 1015, Loss: 0.7352594788648464\n",
      "Test Loss after Epoch 1015: 0.8263443019390107\n",
      "5400\n",
      "Epoch 1016, Loss: 0.7362504497722343\n",
      "Test Loss after Epoch 1016: 0.825661602973938\n",
      "5400\n",
      "Epoch 1017, Loss: 0.7377775093140425\n",
      "Test Loss after Epoch 1017: 0.8258645515441895\n",
      "5400\n",
      "Epoch 1018, Loss: 0.7362191766721231\n",
      "Test Loss after Epoch 1018: 0.8322542049884796\n",
      "5400\n",
      "Epoch 1019, Loss: 0.736096959842576\n",
      "Test Loss after Epoch 1019: 0.8300143122673035\n",
      "5400\n",
      "Epoch 1020, Loss: 0.7362936831624419\n",
      "Test Loss after Epoch 1020: 0.8252223615646362\n",
      "5400\n",
      "Epoch 1021, Loss: 0.7368651718121988\n",
      "Test Loss after Epoch 1021: 0.8291223340034485\n",
      "5400\n",
      "Epoch 1022, Loss: 0.7357563259424986\n",
      "Test Loss after Epoch 1022: 0.8366764280796051\n",
      "5400\n",
      "Epoch 1023, Loss: 0.7352772527050089\n",
      "Test Loss after Epoch 1023: 0.8368885300159454\n",
      "5400\n",
      "Epoch 1024, Loss: 0.7355206197720987\n",
      "Test Loss after Epoch 1024: 0.8364465548992157\n",
      "5400\n",
      "Epoch 1025, Loss: 0.7348542244787569\n",
      "Test Loss after Epoch 1025: 0.8352211880683899\n",
      "5400\n",
      "Epoch 1026, Loss: 0.7365999863986615\n",
      "Test Loss after Epoch 1026: 0.8347595665454864\n",
      "5400\n",
      "Epoch 1027, Loss: 0.7355580827041909\n",
      "Test Loss after Epoch 1027: 0.8340693233013153\n",
      "5400\n",
      "Epoch 1028, Loss: 0.7364451219638188\n",
      "Test Loss after Epoch 1028: 0.8348151772022248\n",
      "5400\n",
      "Epoch 1029, Loss: 0.736827505186752\n",
      "Test Loss after Epoch 1029: 0.8346798582077026\n",
      "5400\n",
      "Epoch 1030, Loss: 0.7340764072206285\n",
      "Test Loss after Epoch 1030: 0.8343703143596649\n",
      "5400\n",
      "Epoch 1031, Loss: 0.7335104268127017\n",
      "Test Loss after Epoch 1031: 0.8335462522506714\n",
      "5400\n",
      "Epoch 1032, Loss: 0.7341878617251361\n",
      "Test Loss after Epoch 1032: 0.8356673221588135\n",
      "5400\n",
      "Epoch 1033, Loss: 0.7349106948022489\n",
      "Test Loss after Epoch 1033: 0.8367658147811889\n",
      "5400\n",
      "Epoch 1034, Loss: 0.7350397514193147\n",
      "Test Loss after Epoch 1034: 0.8346216359138489\n",
      "5400\n",
      "Epoch 1035, Loss: 0.7342169022118604\n",
      "Test Loss after Epoch 1035: 0.8345526735782623\n",
      "5400\n",
      "Epoch 1036, Loss: 0.7354460111812309\n",
      "Test Loss after Epoch 1036: 0.8339976074695588\n",
      "5400\n",
      "Epoch 1037, Loss: 0.7346979586724882\n",
      "Test Loss after Epoch 1037: 0.8333527629375458\n",
      "5400\n",
      "Epoch 1038, Loss: 0.7349047058820725\n",
      "Test Loss after Epoch 1038: 0.8308609735965728\n",
      "5400\n",
      "Epoch 1039, Loss: 0.7343965064817005\n",
      "Test Loss after Epoch 1039: 0.830976747751236\n",
      "5400\n",
      "Epoch 1040, Loss: 0.734251873449043\n",
      "Test Loss after Epoch 1040: 0.8324340305328369\n",
      "5400\n",
      "Epoch 1041, Loss: 0.7342450420061747\n",
      "Test Loss after Epoch 1041: 0.8332712099552154\n",
      "5400\n",
      "Epoch 1042, Loss: 0.7357847062084409\n",
      "Test Loss after Epoch 1042: 0.8329591484069824\n",
      "5400\n",
      "Epoch 1043, Loss: 0.7351912468892556\n",
      "Test Loss after Epoch 1043: 0.8316250803470612\n",
      "5400\n",
      "Epoch 1044, Loss: 0.7345867897625322\n",
      "Test Loss after Epoch 1044: 0.8318307347297669\n",
      "5400\n",
      "Epoch 1045, Loss: 0.7340229259596931\n",
      "Test Loss after Epoch 1045: 0.8309437165260315\n",
      "5400\n",
      "Epoch 1046, Loss: 0.7345191563058783\n",
      "Test Loss after Epoch 1046: 0.83220667552948\n",
      "5400\n",
      "Epoch 1047, Loss: 0.7332428952941188\n",
      "Test Loss after Epoch 1047: 0.8353178436756135\n",
      "5400\n",
      "Epoch 1048, Loss: 0.7343092200491164\n",
      "Test Loss after Epoch 1048: 0.8345902881622315\n",
      "5400\n",
      "Epoch 1049, Loss: 0.7331663918495178\n",
      "Test Loss after Epoch 1049: 0.8332798080444336\n",
      "5400\n",
      "Epoch 1050, Loss: 0.7343420281675127\n",
      "Test Loss after Epoch 1050: 0.8322995307445527\n",
      "5400\n",
      "Epoch 1051, Loss: 0.7358539086801035\n",
      "Test Loss after Epoch 1051: 0.8340953552722931\n",
      "5400\n",
      "Epoch 1052, Loss: 0.7358798024389479\n",
      "Test Loss after Epoch 1052: 0.836576679468155\n",
      "5400\n",
      "Epoch 1053, Loss: 0.7342647881861086\n",
      "Test Loss after Epoch 1053: 0.8355891282558441\n",
      "5400\n",
      "Epoch 1054, Loss: 0.7339933817033415\n",
      "Test Loss after Epoch 1054: 0.8324658226966858\n",
      "5400\n",
      "Epoch 1055, Loss: 0.734462222456932\n",
      "Test Loss after Epoch 1055: 0.8301062624454498\n",
      "5400\n",
      "Epoch 1056, Loss: 0.7347629772071486\n",
      "Test Loss after Epoch 1056: 0.8291668105125427\n",
      "5400\n",
      "Epoch 1057, Loss: 0.7336842892125801\n",
      "Test Loss after Epoch 1057: 0.8276527953147889\n",
      "5400\n",
      "Epoch 1058, Loss: 0.7352900935985424\n",
      "Test Loss after Epoch 1058: 0.8261200609207153\n",
      "5400\n",
      "Epoch 1059, Loss: 0.7340626069572237\n",
      "Test Loss after Epoch 1059: 0.8285027675628662\n",
      "5400\n",
      "Epoch 1060, Loss: 0.7341785878384555\n",
      "Test Loss after Epoch 1060: 0.8307931206226349\n",
      "5400\n",
      "Epoch 1061, Loss: 0.7341368759119952\n",
      "Test Loss after Epoch 1061: 0.831663675069809\n",
      "5400\n",
      "Epoch 1062, Loss: 0.7347164299973735\n",
      "Test Loss after Epoch 1062: 0.8356705348491669\n",
      "5400\n",
      "Epoch 1063, Loss: 0.7350355841053857\n",
      "Test Loss after Epoch 1063: 0.8372993266582489\n",
      "5400\n",
      "Epoch 1064, Loss: 0.7335376492032298\n",
      "Test Loss after Epoch 1064: 0.8361717419624328\n",
      "5400\n",
      "Epoch 1065, Loss: 0.7350096217570481\n",
      "Test Loss after Epoch 1065: 0.8346012637615204\n",
      "5400\n",
      "Epoch 1066, Loss: 0.7335565185988391\n",
      "Test Loss after Epoch 1066: 0.8347708628177642\n",
      "5400\n",
      "Epoch 1067, Loss: 0.7348114459161406\n",
      "Test Loss after Epoch 1067: 0.8324740114212036\n",
      "5400\n",
      "Epoch 1068, Loss: 0.7330524667987117\n",
      "Test Loss after Epoch 1068: 0.8305671999454498\n",
      "5400\n",
      "Epoch 1069, Loss: 0.7332738400830163\n",
      "Test Loss after Epoch 1069: 0.8316001377105713\n",
      "5400\n",
      "Epoch 1070, Loss: 0.7334300903700016\n",
      "Test Loss after Epoch 1070: 0.8319323673248291\n",
      "5400\n",
      "Epoch 1071, Loss: 0.7346376792369066\n",
      "Test Loss after Epoch 1071: 0.8317383606433868\n",
      "5400\n",
      "Epoch 1072, Loss: 0.7344085355820479\n",
      "Test Loss after Epoch 1072: 0.831880304813385\n",
      "5400\n",
      "Epoch 1073, Loss: 0.7342112113811352\n",
      "Test Loss after Epoch 1073: 0.8326799819469451\n",
      "5400\n",
      "Epoch 1074, Loss: 0.7318445854054557\n",
      "Test Loss after Epoch 1074: 0.8322158815860748\n",
      "5400\n",
      "Epoch 1075, Loss: 0.7326140732456137\n",
      "Test Loss after Epoch 1075: 0.8315403404235839\n",
      "5400\n",
      "Epoch 1076, Loss: 0.7337546077701781\n",
      "Test Loss after Epoch 1076: 0.8297533745765686\n",
      "5400\n",
      "Epoch 1077, Loss: 0.7344294563046209\n",
      "Test Loss after Epoch 1077: 0.8293413822650909\n",
      "5400\n",
      "Epoch 1078, Loss: 0.7342969301232585\n",
      "Test Loss after Epoch 1078: 0.8274940011501313\n",
      "5400\n",
      "Epoch 1079, Loss: 0.7332018026599177\n",
      "Test Loss after Epoch 1079: 0.8271166396141052\n",
      "5400\n",
      "Epoch 1080, Loss: 0.7328103085579696\n",
      "Test Loss after Epoch 1080: 0.8266681551933288\n",
      "5400\n",
      "Epoch 1081, Loss: 0.7344012621817766\n",
      "Test Loss after Epoch 1081: 0.8298051512241363\n",
      "5400\n",
      "Epoch 1082, Loss: 0.733725029097663\n",
      "Test Loss after Epoch 1082: 0.8355813398361206\n",
      "5400\n",
      "Epoch 1083, Loss: 0.7321202918114486\n",
      "Test Loss after Epoch 1083: 0.8416752226352692\n",
      "5400\n",
      "Epoch 1084, Loss: 0.7326281525912108\n",
      "Test Loss after Epoch 1084: 0.842648389339447\n",
      "5400\n",
      "Epoch 1085, Loss: 0.7332074437318025\n",
      "Test Loss after Epoch 1085: 0.8386441583633423\n",
      "5400\n",
      "Epoch 1086, Loss: 0.7347195562830677\n",
      "Test Loss after Epoch 1086: 0.831793863773346\n",
      "5400\n",
      "Epoch 1087, Loss: 0.7347235882944531\n",
      "Test Loss after Epoch 1087: 0.8308526864051818\n",
      "5400\n",
      "Epoch 1088, Loss: 0.7343801517619027\n",
      "Test Loss after Epoch 1088: 0.8330229442119599\n",
      "5400\n",
      "Epoch 1089, Loss: 0.7349685073340381\n",
      "Test Loss after Epoch 1089: 0.8322045788764953\n",
      "5400\n",
      "Epoch 1090, Loss: 0.7341859323669363\n",
      "Test Loss after Epoch 1090: 0.8329038519859314\n",
      "5400\n",
      "Epoch 1091, Loss: 0.7352045669599816\n",
      "Test Loss after Epoch 1091: 0.8314927520751954\n",
      "5400\n",
      "Epoch 1092, Loss: 0.7334129224000153\n",
      "Test Loss after Epoch 1092: 0.8308050043582916\n",
      "5400\n",
      "Epoch 1093, Loss: 0.7334093050603513\n",
      "Test Loss after Epoch 1093: 0.832373208284378\n",
      "5400\n",
      "Epoch 1094, Loss: 0.7332160003759243\n",
      "Test Loss after Epoch 1094: 0.8334155042171478\n",
      "5400\n",
      "Epoch 1095, Loss: 0.7328478356202444\n",
      "Test Loss after Epoch 1095: 0.8326360838413238\n",
      "5400\n",
      "Epoch 1096, Loss: 0.7331229238377677\n",
      "Test Loss after Epoch 1096: 0.8324626924991607\n",
      "5400\n",
      "Epoch 1097, Loss: 0.7325868876112832\n",
      "Test Loss after Epoch 1097: 0.8309260423183441\n",
      "5400\n",
      "Epoch 1098, Loss: 0.7321025462503786\n",
      "Test Loss after Epoch 1098: 0.8302448189258576\n",
      "5400\n",
      "Epoch 1099, Loss: 0.7328708919110122\n",
      "Test Loss after Epoch 1099: 0.8298718984127045\n",
      "5400\n",
      "Epoch 1100, Loss: 0.7333168418760653\n",
      "Test Loss after Epoch 1100: 0.8346122510433197\n",
      "5400\n",
      "Epoch 1101, Loss: 0.7339850308718505\n",
      "Test Loss after Epoch 1101: 0.8370660209655761\n",
      "5400\n",
      "Epoch 1102, Loss: 0.732808182394063\n",
      "Test Loss after Epoch 1102: 0.8374703919887543\n",
      "5400\n",
      "Epoch 1103, Loss: 0.7346486429373423\n",
      "Test Loss after Epoch 1103: 0.8340881249904633\n",
      "5400\n",
      "Epoch 1104, Loss: 0.7328510216871897\n",
      "Test Loss after Epoch 1104: 0.8299001739025116\n",
      "5400\n",
      "Epoch 1105, Loss: 0.7332045369457315\n",
      "Test Loss after Epoch 1105: 0.8304940185546875\n",
      "5400\n",
      "Epoch 1106, Loss: 0.7340303134918212\n",
      "Test Loss after Epoch 1106: 0.8298727397918702\n",
      "5400\n",
      "Epoch 1107, Loss: 0.7344841320426376\n",
      "Test Loss after Epoch 1107: 0.8310327243804931\n",
      "Rolling back to best model from epoch 893\n",
      "Best test loss: 0.8204747023582458\n",
      "5400\n",
      "Epoch 1108, Loss: 0.7410430684575328\n",
      "Test Loss after Epoch 1108: 0.8220043003559112\n",
      "5400\n",
      "Epoch 1109, Loss: 0.7394668438257994\n",
      "Test Loss after Epoch 1109: 0.8235184125900269\n",
      "5400\n",
      "Epoch 1110, Loss: 0.7378598543008169\n",
      "Test Loss after Epoch 1110: 0.8256609177589417\n",
      "5400\n",
      "Epoch 1111, Loss: 0.7397188009156122\n",
      "Test Loss after Epoch 1111: 0.827508083820343\n",
      "5400\n",
      "Epoch 1112, Loss: 0.7394985765881008\n",
      "Test Loss after Epoch 1112: 0.8262035768032074\n",
      "5400\n",
      "Epoch 1113, Loss: 0.7380439415463694\n",
      "Test Loss after Epoch 1113: 0.8267940256595612\n",
      "5400\n",
      "Epoch 1114, Loss: 0.7401158201694489\n",
      "Test Loss after Epoch 1114: 0.8252359759807587\n",
      "5400\n",
      "Epoch 1115, Loss: 0.7399008053541184\n",
      "Test Loss after Epoch 1115: 0.8211601998806\n",
      "5400\n",
      "Epoch 1116, Loss: 0.7404574354268887\n",
      "Test Loss after Epoch 1116: 0.8197857475280762\n",
      "5400\n",
      "Epoch 1117, Loss: 0.73833607589757\n",
      "Test Loss after Epoch 1117: 0.8220360519886016\n",
      "5400\n",
      "Epoch 1118, Loss: 0.7389585813769588\n",
      "Test Loss after Epoch 1118: 0.8216624336242676\n",
      "5400\n",
      "Epoch 1119, Loss: 0.7394346242701566\n",
      "Test Loss after Epoch 1119: 0.8248184914588929\n",
      "5400\n",
      "Epoch 1120, Loss: 0.7392632251757163\n",
      "Test Loss after Epoch 1120: 0.8275144300460815\n",
      "5400\n",
      "Epoch 1121, Loss: 0.7409187952898166\n",
      "Test Loss after Epoch 1121: 0.8238298072814941\n",
      "5400\n",
      "Epoch 1122, Loss: 0.7406590438330615\n",
      "Test Loss after Epoch 1122: 0.8221379957199096\n",
      "5400\n",
      "Epoch 1123, Loss: 0.7400492280721664\n",
      "Test Loss after Epoch 1123: 0.8253338940143585\n",
      "5400\n",
      "Epoch 1124, Loss: 0.7405413822333018\n",
      "Test Loss after Epoch 1124: 0.8267181668281555\n",
      "5400\n",
      "Epoch 1125, Loss: 0.7398408281582373\n",
      "Test Loss after Epoch 1125: 0.8278170840740204\n",
      "5400\n",
      "Epoch 1126, Loss: 0.7395345590070442\n",
      "Test Loss after Epoch 1126: 0.8281985919475555\n",
      "5400\n",
      "Epoch 1127, Loss: 0.7401916075415081\n",
      "Test Loss after Epoch 1127: 0.8275478706359863\n",
      "5400\n",
      "Epoch 1128, Loss: 0.740703610557097\n",
      "Test Loss after Epoch 1128: 0.8299948105812073\n",
      "5400\n",
      "Epoch 1129, Loss: 0.7391576149728563\n",
      "Test Loss after Epoch 1129: 0.8293253240585328\n",
      "5400\n",
      "Epoch 1130, Loss: 0.736709027422799\n",
      "Test Loss after Epoch 1130: 0.828991278886795\n",
      "5400\n",
      "Epoch 1131, Loss: 0.7391893314432215\n",
      "Test Loss after Epoch 1131: 0.8298944594860077\n",
      "5400\n",
      "Epoch 1132, Loss: 0.7398205586053707\n",
      "Test Loss after Epoch 1132: 0.8285908145904541\n",
      "5400\n",
      "Epoch 1133, Loss: 0.7401128079273083\n",
      "Test Loss after Epoch 1133: 0.8276829566955567\n",
      "5400\n",
      "Epoch 1134, Loss: 0.7400327386900231\n",
      "Test Loss after Epoch 1134: 0.831016836643219\n",
      "5400\n",
      "Epoch 1135, Loss: 0.7384896032677757\n",
      "Test Loss after Epoch 1135: 0.8366343638896943\n",
      "5400\n",
      "Epoch 1136, Loss: 0.7381041992593694\n",
      "Test Loss after Epoch 1136: 0.8326465146541595\n",
      "5400\n",
      "Epoch 1137, Loss: 0.7394393081797493\n",
      "Test Loss after Epoch 1137: 0.8268537976741791\n",
      "5400\n",
      "Epoch 1138, Loss: 0.7391128130312319\n",
      "Test Loss after Epoch 1138: 0.8257379012107849\n",
      "5400\n",
      "Epoch 1139, Loss: 0.7387941332216617\n",
      "Test Loss after Epoch 1139: 0.829515293598175\n",
      "5400\n",
      "Epoch 1140, Loss: 0.739050480613002\n",
      "Test Loss after Epoch 1140: 0.8297448799610138\n",
      "5400\n",
      "Epoch 1141, Loss: 0.7388841434540572\n",
      "Test Loss after Epoch 1141: 0.826856036901474\n",
      "5400\n",
      "Epoch 1142, Loss: 0.7389749462736978\n",
      "Test Loss after Epoch 1142: 0.8270711238384247\n",
      "5400\n",
      "Epoch 1143, Loss: 0.7390207822676058\n",
      "Test Loss after Epoch 1143: 0.8282237441539765\n",
      "5400\n",
      "Epoch 1144, Loss: 0.7395949786018442\n",
      "Test Loss after Epoch 1144: 0.8284877135753631\n",
      "5400\n",
      "Epoch 1145, Loss: 0.7379275677601497\n",
      "Test Loss after Epoch 1145: 0.8310654363632202\n",
      "5400\n",
      "Epoch 1146, Loss: 0.7412405514717102\n",
      "Test Loss after Epoch 1146: 0.8348276996612549\n",
      "5400\n",
      "Epoch 1147, Loss: 0.7379050346215567\n",
      "Test Loss after Epoch 1147: 0.8303287978172302\n",
      "5400\n",
      "Epoch 1148, Loss: 0.7388707280158997\n",
      "Test Loss after Epoch 1148: 0.8259095733165741\n",
      "5400\n",
      "Epoch 1149, Loss: 0.7365262075706764\n",
      "Test Loss after Epoch 1149: 0.8283336403369903\n",
      "5400\n",
      "Epoch 1150, Loss: 0.7384204318126043\n",
      "Test Loss after Epoch 1150: 0.8336266641616821\n",
      "5400\n",
      "Epoch 1151, Loss: 0.7406902998244321\n",
      "Test Loss after Epoch 1151: 0.8313122589588166\n",
      "5400\n",
      "Epoch 1152, Loss: 0.7412844436256973\n",
      "Test Loss after Epoch 1152: 0.8312387948036194\n",
      "5400\n",
      "Epoch 1153, Loss: 0.7385050111567533\n",
      "Test Loss after Epoch 1153: 0.8285891931056977\n",
      "5400\n",
      "Epoch 1154, Loss: 0.7371691845081471\n",
      "Test Loss after Epoch 1154: 0.8289609372615814\n",
      "5400\n",
      "Epoch 1155, Loss: 0.7384432521131303\n",
      "Test Loss after Epoch 1155: 0.8270062065124512\n",
      "5400\n",
      "Epoch 1156, Loss: 0.7379143262792517\n",
      "Test Loss after Epoch 1156: 0.8256515800952912\n",
      "5400\n",
      "Epoch 1157, Loss: 0.7390892159938812\n",
      "Test Loss after Epoch 1157: 0.827116902589798\n",
      "5400\n",
      "Epoch 1158, Loss: 0.7371770783927706\n",
      "Test Loss after Epoch 1158: 0.8257559225559234\n",
      "5400\n",
      "Epoch 1159, Loss: 0.7368549868354091\n",
      "Test Loss after Epoch 1159: 0.8278987174034118\n",
      "5400\n",
      "Epoch 1160, Loss: 0.7360745246542825\n",
      "Test Loss after Epoch 1160: 0.8290705904960632\n",
      "5400\n",
      "Epoch 1161, Loss: 0.7369469302671927\n",
      "Test Loss after Epoch 1161: 0.8288708128929139\n",
      "5400\n",
      "Epoch 1162, Loss: 0.7368458632848881\n",
      "Test Loss after Epoch 1162: 0.8251980361938477\n",
      "5400\n",
      "Epoch 1163, Loss: 0.7367769069804085\n",
      "Test Loss after Epoch 1163: 0.8234571435451508\n",
      "5400\n",
      "Epoch 1164, Loss: 0.7365016127294964\n",
      "Test Loss after Epoch 1164: 0.8262039697170258\n",
      "5400\n",
      "Epoch 1165, Loss: 0.7370305280332212\n",
      "Test Loss after Epoch 1165: 0.8285813884735107\n",
      "5400\n",
      "Epoch 1166, Loss: 0.7380324842973992\n",
      "Test Loss after Epoch 1166: 0.8276570415496827\n",
      "5400\n",
      "Epoch 1167, Loss: 0.7367599848464683\n",
      "Test Loss after Epoch 1167: 0.8257804498672485\n",
      "5400\n",
      "Epoch 1168, Loss: 0.7375631555804499\n",
      "Test Loss after Epoch 1168: 0.823883470773697\n",
      "5400\n",
      "Epoch 1169, Loss: 0.7350636818894634\n",
      "Test Loss after Epoch 1169: 0.8232099649906158\n",
      "5400\n",
      "Epoch 1170, Loss: 0.7364400253693263\n",
      "Test Loss after Epoch 1170: 0.82486141872406\n",
      "5400\n",
      "Epoch 1171, Loss: 0.7372552926452072\n",
      "Test Loss after Epoch 1171: 0.8262550625801086\n",
      "5400\n",
      "Epoch 1172, Loss: 0.7373960714870029\n",
      "Test Loss after Epoch 1172: 0.8275275702476501\n",
      "5400\n",
      "Epoch 1173, Loss: 0.7372285446855757\n",
      "Test Loss after Epoch 1173: 0.8295731370449066\n",
      "5400\n",
      "Epoch 1174, Loss: 0.7371966911466034\n",
      "Test Loss after Epoch 1174: 0.8285437016487122\n",
      "5400\n",
      "Epoch 1175, Loss: 0.7365668311383989\n",
      "Test Loss after Epoch 1175: 0.8309297723770142\n",
      "5400\n",
      "Epoch 1176, Loss: 0.7377352998654048\n",
      "Test Loss after Epoch 1176: 0.8280859804153442\n",
      "5400\n",
      "Epoch 1177, Loss: 0.7381654074898473\n",
      "Test Loss after Epoch 1177: 0.8288487529754639\n",
      "5400\n",
      "Epoch 1178, Loss: 0.7367580219992885\n",
      "Test Loss after Epoch 1178: 0.8331479759216308\n",
      "5400\n",
      "Epoch 1179, Loss: 0.7360891276377218\n",
      "Test Loss after Epoch 1179: 0.8365773816108704\n",
      "5400\n",
      "Epoch 1180, Loss: 0.7371434348159366\n",
      "Test Loss after Epoch 1180: 0.8338088045120239\n",
      "5400\n",
      "Epoch 1181, Loss: 0.7356847203219379\n",
      "Test Loss after Epoch 1181: 0.8294347417354584\n",
      "5400\n",
      "Epoch 1182, Loss: 0.7359316457421692\n",
      "Test Loss after Epoch 1182: 0.8278189120292664\n",
      "5400\n",
      "Epoch 1183, Loss: 0.7354155326772619\n",
      "Test Loss after Epoch 1183: 0.82917573595047\n",
      "5400\n",
      "Epoch 1184, Loss: 0.7348963297737969\n",
      "Test Loss after Epoch 1184: 0.835604633808136\n",
      "5400\n",
      "Epoch 1185, Loss: 0.7348463029331631\n",
      "Test Loss after Epoch 1185: 0.8367011823654175\n",
      "5400\n",
      "Epoch 1186, Loss: 0.7380964669254091\n",
      "Test Loss after Epoch 1186: 0.8354225885868073\n",
      "5400\n",
      "Epoch 1187, Loss: 0.7370567563948808\n",
      "Test Loss after Epoch 1187: 0.8325097572803497\n",
      "5400\n",
      "Epoch 1188, Loss: 0.7357041468002178\n",
      "Test Loss after Epoch 1188: 0.8299638397693634\n",
      "5400\n",
      "Epoch 1189, Loss: 0.7346580776461848\n",
      "Test Loss after Epoch 1189: 0.8305487229824066\n",
      "5400\n",
      "Epoch 1190, Loss: 0.7356660024987327\n",
      "Test Loss after Epoch 1190: 0.832166564464569\n",
      "5400\n",
      "Epoch 1191, Loss: 0.735737541715304\n",
      "Test Loss after Epoch 1191: 0.8320410380363464\n",
      "5400\n",
      "Epoch 1192, Loss: 0.7361755889212643\n",
      "Test Loss after Epoch 1192: 0.8330708675384522\n",
      "5400\n",
      "Epoch 1193, Loss: 0.7358862355903343\n",
      "Test Loss after Epoch 1193: 0.8353344376087188\n",
      "5400\n",
      "Epoch 1194, Loss: 0.7352493960327572\n",
      "Test Loss after Epoch 1194: 0.8358102252483368\n",
      "5400\n",
      "Epoch 1195, Loss: 0.735713057451778\n",
      "Test Loss after Epoch 1195: 0.8352789618968963\n",
      "5400\n",
      "Epoch 1196, Loss: 0.7378225568268034\n",
      "Test Loss after Epoch 1196: 0.835335221529007\n",
      "5400\n",
      "Epoch 1197, Loss: 0.7379160631144488\n",
      "Test Loss after Epoch 1197: 0.8350999596118927\n",
      "5400\n",
      "Epoch 1198, Loss: 0.7364034960005018\n",
      "Test Loss after Epoch 1198: 0.8348297491073609\n",
      "5400\n",
      "Epoch 1199, Loss: 0.7352333355170709\n",
      "Test Loss after Epoch 1199: 0.8348098418712616\n",
      "5400\n",
      "Epoch 1200, Loss: 0.7346220761096036\n",
      "Test Loss after Epoch 1200: 0.8371007380485534\n",
      "5400\n",
      "Epoch 1201, Loss: 0.7380610535321412\n",
      "Test Loss after Epoch 1201: 0.8395145876407624\n",
      "5400\n",
      "Epoch 1202, Loss: 0.7368861843038488\n",
      "Test Loss after Epoch 1202: 0.84022039437294\n",
      "5400\n",
      "Epoch 1203, Loss: 0.7375514877504773\n",
      "Test Loss after Epoch 1203: 0.8424040400981903\n",
      "5400\n",
      "Epoch 1204, Loss: 0.737056869952767\n",
      "Test Loss after Epoch 1204: 0.841483145236969\n",
      "5400\n",
      "Epoch 1205, Loss: 0.735826526372521\n",
      "Test Loss after Epoch 1205: 0.8368358962535858\n",
      "5400\n",
      "Epoch 1206, Loss: 0.7356619422303305\n",
      "Test Loss after Epoch 1206: 0.8322392573356628\n",
      "5400\n",
      "Epoch 1207, Loss: 0.7357869491753755\n",
      "Test Loss after Epoch 1207: 0.8328439900875092\n",
      "5400\n",
      "Epoch 1208, Loss: 0.7348581160880902\n",
      "Test Loss after Epoch 1208: 0.8348082389831543\n",
      "5400\n",
      "Epoch 1209, Loss: 0.7351008484319405\n",
      "Test Loss after Epoch 1209: 0.83469455575943\n",
      "5400\n",
      "Epoch 1210, Loss: 0.7351398866927182\n",
      "Test Loss after Epoch 1210: 0.8326837031841278\n",
      "5400\n",
      "Epoch 1211, Loss: 0.7378277356977816\n",
      "Test Loss after Epoch 1211: 0.8309072477817535\n",
      "5400\n",
      "Epoch 1212, Loss: 0.7366626624266307\n",
      "Test Loss after Epoch 1212: 0.8289139356613159\n",
      "5400\n",
      "Epoch 1213, Loss: 0.7364314468039407\n",
      "Test Loss after Epoch 1213: 0.8280955057144165\n",
      "5400\n",
      "Epoch 1214, Loss: 0.7346153195919813\n",
      "Test Loss after Epoch 1214: 0.8281375722885131\n",
      "5400\n",
      "Epoch 1215, Loss: 0.7355678707361222\n",
      "Test Loss after Epoch 1215: 0.8296946902275085\n",
      "5400\n",
      "Epoch 1216, Loss: 0.7367552495885779\n",
      "Test Loss after Epoch 1216: 0.830050479888916\n",
      "5400\n",
      "Epoch 1217, Loss: 0.7381630133478729\n",
      "Test Loss after Epoch 1217: 0.8312080044746399\n",
      "5400\n",
      "Epoch 1218, Loss: 0.7347968655603904\n",
      "Test Loss after Epoch 1218: 0.8348383622169495\n",
      "5400\n",
      "Epoch 1219, Loss: 0.7350283492936028\n",
      "Test Loss after Epoch 1219: 0.8376157720088959\n",
      "5400\n",
      "Epoch 1220, Loss: 0.7341419888646514\n",
      "Test Loss after Epoch 1220: 0.8359623031616211\n",
      "5400\n",
      "Epoch 1221, Loss: 0.7371157965615943\n",
      "Test Loss after Epoch 1221: 0.8334740211963654\n",
      "5400\n",
      "Epoch 1222, Loss: 0.7351002413255197\n",
      "Test Loss after Epoch 1222: 0.8338646197319031\n",
      "5400\n",
      "Epoch 1223, Loss: 0.7342161047458649\n",
      "Test Loss after Epoch 1223: 0.8344291861057281\n",
      "5400\n",
      "Epoch 1224, Loss: 0.7340591282093967\n",
      "Test Loss after Epoch 1224: 0.834595180273056\n",
      "5400\n",
      "Epoch 1225, Loss: 0.7329152533522358\n",
      "Test Loss after Epoch 1225: 0.8364662024974823\n",
      "5400\n",
      "Epoch 1226, Loss: 0.7374779819117652\n",
      "Test Loss after Epoch 1226: 0.841695558309555\n",
      "5400\n",
      "Epoch 1227, Loss: 0.7358244794165647\n",
      "Test Loss after Epoch 1227: 0.843561883687973\n",
      "5400\n",
      "Epoch 1228, Loss: 0.7337020462751389\n",
      "Test Loss after Epoch 1228: 0.8447924220561981\n",
      "5400\n",
      "Epoch 1229, Loss: 0.7345856835444768\n",
      "Test Loss after Epoch 1229: 0.8426966733932495\n",
      "5400\n",
      "Epoch 1230, Loss: 0.7347915748092864\n",
      "Test Loss after Epoch 1230: 0.8405031814575196\n",
      "5400\n",
      "Epoch 1231, Loss: 0.7357305828509507\n",
      "Test Loss after Epoch 1231: 0.8413041994571686\n",
      "5400\n",
      "Epoch 1232, Loss: 0.7340960066186057\n",
      "Test Loss after Epoch 1232: 0.8428532433509827\n",
      "5400\n",
      "Epoch 1233, Loss: 0.734037746389707\n",
      "Test Loss after Epoch 1233: 0.8425888621807098\n",
      "5400\n",
      "Epoch 1234, Loss: 0.7335038556655248\n",
      "Test Loss after Epoch 1234: 0.841970627784729\n",
      "5400\n",
      "Epoch 1235, Loss: 0.7354734256532457\n",
      "Test Loss after Epoch 1235: 0.8397441673278808\n",
      "5400\n",
      "Epoch 1236, Loss: 0.7347649544918978\n",
      "Test Loss after Epoch 1236: 0.837776953458786\n",
      "5400\n",
      "Epoch 1237, Loss: 0.7347158029565105\n",
      "Test Loss after Epoch 1237: 0.8377087566852569\n",
      "5400\n",
      "Epoch 1238, Loss: 0.7342774575948715\n",
      "Test Loss after Epoch 1238: 0.8384291918277741\n",
      "5400\n",
      "Epoch 1239, Loss: 0.7337166617976295\n",
      "Test Loss after Epoch 1239: 0.8406182997226715\n",
      "5400\n",
      "Epoch 1240, Loss: 0.7337681699682165\n",
      "Test Loss after Epoch 1240: 0.8413294930458068\n",
      "5400\n",
      "Epoch 1241, Loss: 0.7336189398500654\n",
      "Test Loss after Epoch 1241: 0.8390163674354553\n",
      "5400\n",
      "Epoch 1242, Loss: 0.7338965697862484\n",
      "Test Loss after Epoch 1242: 0.8334983386993409\n",
      "5400\n",
      "Epoch 1243, Loss: 0.7337784685691198\n",
      "Test Loss after Epoch 1243: 0.8336838858127594\n",
      "5400\n",
      "Epoch 1244, Loss: 0.7342074586947759\n",
      "Test Loss after Epoch 1244: 0.8360298132896423\n",
      "5400\n",
      "Epoch 1245, Loss: 0.7339966844629359\n",
      "Test Loss after Epoch 1245: 0.8367466125488281\n",
      "5400\n",
      "Epoch 1246, Loss: 0.7353011263299871\n",
      "Test Loss after Epoch 1246: 0.8365842912197113\n",
      "5400\n",
      "Epoch 1247, Loss: 0.7351257490007965\n",
      "Test Loss after Epoch 1247: 0.8319885692596436\n",
      "5400\n",
      "Epoch 1248, Loss: 0.7356512550512949\n",
      "Test Loss after Epoch 1248: 0.8279199419021607\n",
      "5400\n",
      "Epoch 1249, Loss: 0.7337799835867352\n",
      "Test Loss after Epoch 1249: 0.8290128338336945\n",
      "5400\n",
      "Epoch 1250, Loss: 0.7337146953520951\n",
      "Test Loss after Epoch 1250: 0.8320088474750519\n",
      "5400\n",
      "Epoch 1251, Loss: 0.7362793532565788\n",
      "Test Loss after Epoch 1251: 0.831423593044281\n",
      "5400\n",
      "Epoch 1252, Loss: 0.7346762794476969\n",
      "Test Loss after Epoch 1252: 0.8325331292152405\n",
      "5400\n",
      "Epoch 1253, Loss: 0.7356006237974874\n",
      "Test Loss after Epoch 1253: 0.8325195538997651\n",
      "5400\n",
      "Epoch 1254, Loss: 0.7346416476479283\n",
      "Test Loss after Epoch 1254: 0.8323522577285767\n",
      "5400\n",
      "Epoch 1255, Loss: 0.7329572605424457\n",
      "Test Loss after Epoch 1255: 0.8343841788768769\n",
      "5400\n",
      "Epoch 1256, Loss: 0.7342475891554797\n",
      "Test Loss after Epoch 1256: 0.8337624821662902\n",
      "4500\n",
      "Epoch 1257, Loss: 0.7299801605012681\n",
      "Test Loss after Epoch 1257: 0.8342219278812408\n",
      "4500\n",
      "Epoch 1258, Loss: 0.7301484843360053\n",
      "Test Loss after Epoch 1258: 0.8348916726112365\n",
      "4500\n",
      "Epoch 1259, Loss: 0.7299095879395803\n",
      "Test Loss after Epoch 1259: 0.8378345549106598\n",
      "4500\n",
      "Epoch 1260, Loss: 0.7306463735898335\n",
      "Test Loss after Epoch 1260: 0.8385883121490478\n",
      "4500\n",
      "Epoch 1261, Loss: 0.7291171958711412\n",
      "Test Loss after Epoch 1261: 0.8373087704181671\n",
      "4500\n",
      "Epoch 1262, Loss: 0.7291741977797614\n",
      "Test Loss after Epoch 1262: 0.8352631912231445\n",
      "4500\n",
      "Epoch 1263, Loss: 0.7297187411785125\n",
      "Test Loss after Epoch 1263: 0.8348912749290466\n",
      "4500\n",
      "Epoch 1264, Loss: 0.7295663461685181\n",
      "Test Loss after Epoch 1264: 0.8361006107330322\n",
      "4500\n",
      "Epoch 1265, Loss: 0.7299830703470442\n",
      "Test Loss after Epoch 1265: 0.8388400731086731\n",
      "4500\n",
      "Epoch 1266, Loss: 0.7301197057035235\n",
      "Test Loss after Epoch 1266: 0.8378805685043335\n",
      "4500\n",
      "Epoch 1267, Loss: 0.7299225560824076\n",
      "Test Loss after Epoch 1267: 0.8363184223175049\n",
      "4500\n",
      "Epoch 1268, Loss: 0.7296202882130941\n",
      "Test Loss after Epoch 1268: 0.8391909773349762\n",
      "4500\n",
      "Epoch 1269, Loss: 0.7295748234324985\n",
      "Test Loss after Epoch 1269: 0.8426347696781158\n",
      "4500\n",
      "Epoch 1270, Loss: 0.7301056323051452\n",
      "Test Loss after Epoch 1270: 0.8410436413288116\n",
      "4500\n",
      "Epoch 1271, Loss: 0.7298927323023479\n",
      "Test Loss after Epoch 1271: 0.8396316633224488\n",
      "4500\n",
      "Epoch 1272, Loss: 0.7292300505373213\n",
      "Test Loss after Epoch 1272: 0.8404935517311096\n",
      "4500\n",
      "Epoch 1273, Loss: 0.7305503473546769\n",
      "Test Loss after Epoch 1273: 0.8413895502090454\n",
      "4500\n",
      "Epoch 1274, Loss: 0.7302576677799225\n",
      "Test Loss after Epoch 1274: 0.8422434904575348\n",
      "4500\n",
      "Epoch 1275, Loss: 0.7304077791108026\n",
      "Test Loss after Epoch 1275: 0.8437203242778778\n",
      "4500\n",
      "Epoch 1276, Loss: 0.7295885691112942\n",
      "Test Loss after Epoch 1276: 0.8472972078323364\n",
      "4500\n",
      "Epoch 1277, Loss: 0.7294359325302971\n",
      "Test Loss after Epoch 1277: 0.848064361333847\n",
      "4500\n",
      "Epoch 1278, Loss: 0.7292616386148665\n",
      "Test Loss after Epoch 1278: 0.8456647453308106\n",
      "4500\n",
      "Epoch 1279, Loss: 0.7300265507698059\n",
      "Test Loss after Epoch 1279: 0.8441938407421112\n",
      "4500\n",
      "Epoch 1280, Loss: 0.7300320045948029\n",
      "Test Loss after Epoch 1280: 0.8408567004203796\n",
      "4500\n",
      "Epoch 1281, Loss: 0.7307141942183176\n",
      "Test Loss after Epoch 1281: 0.8390617454051972\n",
      "4500\n",
      "Epoch 1282, Loss: 0.7302101883358425\n",
      "Test Loss after Epoch 1282: 0.8388112025260925\n",
      "4500\n",
      "Epoch 1283, Loss: 0.7296724697483911\n",
      "Test Loss after Epoch 1283: 0.8390184218883514\n",
      "4500\n",
      "Epoch 1284, Loss: 0.7308377518124051\n",
      "Test Loss after Epoch 1284: 0.8377541253566742\n",
      "4500\n",
      "Epoch 1285, Loss: 0.7293864529662663\n",
      "Test Loss after Epoch 1285: 0.8369640805721283\n",
      "4500\n",
      "Epoch 1286, Loss: 0.7293853987058003\n",
      "Test Loss after Epoch 1286: 0.8375524435043334\n",
      "4500\n",
      "Epoch 1287, Loss: 0.7299249342017704\n",
      "Test Loss after Epoch 1287: 0.8388862113952636\n",
      "4500\n",
      "Epoch 1288, Loss: 0.7295681387848324\n",
      "Test Loss after Epoch 1288: 0.8384250013828277\n",
      "4500\n",
      "Epoch 1289, Loss: 0.7307797418700324\n",
      "Test Loss after Epoch 1289: 0.8386444931030274\n",
      "4500\n",
      "Epoch 1290, Loss: 0.7295386674933964\n",
      "Test Loss after Epoch 1290: 0.8393032307624817\n",
      "4500\n",
      "Epoch 1291, Loss: 0.7292800132698483\n",
      "Test Loss after Epoch 1291: 0.8413610610961914\n",
      "4500\n",
      "Epoch 1292, Loss: 0.7298000370396508\n",
      "Test Loss after Epoch 1292: 0.843676564693451\n",
      "4500\n",
      "Epoch 1293, Loss: 0.730015046702491\n",
      "Test Loss after Epoch 1293: 0.8403707642555237\n",
      "4500\n",
      "Epoch 1294, Loss: 0.7294904351764255\n",
      "Test Loss after Epoch 1294: 0.8405531046390533\n",
      "4500\n",
      "Epoch 1295, Loss: 0.729586422363917\n",
      "Test Loss after Epoch 1295: 0.8420763549804687\n",
      "4500\n",
      "Epoch 1296, Loss: 0.7300234987735749\n",
      "Test Loss after Epoch 1296: 0.8399609603881836\n",
      "4500\n",
      "Epoch 1297, Loss: 0.7295695971118079\n",
      "Test Loss after Epoch 1297: 0.8375138056278228\n",
      "4500\n",
      "Epoch 1298, Loss: 0.7300116149054633\n",
      "Test Loss after Epoch 1298: 0.8351912183761596\n",
      "4500\n",
      "Epoch 1299, Loss: 0.730562024169498\n",
      "Test Loss after Epoch 1299: 0.8325226821899414\n",
      "4500\n",
      "Epoch 1300, Loss: 0.7292374126381345\n",
      "Test Loss after Epoch 1300: 0.8344848909378052\n",
      "4500\n",
      "Epoch 1301, Loss: 0.7295573261578878\n",
      "Test Loss after Epoch 1301: 0.8385299308300018\n",
      "4500\n",
      "Epoch 1302, Loss: 0.7296764147546556\n",
      "Test Loss after Epoch 1302: 0.8412914605140686\n",
      "4500\n",
      "Epoch 1303, Loss: 0.7285035523308648\n",
      "Test Loss after Epoch 1303: 0.8428874821662903\n",
      "4500\n",
      "Epoch 1304, Loss: 0.728310259660085\n",
      "Test Loss after Epoch 1304: 0.845458890914917\n",
      "4500\n",
      "Epoch 1305, Loss: 0.7292873190773858\n",
      "Test Loss after Epoch 1305: 0.8472798039913177\n",
      "4500\n",
      "Epoch 1306, Loss: 0.7300306130515205\n",
      "Test Loss after Epoch 1306: 0.8466573071479797\n",
      "4500\n",
      "Epoch 1307, Loss: 0.730224736266666\n",
      "Test Loss after Epoch 1307: 0.8459605438709259\n",
      "4500\n",
      "Epoch 1308, Loss: 0.7291727831628587\n",
      "Test Loss after Epoch 1308: 0.8441060616970062\n",
      "4500\n",
      "Epoch 1309, Loss: 0.7306394594510396\n",
      "Test Loss after Epoch 1309: 0.842296198129654\n",
      "4500\n",
      "Epoch 1310, Loss: 0.7293142203754849\n",
      "Test Loss after Epoch 1310: 0.8429283890724182\n",
      "4500\n",
      "Epoch 1311, Loss: 0.7307305606206258\n",
      "Test Loss after Epoch 1311: 0.8440627043247223\n",
      "4500\n",
      "Epoch 1312, Loss: 0.7285184732808007\n",
      "Test Loss after Epoch 1312: 0.8453568391799927\n",
      "4500\n",
      "Epoch 1313, Loss: 0.7295863898330265\n",
      "Test Loss after Epoch 1313: 0.8466452054977417\n",
      "4500\n",
      "Epoch 1314, Loss: 0.7304714997874366\n",
      "Test Loss after Epoch 1314: 0.8406232080459595\n",
      "4500\n",
      "Epoch 1315, Loss: 0.7285028615262773\n",
      "Test Loss after Epoch 1315: 0.8388126766681672\n",
      "4500\n",
      "Epoch 1316, Loss: 0.7306243676609463\n",
      "Test Loss after Epoch 1316: 0.8395949242115021\n",
      "4500\n",
      "Epoch 1317, Loss: 0.73022471247779\n",
      "Test Loss after Epoch 1317: 0.8363438305854798\n",
      "4500\n",
      "Epoch 1318, Loss: 0.7293894138866001\n",
      "Test Loss after Epoch 1318: 0.8329129784107209\n",
      "4500\n",
      "Epoch 1319, Loss: 0.7305792021751404\n",
      "Test Loss after Epoch 1319: 0.832361044883728\n",
      "4500\n",
      "Epoch 1320, Loss: 0.7304608808093601\n",
      "Test Loss after Epoch 1320: 0.8339737651348114\n",
      "4500\n",
      "Epoch 1321, Loss: 0.7290583964453803\n",
      "Test Loss after Epoch 1321: 0.8355529720783234\n",
      "4500\n",
      "Epoch 1322, Loss: 0.7299209852748447\n",
      "Test Loss after Epoch 1322: 0.8369800596237182\n",
      "4500\n",
      "Epoch 1323, Loss: 0.7290168172518412\n",
      "Test Loss after Epoch 1323: 0.839937745809555\n",
      "4500\n",
      "Epoch 1324, Loss: 0.7299752631187439\n",
      "Test Loss after Epoch 1324: 0.8428689289093018\n",
      "4500\n",
      "Epoch 1325, Loss: 0.728989334265391\n",
      "Test Loss after Epoch 1325: 0.8447310905456543\n",
      "4500\n",
      "Epoch 1326, Loss: 0.7295339246855842\n",
      "Test Loss after Epoch 1326: 0.8460761618614197\n",
      "4500\n",
      "Epoch 1327, Loss: 0.7286387555599213\n",
      "Test Loss after Epoch 1327: 0.8474626169204712\n",
      "4500\n",
      "Epoch 1328, Loss: 0.7293142465220558\n",
      "Test Loss after Epoch 1328: 0.8483975253105164\n",
      "4500\n",
      "Epoch 1329, Loss: 0.7295410825941298\n",
      "Test Loss after Epoch 1329: 0.8479747993946075\n",
      "4500\n",
      "Epoch 1330, Loss: 0.7290507035785251\n",
      "Test Loss after Epoch 1330: 0.848282830953598\n",
      "4500\n",
      "Epoch 1331, Loss: 0.7303193048636119\n",
      "Test Loss after Epoch 1331: 0.8482307403087616\n",
      "4500\n",
      "Epoch 1332, Loss: 0.7292022638056014\n",
      "Test Loss after Epoch 1332: 0.8461854684352875\n",
      "4500\n",
      "Epoch 1333, Loss: 0.7296709954208798\n",
      "Test Loss after Epoch 1333: 0.8478514666557312\n",
      "4500\n",
      "Epoch 1334, Loss: 0.7286115865442488\n",
      "Test Loss after Epoch 1334: 0.8506333773136139\n",
      "4500\n",
      "Epoch 1335, Loss: 0.7283999701076084\n",
      "Test Loss after Epoch 1335: 0.8493846650123597\n",
      "4500\n",
      "Epoch 1336, Loss: 0.7306536704434289\n",
      "Test Loss after Epoch 1336: 0.8456488077640534\n",
      "4500\n",
      "Epoch 1337, Loss: 0.7287013830873701\n",
      "Test Loss after Epoch 1337: 0.8449262874126434\n",
      "4500\n",
      "Epoch 1338, Loss: 0.7300087149937947\n",
      "Test Loss after Epoch 1338: 0.8451827051639557\n",
      "4500\n",
      "Epoch 1339, Loss: 0.728533199124866\n",
      "Test Loss after Epoch 1339: 0.8450089454650879\n",
      "4500\n",
      "Epoch 1340, Loss: 0.7294756085342831\n",
      "Test Loss after Epoch 1340: 0.8463970370292664\n",
      "4500\n",
      "Epoch 1341, Loss: 0.728702320019404\n",
      "Test Loss after Epoch 1341: 0.8490168673992157\n",
      "4500\n",
      "Epoch 1342, Loss: 0.730710739824507\n",
      "Test Loss after Epoch 1342: 0.8489207653999329\n",
      "4500\n",
      "Epoch 1343, Loss: 0.7302591115633646\n",
      "Test Loss after Epoch 1343: 0.8479621999263763\n",
      "4500\n",
      "Epoch 1344, Loss: 0.7293097254965041\n",
      "Test Loss after Epoch 1344: 0.8506065089702606\n",
      "4500\n",
      "Epoch 1345, Loss: 0.7295914193789165\n",
      "Test Loss after Epoch 1345: 0.8525361189842224\n",
      "4500\n",
      "Epoch 1346, Loss: 0.7284572145673964\n",
      "Test Loss after Epoch 1346: 0.8456485044956207\n",
      "4500\n",
      "Epoch 1347, Loss: 0.7305774130026499\n",
      "Test Loss after Epoch 1347: 0.8455185763835907\n",
      "4500\n",
      "Epoch 1348, Loss: 0.7282455174393124\n",
      "Test Loss after Epoch 1348: 0.8484052081108093\n",
      "4500\n",
      "Epoch 1349, Loss: 0.7291032872464922\n",
      "Test Loss after Epoch 1349: 0.8525445637702942\n",
      "4500\n",
      "Epoch 1350, Loss: 0.7300501695473989\n",
      "Test Loss after Epoch 1350: 0.8543899435997009\n",
      "4500\n",
      "Epoch 1351, Loss: 0.7291373825073242\n",
      "Test Loss after Epoch 1351: 0.8559345309734344\n",
      "4500\n",
      "Epoch 1352, Loss: 0.7284691626230876\n",
      "Test Loss after Epoch 1352: 0.8585132715702057\n",
      "4500\n",
      "Epoch 1353, Loss: 0.7300602216455672\n",
      "Test Loss after Epoch 1353: 0.8603173513412475\n",
      "4500\n",
      "Epoch 1354, Loss: 0.7289820721944174\n",
      "Test Loss after Epoch 1354: 0.8582927489280701\n",
      "4500\n",
      "Epoch 1355, Loss: 0.7289957186910841\n",
      "Test Loss after Epoch 1355: 0.8563798410892487\n",
      "4500\n",
      "Epoch 1356, Loss: 0.7295630610254076\n",
      "Test Loss after Epoch 1356: 0.854902893781662\n",
      "4500\n",
      "Epoch 1357, Loss: 0.7290332524776458\n",
      "Test Loss after Epoch 1357: 0.8555270519256591\n",
      "4500\n",
      "Epoch 1358, Loss: 0.7292415274514092\n",
      "Test Loss after Epoch 1358: 0.8548648447990418\n",
      "4500\n",
      "Epoch 1359, Loss: 0.7280766860114204\n",
      "Test Loss after Epoch 1359: 0.850838784456253\n",
      "4500\n",
      "Epoch 1360, Loss: 0.7300864326159159\n",
      "Test Loss after Epoch 1360: 0.8440510506629944\n",
      "4500\n",
      "Epoch 1361, Loss: 0.7296169452402327\n",
      "Test Loss after Epoch 1361: 0.8427990946769714\n",
      "4500\n",
      "Epoch 1362, Loss: 0.7301063721179962\n",
      "Test Loss after Epoch 1362: 0.843657719373703\n",
      "4500\n",
      "Epoch 1363, Loss: 0.7290944729116228\n",
      "Test Loss after Epoch 1363: 0.8450506091117859\n",
      "4500\n",
      "Epoch 1364, Loss: 0.7290331308841705\n",
      "Test Loss after Epoch 1364: 0.8443625421524048\n",
      "4500\n",
      "Epoch 1365, Loss: 0.7304315082761976\n",
      "Test Loss after Epoch 1365: 0.8435433375835418\n",
      "4500\n",
      "Epoch 1366, Loss: 0.7290035038259294\n",
      "Test Loss after Epoch 1366: 0.8426282770633697\n",
      "4500\n",
      "Epoch 1367, Loss: 0.7287878047360314\n",
      "Test Loss after Epoch 1367: 0.8426276679039002\n",
      "4500\n",
      "Epoch 1368, Loss: 0.7301075546741486\n",
      "Test Loss after Epoch 1368: 0.8429425864219665\n",
      "4500\n",
      "Epoch 1369, Loss: 0.7302702236440447\n",
      "Test Loss after Epoch 1369: 0.8436828098297119\n",
      "4500\n",
      "Epoch 1370, Loss: 0.7289650852945115\n",
      "Test Loss after Epoch 1370: 0.8409063549041748\n",
      "4500\n",
      "Epoch 1371, Loss: 0.7288937570518917\n",
      "Test Loss after Epoch 1371: 0.8390976364612579\n",
      "4500\n",
      "Epoch 1372, Loss: 0.729748085949156\n",
      "Test Loss after Epoch 1372: 0.8393501231670379\n",
      "4500\n",
      "Epoch 1373, Loss: 0.7295364790757497\n",
      "Test Loss after Epoch 1373: 0.8414479379653931\n",
      "4500\n",
      "Epoch 1374, Loss: 0.7287369615501827\n",
      "Test Loss after Epoch 1374: 0.8421844913959503\n",
      "4500\n",
      "Epoch 1375, Loss: 0.7295084461371104\n",
      "Test Loss after Epoch 1375: 0.8433719820976258\n",
      "4500\n",
      "Epoch 1376, Loss: 0.7278616402679019\n",
      "Test Loss after Epoch 1376: 0.8421579654216766\n",
      "4500\n",
      "Epoch 1377, Loss: 0.7293224476178487\n",
      "Test Loss after Epoch 1377: 0.8421488065719605\n",
      "4500\n",
      "Epoch 1378, Loss: 0.729867932955424\n",
      "Test Loss after Epoch 1378: 0.8432684261798858\n",
      "4500\n",
      "Epoch 1379, Loss: 0.7291783362759484\n",
      "Test Loss after Epoch 1379: 0.8435215950012207\n",
      "4500\n",
      "Epoch 1380, Loss: 0.7295523087183634\n",
      "Test Loss after Epoch 1380: 0.8441409187316895\n",
      "4500\n",
      "Epoch 1381, Loss: 0.7280540056493547\n",
      "Test Loss after Epoch 1381: 0.8442528252601623\n",
      "4500\n",
      "Epoch 1382, Loss: 0.729406527572208\n",
      "Test Loss after Epoch 1382: 0.8445707194805145\n",
      "4500\n",
      "Epoch 1383, Loss: 0.7308651965194278\n",
      "Test Loss after Epoch 1383: 0.8445053498744964\n",
      "4500\n",
      "Epoch 1384, Loss: 0.729113293727239\n",
      "Test Loss after Epoch 1384: 0.8465414335727691\n",
      "4500\n",
      "Epoch 1385, Loss: 0.7294348395135668\n",
      "Test Loss after Epoch 1385: 0.8494026741981506\n",
      "4500\n",
      "Epoch 1386, Loss: 0.7288827788564894\n",
      "Test Loss after Epoch 1386: 0.8507894539833069\n",
      "4500\n",
      "Epoch 1387, Loss: 0.7290437761942545\n",
      "Test Loss after Epoch 1387: 0.8506593418121338\n",
      "4500\n",
      "Epoch 1388, Loss: 0.7287309742238787\n",
      "Test Loss after Epoch 1388: 0.8509516668319702\n",
      "4500\n",
      "Epoch 1389, Loss: 0.7289240136146545\n",
      "Test Loss after Epoch 1389: 0.8502507779598236\n",
      "4500\n",
      "Epoch 1390, Loss: 0.7285534883870018\n",
      "Test Loss after Epoch 1390: 0.8482601823806762\n",
      "4500\n",
      "Epoch 1391, Loss: 0.7286942202250163\n",
      "Test Loss after Epoch 1391: 0.8465408699512482\n",
      "4500\n",
      "Epoch 1392, Loss: 0.7302927826510536\n",
      "Test Loss after Epoch 1392: 0.8455745041370392\n",
      "4500\n",
      "Epoch 1393, Loss: 0.7284832421408759\n",
      "Test Loss after Epoch 1393: 0.8451987934112549\n",
      "4500\n",
      "Epoch 1394, Loss: 0.7291534605026245\n",
      "Test Loss after Epoch 1394: 0.8455167844295501\n",
      "4500\n",
      "Epoch 1395, Loss: 0.7287697734832763\n",
      "Test Loss after Epoch 1395: 0.8462724664211273\n",
      "4500\n",
      "Epoch 1396, Loss: 0.7282264218330383\n",
      "Test Loss after Epoch 1396: 0.8474861009120941\n",
      "900 4500 500\n",
      "4500\n",
      "Epoch 1, Loss: 4.079274966451857\n",
      "Test Loss after Epoch 1: 3.654472994804382\n",
      "4500\n",
      "Epoch 2, Loss: 3.693556747542487\n",
      "Test Loss after Epoch 2: 3.4907918233871458\n",
      "4500\n",
      "Epoch 3, Loss: 3.5026330996619333\n",
      "Test Loss after Epoch 3: 3.391609935760498\n",
      "4500\n",
      "Epoch 4, Loss: 3.3995591273837618\n",
      "Test Loss after Epoch 4: 3.350125551223755\n",
      "4500\n",
      "Epoch 5, Loss: 3.335447736846076\n",
      "Test Loss after Epoch 5: 3.3143908252716066\n",
      "4500\n",
      "Epoch 6, Loss: 3.291045888688829\n",
      "Test Loss after Epoch 6: 3.275973525047302\n",
      "4500\n",
      "Epoch 7, Loss: 3.2455770744747587\n",
      "Test Loss after Epoch 7: 3.212467200279236\n",
      "4500\n",
      "Epoch 8, Loss: 3.1843022139867148\n",
      "Test Loss after Epoch 8: 3.104121615409851\n",
      "4500\n",
      "Epoch 9, Loss: 3.0870764546924168\n",
      "Test Loss after Epoch 9: 2.9557396802902223\n",
      "4500\n",
      "Epoch 10, Loss: 2.975717785199483\n",
      "Test Loss after Epoch 10: 2.8739377965927124\n",
      "4500\n",
      "Epoch 11, Loss: 2.8657835546069674\n",
      "Test Loss after Epoch 11: 2.8138995094299317\n",
      "4500\n",
      "Epoch 12, Loss: 2.7816519680023193\n",
      "Test Loss after Epoch 12: 2.7493495950698854\n",
      "4500\n",
      "Epoch 13, Loss: 2.6965085777706572\n",
      "Test Loss after Epoch 13: 2.709199273109436\n",
      "4500\n",
      "Epoch 14, Loss: 2.6245791646109686\n",
      "Test Loss after Epoch 14: 2.652922616958618\n",
      "4500\n",
      "Epoch 15, Loss: 2.554184505038791\n",
      "Test Loss after Epoch 15: 2.6154718465805056\n",
      "4500\n",
      "Epoch 16, Loss: 2.4944947752422757\n",
      "Test Loss after Epoch 16: 2.5680197944641114\n",
      "4500\n",
      "Epoch 17, Loss: 2.4399990372127958\n",
      "Test Loss after Epoch 17: 2.5502973012924195\n",
      "4500\n",
      "Epoch 18, Loss: 2.3901693834728666\n",
      "Test Loss after Epoch 18: 2.5158821153640747\n",
      "4500\n",
      "Epoch 19, Loss: 2.3498827834659153\n",
      "Test Loss after Epoch 19: 2.4729436712265014\n",
      "4500\n",
      "Epoch 20, Loss: 2.3142331892649333\n",
      "Test Loss after Epoch 20: 2.4514394941329956\n",
      "4500\n",
      "Epoch 21, Loss: 2.276669441011217\n",
      "Test Loss after Epoch 21: 2.4045192947387695\n",
      "4500\n",
      "Epoch 22, Loss: 2.244738151020474\n",
      "Test Loss after Epoch 22: 2.3915532779693605\n",
      "4500\n",
      "Epoch 23, Loss: 2.213999434577094\n",
      "Test Loss after Epoch 23: 2.345954107284546\n",
      "4500\n",
      "Epoch 24, Loss: 2.188862299389309\n",
      "Test Loss after Epoch 24: 2.338504758834839\n",
      "4500\n",
      "Epoch 25, Loss: 2.1595199683507285\n",
      "Test Loss after Epoch 25: 2.3025940866470336\n",
      "4500\n",
      "Epoch 26, Loss: 2.1329255515204535\n",
      "Test Loss after Epoch 26: 2.291903967857361\n",
      "4500\n",
      "Epoch 27, Loss: 2.1036951035393607\n",
      "Test Loss after Epoch 27: 2.2602096672058107\n",
      "4500\n",
      "Epoch 28, Loss: 2.0793252024120754\n",
      "Test Loss after Epoch 28: 2.237480189323425\n",
      "4500\n",
      "Epoch 29, Loss: 2.047583690749274\n",
      "Test Loss after Epoch 29: 2.2096810846328734\n",
      "4500\n",
      "Epoch 30, Loss: 2.025297935909695\n",
      "Test Loss after Epoch 30: 2.178095187187195\n",
      "4500\n",
      "Epoch 31, Loss: 2.0015628158781262\n",
      "Test Loss after Epoch 31: 2.1631284008026124\n",
      "4500\n",
      "Epoch 32, Loss: 1.9743874769740635\n",
      "Test Loss after Epoch 32: 2.127266400337219\n",
      "4500\n",
      "Epoch 33, Loss: 1.9529459137386747\n",
      "Test Loss after Epoch 33: 2.104580936431885\n",
      "4500\n",
      "Epoch 34, Loss: 1.9269337673187257\n",
      "Test Loss after Epoch 34: 2.073799711227417\n",
      "4500\n",
      "Epoch 35, Loss: 1.9065980749660068\n",
      "Test Loss after Epoch 35: 2.0666957750320436\n",
      "4500\n",
      "Epoch 36, Loss: 1.8830459425184463\n",
      "Test Loss after Epoch 36: 2.047907269954681\n",
      "4500\n",
      "Epoch 37, Loss: 1.8593378672599792\n",
      "Test Loss after Epoch 37: 2.0284227805137633\n",
      "4500\n",
      "Epoch 38, Loss: 1.8291503654585943\n",
      "Test Loss after Epoch 38: 2.0150589523315428\n",
      "4500\n",
      "Epoch 39, Loss: 1.8124752499792312\n",
      "Test Loss after Epoch 39: 1.9933526849746703\n",
      "4500\n",
      "Epoch 40, Loss: 1.7913782197634378\n",
      "Test Loss after Epoch 40: 1.9731360125541686\n",
      "4500\n",
      "Epoch 41, Loss: 1.7667894627783034\n",
      "Test Loss after Epoch 41: 1.948516504764557\n",
      "4500\n",
      "Epoch 42, Loss: 1.7437677942911785\n",
      "Test Loss after Epoch 42: 1.9122563953399658\n",
      "4500\n",
      "Epoch 43, Loss: 1.7269444009462993\n",
      "Test Loss after Epoch 43: 1.910884931564331\n",
      "4500\n",
      "Epoch 44, Loss: 1.703800027953254\n",
      "Test Loss after Epoch 44: 1.8739787201881408\n",
      "4500\n",
      "Epoch 45, Loss: 1.6811341202523973\n",
      "Test Loss after Epoch 45: 1.8730114297866822\n",
      "4500\n",
      "Epoch 46, Loss: 1.6631937404738533\n",
      "Test Loss after Epoch 46: 1.852837664604187\n",
      "4500\n",
      "Epoch 47, Loss: 1.6440010387632582\n",
      "Test Loss after Epoch 47: 1.855030902862549\n",
      "4500\n",
      "Epoch 48, Loss: 1.6211634312735663\n",
      "Test Loss after Epoch 48: 1.81911926984787\n",
      "4500\n",
      "Epoch 49, Loss: 1.6042750033802455\n",
      "Test Loss after Epoch 49: 1.8065108437538147\n",
      "4500\n",
      "Epoch 50, Loss: 1.579404904736413\n",
      "Test Loss after Epoch 50: 1.8116234893798828\n",
      "4500\n",
      "Epoch 51, Loss: 1.5613690720134312\n",
      "Test Loss after Epoch 51: 1.784676787853241\n",
      "4500\n",
      "Epoch 52, Loss: 1.5446444715393914\n",
      "Test Loss after Epoch 52: 1.7677326216697693\n",
      "4500\n",
      "Epoch 53, Loss: 1.5273141669167412\n",
      "Test Loss after Epoch 53: 1.733400631904602\n",
      "4500\n",
      "Epoch 54, Loss: 1.4944927960501777\n",
      "Test Loss after Epoch 54: 1.7062565641403198\n",
      "4500\n",
      "Epoch 55, Loss: 1.4800094842910767\n",
      "Test Loss after Epoch 55: 1.6672973194122314\n",
      "4500\n",
      "Epoch 56, Loss: 1.4543036420080397\n",
      "Test Loss after Epoch 56: 1.6511175203323365\n",
      "4500\n",
      "Epoch 57, Loss: 1.4281452552477518\n",
      "Test Loss after Epoch 57: 1.6036223130226135\n",
      "4500\n",
      "Epoch 58, Loss: 1.4063528502782185\n",
      "Test Loss after Epoch 58: 1.5896318173408508\n",
      "4500\n",
      "Epoch 59, Loss: 1.3804181005160014\n",
      "Test Loss after Epoch 59: 1.5439259834289552\n",
      "4500\n",
      "Epoch 60, Loss: 1.3619868603282506\n",
      "Test Loss after Epoch 60: 1.53748175907135\n",
      "4500\n",
      "Epoch 61, Loss: 1.333638748380873\n",
      "Test Loss after Epoch 61: 1.4845999794006348\n",
      "4500\n",
      "Epoch 62, Loss: 1.3147419011857775\n",
      "Test Loss after Epoch 62: 1.5098796033859252\n",
      "4500\n",
      "Epoch 63, Loss: 1.2982840175628663\n",
      "Test Loss after Epoch 63: 1.443604561328888\n",
      "4500\n",
      "Epoch 64, Loss: 1.2751135504510667\n",
      "Test Loss after Epoch 64: 1.4950016298294067\n",
      "4500\n",
      "Epoch 65, Loss: 1.257863284164005\n",
      "Test Loss after Epoch 65: 1.4296978483200073\n",
      "4500\n",
      "Epoch 66, Loss: 1.2371606097221375\n",
      "Test Loss after Epoch 66: 1.4644212713241578\n",
      "4500\n",
      "Epoch 67, Loss: 1.21296515417099\n",
      "Test Loss after Epoch 67: 1.4040258054733277\n",
      "4500\n",
      "Epoch 68, Loss: 1.1999844131999546\n",
      "Test Loss after Epoch 68: 1.4507217736244202\n",
      "4500\n",
      "Epoch 69, Loss: 1.177668588426378\n",
      "Test Loss after Epoch 69: 1.4201118021011352\n",
      "4500\n",
      "Epoch 70, Loss: 1.1654520599577163\n",
      "Test Loss after Epoch 70: 1.4515589365959167\n",
      "4500\n",
      "Epoch 71, Loss: 1.1466012426482306\n",
      "Test Loss after Epoch 71: 1.4173119511604308\n",
      "4500\n",
      "Epoch 72, Loss: 1.1315872224701775\n",
      "Test Loss after Epoch 72: 1.435714644908905\n",
      "4500\n",
      "Epoch 73, Loss: 1.116653399096595\n",
      "Test Loss after Epoch 73: 1.438747705936432\n",
      "4500\n",
      "Epoch 74, Loss: 1.10597718779246\n",
      "Test Loss after Epoch 74: 1.4079108128547668\n",
      "4500\n",
      "Epoch 75, Loss: 1.0873562478489347\n",
      "Test Loss after Epoch 75: 1.429950297355652\n",
      "4500\n",
      "Epoch 76, Loss: 1.0782665043407016\n",
      "Test Loss after Epoch 76: 1.4051403307914734\n",
      "4500\n",
      "Epoch 77, Loss: 1.0667428097195095\n",
      "Test Loss after Epoch 77: 1.390630274772644\n",
      "4500\n",
      "Epoch 78, Loss: 1.050252181371053\n",
      "Test Loss after Epoch 78: 1.404353446483612\n",
      "4500\n",
      "Epoch 79, Loss: 1.040475152598487\n",
      "Test Loss after Epoch 79: 1.3537031836509705\n",
      "4500\n",
      "Epoch 80, Loss: 1.0323702113893296\n",
      "Test Loss after Epoch 80: 1.3696289868354798\n",
      "4500\n",
      "Epoch 81, Loss: 1.0203876229392157\n",
      "Test Loss after Epoch 81: 1.3373500061035157\n",
      "4500\n",
      "Epoch 82, Loss: 1.0109343824386596\n",
      "Test Loss after Epoch 82: 1.332371642589569\n",
      "4500\n",
      "Epoch 83, Loss: 0.9980555622312758\n",
      "Test Loss after Epoch 83: 1.3114017028808593\n",
      "4500\n",
      "Epoch 84, Loss: 0.9911144750912985\n",
      "Test Loss after Epoch 84: 1.3143284163475037\n",
      "4500\n",
      "Epoch 85, Loss: 0.9809865900675456\n",
      "Test Loss after Epoch 85: 1.3206114349365234\n",
      "4500\n",
      "Epoch 86, Loss: 0.9716520387596554\n",
      "Test Loss after Epoch 86: 1.3531901640892028\n",
      "4500\n",
      "Epoch 87, Loss: 0.9646989503701527\n",
      "Test Loss after Epoch 87: 1.3015357160568237\n",
      "4500\n",
      "Epoch 88, Loss: 0.9540537508593665\n",
      "Test Loss after Epoch 88: 1.2939468102455138\n",
      "4500\n",
      "Epoch 89, Loss: 0.9472029886510637\n",
      "Test Loss after Epoch 89: 1.2871011686325073\n",
      "4500\n",
      "Epoch 90, Loss: 0.9378677530023787\n",
      "Test Loss after Epoch 90: 1.3098083214759826\n",
      "4500\n",
      "Epoch 91, Loss: 0.9309874419106378\n",
      "Test Loss after Epoch 91: 1.2868974113464355\n",
      "4500\n",
      "Epoch 92, Loss: 0.9288395209842258\n",
      "Test Loss after Epoch 92: 1.2807898931503297\n",
      "4500\n",
      "Epoch 93, Loss: 0.9154044912921058\n",
      "Test Loss after Epoch 93: 1.2606263918876648\n",
      "4500\n",
      "Epoch 94, Loss: 0.9163687638176812\n",
      "Test Loss after Epoch 94: 1.2600873074531556\n",
      "4500\n",
      "Epoch 95, Loss: 0.9052123300764295\n",
      "Test Loss after Epoch 95: 1.2534091029167176\n",
      "4500\n",
      "Epoch 96, Loss: 0.9012868315378825\n",
      "Test Loss after Epoch 96: 1.2175542573928833\n",
      "4500\n",
      "Epoch 97, Loss: 0.8941169509887695\n",
      "Test Loss after Epoch 97: 1.2994031958580017\n",
      "4500\n",
      "Epoch 98, Loss: 0.8921290838718414\n",
      "Test Loss after Epoch 98: 1.2711273455619811\n",
      "4500\n",
      "Epoch 99, Loss: 0.8892604866027832\n",
      "Test Loss after Epoch 99: 1.1899992094039917\n",
      "4500\n",
      "Epoch 100, Loss: 0.8842737384902106\n",
      "Test Loss after Epoch 100: 1.2624732871055604\n",
      "4500\n",
      "Epoch 101, Loss: 0.8777535388204787\n",
      "Test Loss after Epoch 101: 1.1902619652748108\n",
      "4500\n",
      "Epoch 102, Loss: 0.8761141901546055\n",
      "Test Loss after Epoch 102: 1.2145046224594116\n",
      "4500\n",
      "Epoch 103, Loss: 0.8704674283663432\n",
      "Test Loss after Epoch 103: 1.1846252508163453\n",
      "4500\n",
      "Epoch 104, Loss: 0.8676892079777188\n",
      "Test Loss after Epoch 104: 1.237027603149414\n",
      "4500\n",
      "Epoch 105, Loss: 0.8644984855122037\n",
      "Test Loss after Epoch 105: 1.196030993461609\n",
      "4500\n",
      "Epoch 106, Loss: 0.8593755299250285\n",
      "Test Loss after Epoch 106: 1.1867280101776123\n",
      "4500\n",
      "Epoch 107, Loss: 0.8607956166267395\n",
      "Test Loss after Epoch 107: 1.1736729259490968\n",
      "4500\n",
      "Epoch 108, Loss: 0.856956066025628\n",
      "Test Loss after Epoch 108: 1.152502236366272\n",
      "4500\n",
      "Epoch 109, Loss: 0.8551782394250234\n",
      "Test Loss after Epoch 109: 1.1536663174629211\n",
      "4500\n",
      "Epoch 110, Loss: 0.8521263320446014\n",
      "Test Loss after Epoch 110: 1.2050863642692566\n",
      "4500\n",
      "Epoch 111, Loss: 0.8509595175849067\n",
      "Test Loss after Epoch 111: 1.134405207157135\n",
      "4500\n",
      "Epoch 112, Loss: 0.8443141967985365\n",
      "Test Loss after Epoch 112: 1.1772211294174195\n",
      "4500\n",
      "Epoch 113, Loss: 0.8404804069466061\n",
      "Test Loss after Epoch 113: 1.0978568000793456\n",
      "4500\n",
      "Epoch 114, Loss: 0.8434636545976003\n",
      "Test Loss after Epoch 114: 1.1384835705757141\n",
      "4500\n",
      "Epoch 115, Loss: 0.8370287561681535\n",
      "Test Loss after Epoch 115: 1.1125039181709289\n",
      "4500\n",
      "Epoch 116, Loss: 0.834718001127243\n",
      "Test Loss after Epoch 116: 1.152116051673889\n",
      "4500\n",
      "Epoch 117, Loss: 0.8342780270841387\n",
      "Test Loss after Epoch 117: 1.1075983757972718\n",
      "4500\n",
      "Epoch 118, Loss: 0.82948255499204\n",
      "Test Loss after Epoch 118: 1.149743814945221\n",
      "4500\n",
      "Epoch 119, Loss: 0.8273518563111623\n",
      "Test Loss after Epoch 119: 1.1630899448394776\n",
      "4500\n",
      "Epoch 120, Loss: 0.8292785262001885\n",
      "Test Loss after Epoch 120: 1.1238844804763795\n",
      "4500\n",
      "Epoch 121, Loss: 0.8231413322024875\n",
      "Test Loss after Epoch 121: 1.1489652924537659\n",
      "4500\n",
      "Epoch 122, Loss: 0.8212726128631168\n",
      "Test Loss after Epoch 122: 1.1230180106163026\n",
      "4500\n",
      "Epoch 123, Loss: 0.8198691785335541\n",
      "Test Loss after Epoch 123: 1.176880114555359\n",
      "4500\n",
      "Epoch 124, Loss: 0.8174823173152076\n",
      "Test Loss after Epoch 124: 1.114792281150818\n",
      "4500\n",
      "Epoch 125, Loss: 0.820080029990938\n",
      "Test Loss after Epoch 125: 1.166855938911438\n",
      "4500\n",
      "Epoch 126, Loss: 0.8172403575579326\n",
      "Test Loss after Epoch 126: 1.1490539326667786\n",
      "4500\n",
      "Epoch 127, Loss: 0.8159854406250848\n",
      "Test Loss after Epoch 127: 1.152738639831543\n",
      "4500\n",
      "Epoch 128, Loss: 0.8146612548033396\n",
      "Test Loss after Epoch 128: 1.1368437790870667\n",
      "4500\n",
      "Epoch 129, Loss: 0.8110986438062456\n",
      "Test Loss after Epoch 129: 1.183516360282898\n",
      "4500\n",
      "Epoch 130, Loss: 0.8126560797956255\n",
      "Test Loss after Epoch 130: 1.1152620387077332\n",
      "4500\n",
      "Epoch 131, Loss: 0.8129153134822845\n",
      "Test Loss after Epoch 131: 1.1535457129478455\n",
      "4500\n",
      "Epoch 132, Loss: 0.8099701902601454\n",
      "Test Loss after Epoch 132: 1.1414793543815613\n",
      "4500\n",
      "Epoch 133, Loss: 0.8080996188587612\n",
      "Test Loss after Epoch 133: 1.1558884110450744\n",
      "Rolling back to best model from epoch 113\n",
      "Best test loss: 1.0978568000793456\n",
      "4500\n",
      "Epoch 134, Loss: 0.8400239746305678\n",
      "Test Loss after Epoch 134: 1.1757295660972595\n",
      "4500\n",
      "Epoch 135, Loss: 0.8391902145279778\n",
      "Test Loss after Epoch 135: 1.0979650459289552\n",
      "4500\n",
      "Epoch 136, Loss: 0.8341294687059191\n",
      "Test Loss after Epoch 136: 1.1487274074554443\n",
      "4500\n",
      "Epoch 137, Loss: 0.8343193288114336\n",
      "Test Loss after Epoch 137: 1.0961194686889648\n",
      "4500\n",
      "Epoch 138, Loss: 0.8316487573517694\n",
      "Test Loss after Epoch 138: 1.1247610745429992\n",
      "4500\n",
      "Epoch 139, Loss: 0.8325658829742008\n",
      "Test Loss after Epoch 139: 1.1055609593391418\n",
      "4500\n",
      "Epoch 140, Loss: 0.8279938165346782\n",
      "Test Loss after Epoch 140: 1.1565797443389894\n",
      "4500\n",
      "Epoch 141, Loss: 0.8264854181607564\n",
      "Test Loss after Epoch 141: 1.1265459527969361\n",
      "4500\n",
      "Epoch 142, Loss: 0.8236547156704797\n",
      "Test Loss after Epoch 142: 1.1667640776634216\n",
      "4500\n",
      "Epoch 143, Loss: 0.8220800662835439\n",
      "Test Loss after Epoch 143: 1.1167364163398743\n",
      "4500\n",
      "Epoch 144, Loss: 0.8194395714865791\n",
      "Test Loss after Epoch 144: 1.17928879737854\n",
      "4500\n",
      "Epoch 145, Loss: 0.8194230584833357\n",
      "Test Loss after Epoch 145: 1.1816694045066833\n",
      "4500\n",
      "Epoch 146, Loss: 0.8155818397468991\n",
      "Test Loss after Epoch 146: 1.1799367456436156\n",
      "4500\n",
      "Epoch 147, Loss: 0.8173913133144378\n",
      "Test Loss after Epoch 147: 1.1592424044609069\n",
      "4500\n",
      "Epoch 148, Loss: 0.8120894059869979\n",
      "Test Loss after Epoch 148: 1.1802146792411805\n",
      "4500\n",
      "Epoch 149, Loss: 0.809683215353224\n",
      "Test Loss after Epoch 149: 1.1104328255653382\n",
      "4500\n",
      "Epoch 150, Loss: 0.8119196314811706\n",
      "Test Loss after Epoch 150: 1.1197750763893128\n",
      "4500\n",
      "Epoch 151, Loss: 0.8075285970899794\n",
      "Test Loss after Epoch 151: 1.1147411189079284\n",
      "4500\n",
      "Epoch 152, Loss: 0.8101656464735667\n",
      "Test Loss after Epoch 152: 1.138027183532715\n",
      "4500\n",
      "Epoch 153, Loss: 0.8056333413653903\n",
      "Test Loss after Epoch 153: 1.1212633147239686\n",
      "4500\n",
      "Epoch 154, Loss: 0.8065519133938683\n",
      "Test Loss after Epoch 154: 1.1398025131225586\n",
      "4500\n",
      "Epoch 155, Loss: 0.8054895336627961\n",
      "Test Loss after Epoch 155: 1.14398007440567\n",
      "4500\n",
      "Epoch 156, Loss: 0.8043370002375708\n",
      "Test Loss after Epoch 156: 1.149434175491333\n",
      "4500\n",
      "Epoch 157, Loss: 0.8036192474365235\n",
      "Test Loss after Epoch 157: 1.1160196332931518\n",
      "4500\n",
      "Epoch 158, Loss: 0.8027321334150103\n",
      "Test Loss after Epoch 158: 1.1450753455162048\n",
      "4500\n",
      "Epoch 159, Loss: 0.8006550512843662\n",
      "Test Loss after Epoch 159: 1.0672919235229492\n",
      "4500\n",
      "Epoch 160, Loss: 0.7990208676391177\n",
      "Test Loss after Epoch 160: 1.0960585536956786\n",
      "4500\n",
      "Epoch 161, Loss: 0.7959901220533583\n",
      "Test Loss after Epoch 161: 1.0631621894836425\n",
      "4500\n",
      "Epoch 162, Loss: 0.7975651968055302\n",
      "Test Loss after Epoch 162: 1.148633610725403\n",
      "4500\n",
      "Epoch 163, Loss: 0.7943305860625373\n",
      "Test Loss after Epoch 163: 1.0503016514778136\n",
      "4500\n",
      "Epoch 164, Loss: 0.7964472009605832\n",
      "Test Loss after Epoch 164: 1.0932803921699523\n",
      "4500\n",
      "Epoch 165, Loss: 0.792882345173094\n",
      "Test Loss after Epoch 165: 1.043212525844574\n",
      "4500\n",
      "Epoch 166, Loss: 0.7955511387983958\n",
      "Test Loss after Epoch 166: 1.0546268939971923\n",
      "4500\n",
      "Epoch 167, Loss: 0.7915641052458021\n",
      "Test Loss after Epoch 167: 1.0472315926551818\n",
      "4500\n",
      "Epoch 168, Loss: 0.7885526144239637\n",
      "Test Loss after Epoch 168: 1.0570759081840515\n",
      "4500\n",
      "Epoch 169, Loss: 0.7882321258385976\n",
      "Test Loss after Epoch 169: 1.0675474615097047\n",
      "4500\n",
      "Epoch 170, Loss: 0.7897966557078892\n",
      "Test Loss after Epoch 170: 1.104511203289032\n",
      "4500\n",
      "Epoch 171, Loss: 0.7899835368527306\n",
      "Test Loss after Epoch 171: 1.1135833191871642\n",
      "4500\n",
      "Epoch 172, Loss: 0.7923185777664185\n",
      "Test Loss after Epoch 172: 1.085987723827362\n",
      "4500\n",
      "Epoch 173, Loss: 0.7897201593981849\n",
      "Test Loss after Epoch 173: 1.084424639225006\n",
      "4500\n",
      "Epoch 174, Loss: 0.785455439514584\n",
      "Test Loss after Epoch 174: 1.0350855469703675\n",
      "4500\n",
      "Epoch 175, Loss: 0.7883356286684672\n",
      "Test Loss after Epoch 175: 1.0002742357254029\n",
      "4500\n",
      "Epoch 176, Loss: 0.7848648330370586\n",
      "Test Loss after Epoch 176: 1.0323476371765137\n",
      "4500\n",
      "Epoch 177, Loss: 0.7849180016252729\n",
      "Test Loss after Epoch 177: 1.043757749557495\n",
      "4500\n",
      "Epoch 178, Loss: 0.7848237206670973\n",
      "Test Loss after Epoch 178: 1.0525420608520508\n",
      "4500\n",
      "Epoch 179, Loss: 0.7829164029492273\n",
      "Test Loss after Epoch 179: 1.045942757129669\n",
      "4500\n",
      "Epoch 180, Loss: 0.7849414979616801\n",
      "Test Loss after Epoch 180: 1.0368062918186187\n",
      "4500\n",
      "Epoch 181, Loss: 0.7809349843131171\n",
      "Test Loss after Epoch 181: 1.0290618271827698\n",
      "4500\n",
      "Epoch 182, Loss: 0.7818343996736739\n",
      "Test Loss after Epoch 182: 1.0193458325862885\n",
      "4500\n",
      "Epoch 183, Loss: 0.7802293377717336\n",
      "Test Loss after Epoch 183: 1.0395751185417175\n",
      "4500\n",
      "Epoch 184, Loss: 0.7812158304320441\n",
      "Test Loss after Epoch 184: 1.002452489376068\n",
      "4500\n",
      "Epoch 185, Loss: 0.7802607883082496\n",
      "Test Loss after Epoch 185: 1.0043758845329285\n",
      "4500\n",
      "Epoch 186, Loss: 0.7790280473497179\n",
      "Test Loss after Epoch 186: 0.9596298990249634\n",
      "4500\n",
      "Epoch 187, Loss: 0.7821574187013838\n",
      "Test Loss after Epoch 187: 0.9773773009777069\n",
      "4500\n",
      "Epoch 188, Loss: 0.7801087857882182\n",
      "Test Loss after Epoch 188: 0.9673263463973999\n",
      "4500\n",
      "Epoch 189, Loss: 0.77698120909267\n",
      "Test Loss after Epoch 189: 0.9882505593299866\n",
      "4500\n",
      "Epoch 190, Loss: 0.7769823911190032\n",
      "Test Loss after Epoch 190: 0.9859241375923157\n",
      "4500\n",
      "Epoch 191, Loss: 0.7762038718064626\n",
      "Test Loss after Epoch 191: 0.9641876311302185\n",
      "4500\n",
      "Epoch 192, Loss: 0.7748293491999309\n",
      "Test Loss after Epoch 192: 1.0376482977867127\n",
      "4500\n",
      "Epoch 193, Loss: 0.7758739712768131\n",
      "Test Loss after Epoch 193: 0.989308812379837\n",
      "4500\n",
      "Epoch 194, Loss: 0.7762935606373681\n",
      "Test Loss after Epoch 194: 1.0397923860549927\n",
      "4500\n",
      "Epoch 195, Loss: 0.7737708533869849\n",
      "Test Loss after Epoch 195: 0.9888087918758393\n",
      "4500\n",
      "Epoch 196, Loss: 0.7755921489132775\n",
      "Test Loss after Epoch 196: 1.0332579159736632\n",
      "4500\n",
      "Epoch 197, Loss: 0.7716782458358341\n",
      "Test Loss after Epoch 197: 0.9965474829673767\n",
      "4500\n",
      "Epoch 198, Loss: 0.7739744720988804\n",
      "Test Loss after Epoch 198: 1.0592706332206725\n",
      "4500\n",
      "Epoch 199, Loss: 0.7746339834796058\n",
      "Test Loss after Epoch 199: 1.030505735874176\n",
      "4500\n",
      "Epoch 200, Loss: 0.7767337313493092\n",
      "Test Loss after Epoch 200: 0.9921427168846131\n",
      "4500\n",
      "Epoch 201, Loss: 0.7733585426807403\n",
      "Test Loss after Epoch 201: 1.0248285999298097\n",
      "4500\n",
      "Epoch 202, Loss: 0.7727290541330973\n",
      "Test Loss after Epoch 202: 0.9903056578636169\n",
      "4500\n",
      "Epoch 203, Loss: 0.7728636026117537\n",
      "Test Loss after Epoch 203: 1.052141580104828\n",
      "4500\n",
      "Epoch 204, Loss: 0.7696978517638312\n",
      "Test Loss after Epoch 204: 1.0647666835784912\n",
      "4500\n",
      "Epoch 205, Loss: 0.7714555375576019\n",
      "Test Loss after Epoch 205: 1.0805839114189149\n",
      "4500\n",
      "Epoch 206, Loss: 0.7713531169626447\n",
      "Test Loss after Epoch 206: 1.0483002977371216\n",
      "4500\n",
      "Epoch 207, Loss: 0.7692864038679335\n",
      "Test Loss after Epoch 207: 1.0071041474342346\n",
      "4500\n",
      "Epoch 208, Loss: 0.7716013205581241\n",
      "Test Loss after Epoch 208: 1.044554479598999\n",
      "4500\n",
      "Epoch 209, Loss: 0.7697603083981408\n",
      "Test Loss after Epoch 209: 0.9619893689155579\n",
      "4500\n",
      "Epoch 210, Loss: 0.7690605642530653\n",
      "Test Loss after Epoch 210: 0.9897126507759094\n",
      "4500\n",
      "Epoch 211, Loss: 0.7711403476927016\n",
      "Test Loss after Epoch 211: 0.9709569797515869\n",
      "4500\n",
      "Epoch 212, Loss: 0.7706328267786238\n",
      "Test Loss after Epoch 212: 0.9522892518043518\n",
      "4500\n",
      "Epoch 213, Loss: 0.7667060791121589\n",
      "Test Loss after Epoch 213: 0.9960919873714447\n",
      "4500\n",
      "Epoch 214, Loss: 0.7689564652972751\n",
      "Test Loss after Epoch 214: 0.9996601552963257\n",
      "4500\n",
      "Epoch 215, Loss: 0.7706167254447936\n",
      "Test Loss after Epoch 215: 1.0030901203155518\n",
      "4500\n",
      "Epoch 216, Loss: 0.7666689984003703\n",
      "Test Loss after Epoch 216: 0.9847447245121003\n",
      "4500\n",
      "Epoch 217, Loss: 0.7689416076342265\n",
      "Test Loss after Epoch 217: 0.9798659679889679\n",
      "4500\n",
      "Epoch 218, Loss: 0.7649708681636387\n",
      "Test Loss after Epoch 218: 0.9896029987335205\n",
      "4500\n",
      "Epoch 219, Loss: 0.768618063264423\n",
      "Test Loss after Epoch 219: 0.9502163727283478\n",
      "4500\n",
      "Epoch 220, Loss: 0.7682958211104075\n",
      "Test Loss after Epoch 220: 0.9907245111465454\n",
      "4500\n",
      "Epoch 221, Loss: 0.7676160614755418\n",
      "Test Loss after Epoch 221: 0.9789489226341248\n",
      "4500\n",
      "Epoch 222, Loss: 0.7654014844894409\n",
      "Test Loss after Epoch 222: 1.0112388257980347\n",
      "4500\n",
      "Epoch 223, Loss: 0.7681858300103082\n",
      "Test Loss after Epoch 223: 0.9422788548469544\n",
      "4500\n",
      "Epoch 224, Loss: 0.7658559647666083\n",
      "Test Loss after Epoch 224: 1.0444647431373597\n",
      "4500\n",
      "Epoch 225, Loss: 0.7651551866796281\n",
      "Test Loss after Epoch 225: 1.0193571276664735\n",
      "4500\n",
      "Epoch 226, Loss: 0.7644144552813636\n",
      "Test Loss after Epoch 226: 1.0417382655143739\n",
      "4500\n",
      "Epoch 227, Loss: 0.7666671668688456\n",
      "Test Loss after Epoch 227: 0.9768853375911712\n",
      "4500\n",
      "Epoch 228, Loss: 0.7655939809481302\n",
      "Test Loss after Epoch 228: 0.9459707076549531\n",
      "4500\n",
      "Epoch 229, Loss: 0.7641552329063416\n",
      "Test Loss after Epoch 229: 0.9611230762004852\n",
      "4500\n",
      "Epoch 230, Loss: 0.7649945517645942\n",
      "Test Loss after Epoch 230: 0.9701216864585877\n",
      "4500\n",
      "Epoch 231, Loss: 0.7667510153982374\n",
      "Test Loss after Epoch 231: 0.9585521559715271\n",
      "4500\n",
      "Epoch 232, Loss: 0.7646308274269104\n",
      "Test Loss after Epoch 232: 0.9926776907444\n",
      "4500\n",
      "Epoch 233, Loss: 0.7637298976315392\n",
      "Test Loss after Epoch 233: 0.9825218114852905\n",
      "4500\n",
      "Epoch 234, Loss: 0.7615089053048028\n",
      "Test Loss after Epoch 234: 1.0066662492752074\n",
      "4500\n",
      "Epoch 235, Loss: 0.7602757039070129\n",
      "Test Loss after Epoch 235: 0.9567432451248169\n",
      "4500\n",
      "Epoch 236, Loss: 0.762549851735433\n",
      "Test Loss after Epoch 236: 0.9777036995887757\n",
      "4500\n",
      "Epoch 237, Loss: 0.7620393667221069\n",
      "Test Loss after Epoch 237: 0.967320148229599\n",
      "4500\n",
      "Epoch 238, Loss: 0.7666622627576192\n",
      "Test Loss after Epoch 238: 0.9679232213497162\n",
      "4500\n",
      "Epoch 239, Loss: 0.7634733216497633\n",
      "Test Loss after Epoch 239: 0.9649671452045441\n",
      "4500\n",
      "Epoch 240, Loss: 0.7615980885823568\n",
      "Test Loss after Epoch 240: 0.9598279554843903\n",
      "4500\n",
      "Epoch 241, Loss: 0.7658460715346866\n",
      "Test Loss after Epoch 241: 0.999294102191925\n",
      "4500\n",
      "Epoch 242, Loss: 0.764285525004069\n",
      "Test Loss after Epoch 242: 0.9689742980003357\n",
      "4500\n",
      "Epoch 243, Loss: 0.7618751855161455\n",
      "Test Loss after Epoch 243: 0.9701479599475861\n",
      "4500\n",
      "Epoch 244, Loss: 0.7635828828016917\n",
      "Test Loss after Epoch 244: 0.938399751663208\n",
      "4500\n",
      "Epoch 245, Loss: 0.7628799044556088\n",
      "Test Loss after Epoch 245: 0.9804093904495239\n",
      "4500\n",
      "Epoch 246, Loss: 0.7616089812119802\n",
      "Test Loss after Epoch 246: 0.9542972612380981\n",
      "4500\n",
      "Epoch 247, Loss: 0.7618465122911665\n",
      "Test Loss after Epoch 247: 0.9357007155418396\n",
      "4500\n",
      "Epoch 248, Loss: 0.7621779454019335\n",
      "Test Loss after Epoch 248: 0.9544071595668793\n",
      "4500\n",
      "Epoch 249, Loss: 0.7630684376292759\n",
      "Test Loss after Epoch 249: 0.9339623923301696\n",
      "4500\n",
      "Epoch 250, Loss: 0.7622028165658316\n",
      "Test Loss after Epoch 250: 0.9620038046836853\n",
      "4500\n",
      "Epoch 251, Loss: 0.7625979506174724\n",
      "Test Loss after Epoch 251: 0.9662921183109283\n",
      "4500\n",
      "Epoch 252, Loss: 0.7604526268111335\n",
      "Test Loss after Epoch 252: 0.9736700704097748\n",
      "4500\n",
      "Epoch 253, Loss: 0.7624140112664964\n",
      "Test Loss after Epoch 253: 0.9735531454086304\n",
      "4500\n",
      "Epoch 254, Loss: 0.7609463601907094\n",
      "Test Loss after Epoch 254: 0.9788902440071106\n",
      "4500\n",
      "Epoch 255, Loss: 0.7593245284027523\n",
      "Test Loss after Epoch 255: 0.9690800104141235\n",
      "4500\n",
      "Epoch 256, Loss: 0.7600878500143687\n",
      "Test Loss after Epoch 256: 0.9557350897789001\n",
      "4500\n",
      "Epoch 257, Loss: 0.7605974895954132\n",
      "Test Loss after Epoch 257: 0.9537406706809998\n",
      "4500\n",
      "Epoch 258, Loss: 0.7600026567247179\n",
      "Test Loss after Epoch 258: 0.9410485956668854\n",
      "4500\n",
      "Epoch 259, Loss: 0.7598983919090695\n",
      "Test Loss after Epoch 259: 0.9360128259658813\n",
      "4500\n",
      "Epoch 260, Loss: 0.7620374116632673\n",
      "Test Loss after Epoch 260: 0.9463466854095459\n",
      "4500\n",
      "Epoch 261, Loss: 0.7607777656449212\n",
      "Test Loss after Epoch 261: 0.8905306639671325\n",
      "4500\n",
      "Epoch 262, Loss: 0.7597620230515798\n",
      "Test Loss after Epoch 262: 0.9092791876792907\n",
      "4500\n",
      "Epoch 263, Loss: 0.7609751805464426\n",
      "Test Loss after Epoch 263: 0.9273634319305419\n",
      "4500\n",
      "Epoch 264, Loss: 0.7610802996688419\n",
      "Test Loss after Epoch 264: 0.9251690576076508\n",
      "4500\n",
      "Epoch 265, Loss: 0.7601556622717116\n",
      "Test Loss after Epoch 265: 0.9210514976978302\n",
      "4500\n",
      "Epoch 266, Loss: 0.761829060210122\n",
      "Test Loss after Epoch 266: 0.9588812835216523\n",
      "4500\n",
      "Epoch 267, Loss: 0.7596233193874359\n",
      "Test Loss after Epoch 267: 0.9716539685726165\n",
      "4500\n",
      "Epoch 268, Loss: 0.7626823694176144\n",
      "Test Loss after Epoch 268: 0.9405036134719849\n",
      "4500\n",
      "Epoch 269, Loss: 0.7590852375825247\n",
      "Test Loss after Epoch 269: 0.9421693584918975\n",
      "4500\n",
      "Epoch 270, Loss: 0.7609963670041826\n",
      "Test Loss after Epoch 270: 0.9508110504150391\n",
      "4500\n",
      "Epoch 271, Loss: 0.7610184433990055\n",
      "Test Loss after Epoch 271: 0.9779354076385498\n",
      "4500\n",
      "Epoch 272, Loss: 0.7599492655065324\n",
      "Test Loss after Epoch 272: 0.9758035528659821\n",
      "4500\n",
      "Epoch 273, Loss: 0.7599180395338271\n",
      "Test Loss after Epoch 273: 0.953422575712204\n",
      "4500\n",
      "Epoch 274, Loss: 0.7601720152960884\n",
      "Test Loss after Epoch 274: 0.9542804636955261\n",
      "4500\n",
      "Epoch 275, Loss: 0.7601799886491564\n",
      "Test Loss after Epoch 275: 0.9313280580043792\n",
      "4500\n",
      "Epoch 276, Loss: 0.7614478257762062\n",
      "Test Loss after Epoch 276: 0.9784193503856659\n",
      "4500\n",
      "Epoch 277, Loss: 0.760053945991728\n",
      "Test Loss after Epoch 277: 0.9198903546333314\n",
      "4500\n",
      "Epoch 278, Loss: 0.7605509812831879\n",
      "Test Loss after Epoch 278: 0.954880901813507\n",
      "4500\n",
      "Epoch 279, Loss: 0.758459135611852\n",
      "Test Loss after Epoch 279: 0.9227392928600311\n",
      "4500\n",
      "Epoch 280, Loss: 0.7600677791701422\n",
      "Test Loss after Epoch 280: 0.9103704988956451\n",
      "4500\n",
      "Epoch 281, Loss: 0.7584720606538985\n",
      "Test Loss after Epoch 281: 0.9378084945678711\n",
      "4500\n",
      "Epoch 282, Loss: 0.758436064667172\n",
      "Test Loss after Epoch 282: 0.9221306920051575\n",
      "4500\n",
      "Epoch 283, Loss: 0.7594207866986593\n",
      "Test Loss after Epoch 283: 0.9746487495899201\n",
      "4500\n",
      "Epoch 284, Loss: 0.7571662523216671\n",
      "Test Loss after Epoch 284: 0.9142375085353851\n",
      "4500\n",
      "Epoch 285, Loss: 0.762044915093316\n",
      "Test Loss after Epoch 285: 0.9303863005638122\n",
      "4500\n",
      "Epoch 286, Loss: 0.7596666001213922\n",
      "Test Loss after Epoch 286: 0.963060400724411\n",
      "4500\n",
      "Epoch 287, Loss: 0.7585517730712891\n",
      "Test Loss after Epoch 287: 0.9271460189819336\n",
      "4500\n",
      "Epoch 288, Loss: 0.760488229142295\n",
      "Test Loss after Epoch 288: 0.9267462201118469\n",
      "4500\n",
      "Epoch 289, Loss: 0.7585627733866374\n",
      "Test Loss after Epoch 289: 0.9508804695606232\n",
      "Rolling back to best model from epoch 261\n",
      "Best test loss: 0.8905306639671325\n",
      "4500\n",
      "Epoch 290, Loss: 0.760615460925632\n",
      "Test Loss after Epoch 290: 0.9227995848655701\n",
      "4500\n",
      "Epoch 291, Loss: 0.7576544240315756\n",
      "Test Loss after Epoch 291: 0.9291858477592468\n",
      "4500\n",
      "Epoch 292, Loss: 0.7573295180267758\n",
      "Test Loss after Epoch 292: 0.9136149208545685\n",
      "4500\n",
      "Epoch 293, Loss: 0.7548634534147051\n",
      "Test Loss after Epoch 293: 0.9194675884246826\n",
      "4500\n",
      "Epoch 294, Loss: 0.7551597082614898\n",
      "Test Loss after Epoch 294: 0.9208009247779846\n",
      "4500\n",
      "Epoch 295, Loss: 0.7583465323713091\n",
      "Test Loss after Epoch 295: 0.9615695316791535\n",
      "4500\n",
      "Epoch 296, Loss: 0.7574873743057251\n",
      "Test Loss after Epoch 296: 0.9429885613918304\n",
      "4500\n",
      "Epoch 297, Loss: 0.7587313880655501\n",
      "Test Loss after Epoch 297: 0.9348755505084991\n",
      "4500\n",
      "Epoch 298, Loss: 0.7584269973172082\n",
      "Test Loss after Epoch 298: 0.9668173429965973\n",
      "4500\n",
      "Epoch 299, Loss: 0.7544656614197625\n",
      "Test Loss after Epoch 299: 0.9453487775325775\n",
      "4500\n",
      "Epoch 300, Loss: 0.7585865319040086\n",
      "Test Loss after Epoch 300: 0.9154349715709686\n",
      "4500\n",
      "Epoch 301, Loss: 0.7592099452018738\n",
      "Test Loss after Epoch 301: 0.9634812200069427\n",
      "4500\n",
      "Epoch 302, Loss: 0.7606223816341824\n",
      "Test Loss after Epoch 302: 0.9044081661701202\n",
      "4500\n",
      "Epoch 303, Loss: 0.7611536320315467\n",
      "Test Loss after Epoch 303: 0.928963086605072\n",
      "4500\n",
      "Epoch 304, Loss: 0.7593751866022745\n",
      "Test Loss after Epoch 304: 0.9467138385772705\n",
      "4500\n",
      "Epoch 305, Loss: 0.7567854821417067\n",
      "Test Loss after Epoch 305: 0.914921565771103\n",
      "4500\n",
      "Epoch 306, Loss: 0.7593980116049449\n",
      "Test Loss after Epoch 306: 0.9222061324119568\n",
      "4500\n",
      "Epoch 307, Loss: 0.7620345630910661\n",
      "Test Loss after Epoch 307: 0.9249367113113404\n",
      "4500\n",
      "Epoch 308, Loss: 0.7617990019586351\n",
      "Test Loss after Epoch 308: 0.9116021571159363\n",
      "4500\n",
      "Epoch 309, Loss: 0.7605808676083883\n",
      "Test Loss after Epoch 309: 0.9100622849464417\n",
      "4500\n",
      "Epoch 310, Loss: 0.7618059197531806\n",
      "Test Loss after Epoch 310: 0.908106317281723\n",
      "4500\n",
      "Epoch 311, Loss: 0.761617175022761\n",
      "Test Loss after Epoch 311: 0.9223192772865295\n",
      "4500\n",
      "Epoch 312, Loss: 0.7603206449084812\n",
      "Test Loss after Epoch 312: 0.9348748483657837\n",
      "4500\n",
      "Epoch 313, Loss: 0.7584360713164011\n",
      "Test Loss after Epoch 313: 0.9515823931694031\n",
      "4500\n",
      "Epoch 314, Loss: 0.7596300877465142\n",
      "Test Loss after Epoch 314: 0.9479077582359314\n",
      "4500\n",
      "Epoch 315, Loss: 0.7627345680660672\n",
      "Test Loss after Epoch 315: 1.004290205001831\n",
      "4500\n",
      "Epoch 316, Loss: 0.7602224662833743\n",
      "Test Loss after Epoch 316: 0.9430831570625305\n",
      "4500\n",
      "Epoch 317, Loss: 0.7624401933617062\n",
      "Test Loss after Epoch 317: 0.9389612002372741\n",
      "4500\n",
      "Epoch 318, Loss: 0.7574535196092393\n",
      "Test Loss after Epoch 318: 0.9456405282020569\n",
      "4500\n",
      "Epoch 319, Loss: 0.7623112383683522\n",
      "Test Loss after Epoch 319: 0.9494482684135437\n",
      "4500\n",
      "Epoch 320, Loss: 0.7631109130382537\n",
      "Test Loss after Epoch 320: 0.957714955329895\n",
      "4500\n",
      "Epoch 321, Loss: 0.762099488125907\n",
      "Test Loss after Epoch 321: 0.95482537484169\n",
      "4500\n",
      "Epoch 322, Loss: 0.7583743286662632\n",
      "Test Loss after Epoch 322: 0.9408251969814301\n",
      "4500\n",
      "Epoch 323, Loss: 0.7573415307733747\n",
      "Test Loss after Epoch 323: 0.9559639976024628\n",
      "4500\n",
      "Epoch 324, Loss: 0.7619442699750264\n",
      "Test Loss after Epoch 324: 0.9587266552448273\n",
      "4500\n",
      "Epoch 325, Loss: 0.7618285656240251\n",
      "Test Loss after Epoch 325: 0.9752067124843598\n",
      "4500\n",
      "Epoch 326, Loss: 0.7637728119956122\n",
      "Test Loss after Epoch 326: 0.953778843164444\n",
      "4500\n",
      "Epoch 327, Loss: 0.7595684700542026\n",
      "Test Loss after Epoch 327: 0.9522068445682526\n",
      "4500\n",
      "Epoch 328, Loss: 0.7634059154457516\n",
      "Test Loss after Epoch 328: 0.9317188813686371\n",
      "Rolling back to best model from epoch 261\n",
      "Best test loss: 0.8905306639671325\n",
      "4500\n",
      "Epoch 329, Loss: 0.7588408589892918\n",
      "Test Loss after Epoch 329: 0.9341200380325317\n",
      "4500\n",
      "Epoch 330, Loss: 0.7578641762998369\n",
      "Test Loss after Epoch 330: 0.9330876154899597\n",
      "4500\n",
      "Epoch 331, Loss: 0.7561233275996314\n",
      "Test Loss after Epoch 331: 0.9271414847373962\n",
      "4500\n",
      "Epoch 332, Loss: 0.7595786335203383\n",
      "Test Loss after Epoch 332: 0.9455148615837097\n",
      "4500\n",
      "Epoch 333, Loss: 0.7578140610324012\n",
      "Test Loss after Epoch 333: 0.9498534450531005\n",
      "4500\n",
      "Epoch 334, Loss: 0.7600986080964406\n",
      "Test Loss after Epoch 334: 0.9498915615081787\n",
      "4500\n",
      "Epoch 335, Loss: 0.7619202916887071\n",
      "Test Loss after Epoch 335: 0.9268469181060791\n",
      "6750\n",
      "Epoch 336, Loss: 0.899201795525021\n",
      "Test Loss after Epoch 336: 0.9058546059131622\n",
      "6750\n",
      "Epoch 337, Loss: 0.8568547811861391\n",
      "Test Loss after Epoch 337: 0.9072925429344177\n",
      "6750\n",
      "Epoch 338, Loss: 0.8427162306043837\n",
      "Test Loss after Epoch 338: 0.9553170673847199\n",
      "6750\n",
      "Epoch 339, Loss: 0.836124777423011\n",
      "Test Loss after Epoch 339: 0.8905242676734925\n",
      "6750\n",
      "Epoch 340, Loss: 0.8335620494242068\n",
      "Test Loss after Epoch 340: 0.906968365907669\n",
      "6750\n",
      "Epoch 341, Loss: 0.8272290010275665\n",
      "Test Loss after Epoch 341: 0.9326703882217408\n",
      "6750\n",
      "Epoch 342, Loss: 0.8196462038711265\n",
      "Test Loss after Epoch 342: 0.909020700454712\n",
      "6750\n",
      "Epoch 343, Loss: 0.8138182126857616\n",
      "Test Loss after Epoch 343: 0.9029512403011322\n",
      "6750\n",
      "Epoch 344, Loss: 0.8082739426118356\n",
      "Test Loss after Epoch 344: 0.9038068993091584\n",
      "6750\n",
      "Epoch 345, Loss: 0.8079116089079115\n",
      "Test Loss after Epoch 345: 0.9262855038642883\n",
      "6750\n",
      "Epoch 346, Loss: 0.808040964267872\n",
      "Test Loss after Epoch 346: 0.9139795022010804\n",
      "6750\n",
      "Epoch 347, Loss: 0.8077326896278947\n",
      "Test Loss after Epoch 347: 0.9256669955253601\n",
      "6750\n",
      "Epoch 348, Loss: 0.8041866841139617\n",
      "Test Loss after Epoch 348: 0.9483474049568176\n",
      "6750\n",
      "Epoch 349, Loss: 0.7979652542714719\n",
      "Test Loss after Epoch 349: 0.9461736052036286\n",
      "6750\n",
      "Epoch 350, Loss: 0.7952444899170487\n",
      "Test Loss after Epoch 350: 0.9326168591976166\n",
      "6750\n",
      "Epoch 351, Loss: 0.7984769065645005\n",
      "Test Loss after Epoch 351: 0.9515457766056061\n",
      "6750\n",
      "Epoch 352, Loss: 0.7970379843711853\n",
      "Test Loss after Epoch 352: 0.9324279134273529\n",
      "6750\n",
      "Epoch 353, Loss: 0.7965564932116755\n",
      "Test Loss after Epoch 353: 0.9333298943042755\n",
      "6750\n",
      "Epoch 354, Loss: 0.7920408052338495\n",
      "Test Loss after Epoch 354: 0.9297148060798645\n",
      "6750\n",
      "Epoch 355, Loss: 0.7923988110577619\n",
      "Test Loss after Epoch 355: 0.8724992928504944\n",
      "6750\n",
      "Epoch 356, Loss: 0.7884155624177721\n",
      "Test Loss after Epoch 356: 0.8796955997943878\n",
      "6750\n",
      "Epoch 357, Loss: 0.7908837774064805\n",
      "Test Loss after Epoch 357: 0.874844054222107\n",
      "6750\n",
      "Epoch 358, Loss: 0.7872912756248757\n",
      "Test Loss after Epoch 358: 0.8915846991539002\n",
      "6750\n",
      "Epoch 359, Loss: 0.7880783870485094\n",
      "Test Loss after Epoch 359: 0.8968532066345215\n",
      "6750\n",
      "Epoch 360, Loss: 0.7866603822354917\n",
      "Test Loss after Epoch 360: 0.8915897057056427\n",
      "6750\n",
      "Epoch 361, Loss: 0.7851996327152959\n",
      "Test Loss after Epoch 361: 0.9003289208412171\n",
      "6750\n",
      "Epoch 362, Loss: 0.7848446185323927\n",
      "Test Loss after Epoch 362: 0.9016471333503723\n",
      "6750\n",
      "Epoch 363, Loss: 0.780583095656501\n",
      "Test Loss after Epoch 363: 0.9105578036308288\n",
      "6750\n",
      "Epoch 364, Loss: 0.7793911640025951\n",
      "Test Loss after Epoch 364: 0.8930632700920105\n",
      "6750\n",
      "Epoch 365, Loss: 0.782953356301343\n",
      "Test Loss after Epoch 365: 0.8827797803878784\n",
      "6750\n",
      "Epoch 366, Loss: 0.7787305353305958\n",
      "Test Loss after Epoch 366: 0.9322282383441926\n",
      "6750\n",
      "Epoch 367, Loss: 0.7833801463091815\n",
      "Test Loss after Epoch 367: 0.9307817988395691\n",
      "6750\n",
      "Epoch 368, Loss: 0.7809645509543243\n",
      "Test Loss after Epoch 368: 0.894390929222107\n",
      "6750\n",
      "Epoch 369, Loss: 0.7807551126126889\n",
      "Test Loss after Epoch 369: 0.8965079040527344\n",
      "6750\n",
      "Epoch 370, Loss: 0.7797114985783895\n",
      "Test Loss after Epoch 370: 0.9003509159088134\n",
      "6750\n",
      "Epoch 371, Loss: 0.7810697085062662\n",
      "Test Loss after Epoch 371: 0.9039782326221466\n",
      "6750\n",
      "Epoch 372, Loss: 0.7762233190536499\n",
      "Test Loss after Epoch 372: 0.8931796424388886\n",
      "6750\n",
      "Epoch 373, Loss: 0.7796171586601822\n",
      "Test Loss after Epoch 373: 0.955063692331314\n",
      "6750\n",
      "Epoch 374, Loss: 0.7763680332148517\n",
      "Test Loss after Epoch 374: 0.9054349219799042\n",
      "6750\n",
      "Epoch 375, Loss: 0.7786307456051862\n",
      "Test Loss after Epoch 375: 0.8812058396339416\n",
      "6750\n",
      "Epoch 376, Loss: 0.7764378755180924\n",
      "Test Loss after Epoch 376: 0.8770452828407288\n",
      "6750\n",
      "Epoch 377, Loss: 0.7747874903149075\n",
      "Test Loss after Epoch 377: 0.8576043231487274\n",
      "6750\n",
      "Epoch 378, Loss: 0.7732881965637207\n",
      "Test Loss after Epoch 378: 0.8648490917682647\n",
      "6750\n",
      "Epoch 379, Loss: 0.7718106436023006\n",
      "Test Loss after Epoch 379: 0.8739875717163086\n",
      "6750\n",
      "Epoch 380, Loss: 0.7739067539638943\n",
      "Test Loss after Epoch 380: 0.8962019679546356\n",
      "6750\n",
      "Epoch 381, Loss: 0.7725186771816678\n",
      "Test Loss after Epoch 381: 0.8999540400505066\n",
      "6750\n",
      "Epoch 382, Loss: 0.7712720369409631\n",
      "Test Loss after Epoch 382: 0.862768946647644\n",
      "6750\n",
      "Epoch 383, Loss: 0.7718634213694819\n",
      "Test Loss after Epoch 383: 0.8946663918495178\n",
      "6750\n",
      "Epoch 384, Loss: 0.7707363605499268\n",
      "Test Loss after Epoch 384: 0.8641358389854431\n",
      "6750\n",
      "Epoch 385, Loss: 0.7709125755451344\n",
      "Test Loss after Epoch 385: 0.8655595598220825\n",
      "6750\n",
      "Epoch 386, Loss: 0.7722460497220357\n",
      "Test Loss after Epoch 386: 0.9844390940666199\n",
      "6750\n",
      "Epoch 387, Loss: 0.7704113788781343\n",
      "Test Loss after Epoch 387: 0.8697072720527649\n",
      "6750\n",
      "Epoch 388, Loss: 0.7723084088078251\n",
      "Test Loss after Epoch 388: 0.8596838808059692\n",
      "6750\n",
      "Epoch 389, Loss: 0.7720245442567049\n",
      "Test Loss after Epoch 389: 0.8584214920997619\n",
      "6750\n",
      "Epoch 390, Loss: 0.7676185432716652\n",
      "Test Loss after Epoch 390: 0.8615102114677429\n",
      "6750\n",
      "Epoch 391, Loss: 0.7711620651527687\n",
      "Test Loss after Epoch 391: 0.8497799644470215\n",
      "6750\n",
      "Epoch 392, Loss: 0.7689245596108614\n",
      "Test Loss after Epoch 392: 0.8712711620330811\n",
      "6750\n",
      "Epoch 393, Loss: 0.7631415251449303\n",
      "Test Loss after Epoch 393: 0.8559933190345764\n",
      "6750\n",
      "Epoch 394, Loss: 0.766937013414171\n",
      "Test Loss after Epoch 394: 0.8695099523067474\n",
      "6750\n",
      "Epoch 395, Loss: 0.7649300260543823\n",
      "Test Loss after Epoch 395: 0.8704601907730103\n",
      "6750\n",
      "Epoch 396, Loss: 0.7671518756018745\n",
      "Test Loss after Epoch 396: 0.8796360623836518\n",
      "6750\n",
      "Epoch 397, Loss: 0.7648322401046753\n",
      "Test Loss after Epoch 397: 0.8606664967536927\n",
      "6750\n",
      "Epoch 398, Loss: 0.7635407665570577\n",
      "Test Loss after Epoch 398: 0.8801985995769501\n",
      "6750\n",
      "Epoch 399, Loss: 0.7650087187378495\n",
      "Test Loss after Epoch 399: 0.8638964893817902\n",
      "6750\n",
      "Epoch 400, Loss: 0.7621076433746903\n",
      "Test Loss after Epoch 400: 0.8593999900817871\n",
      "6750\n",
      "Epoch 401, Loss: 0.7666954209186413\n",
      "Test Loss after Epoch 401: 0.8876688749790191\n",
      "6750\n",
      "Epoch 402, Loss: 0.7646308445753874\n",
      "Test Loss after Epoch 402: 0.8590145478248596\n",
      "6750\n",
      "Epoch 403, Loss: 0.7637332513773882\n",
      "Test Loss after Epoch 403: 0.8485486872196197\n",
      "6750\n",
      "Epoch 404, Loss: 0.7652011210476911\n",
      "Test Loss after Epoch 404: 0.8792620992660523\n",
      "6750\n",
      "Epoch 405, Loss: 0.7655338516235352\n",
      "Test Loss after Epoch 405: 0.8536186146736146\n",
      "6750\n",
      "Epoch 406, Loss: 0.7649360583976463\n",
      "Test Loss after Epoch 406: 0.8740132665634155\n",
      "6750\n",
      "Epoch 407, Loss: 0.7642007912000021\n",
      "Test Loss after Epoch 407: 0.8496646564006806\n",
      "6750\n",
      "Epoch 408, Loss: 0.7620527852552909\n",
      "Test Loss after Epoch 408: 0.8627701263427734\n",
      "6750\n",
      "Epoch 409, Loss: 0.76078219071141\n",
      "Test Loss after Epoch 409: 0.8702625072002411\n",
      "6750\n",
      "Epoch 410, Loss: 0.7624191593594021\n",
      "Test Loss after Epoch 410: 0.8568237590789795\n",
      "6750\n",
      "Epoch 411, Loss: 0.7608406392026831\n",
      "Test Loss after Epoch 411: 0.8597855679988861\n",
      "6750\n",
      "Epoch 412, Loss: 0.7585335651856882\n",
      "Test Loss after Epoch 412: 0.8634375820159912\n",
      "6750\n",
      "Epoch 413, Loss: 0.7598832694512826\n",
      "Test Loss after Epoch 413: 0.8825263364315032\n",
      "6750\n",
      "Epoch 414, Loss: 0.761611036989424\n",
      "Test Loss after Epoch 414: 0.8533442311286926\n",
      "6750\n",
      "Epoch 415, Loss: 0.7589089321383724\n",
      "Test Loss after Epoch 415: 0.90575372838974\n",
      "6750\n",
      "Epoch 416, Loss: 0.7615027865303887\n",
      "Test Loss after Epoch 416: 0.8762971317768097\n",
      "6750\n",
      "Epoch 417, Loss: 0.7592326648147018\n",
      "Test Loss after Epoch 417: 0.8538629930019379\n",
      "6750\n",
      "Epoch 418, Loss: 0.7597539931756478\n",
      "Test Loss after Epoch 418: 0.8822746505737304\n",
      "6750\n",
      "Epoch 419, Loss: 0.7566021265630369\n",
      "Test Loss after Epoch 419: 0.8609793794155121\n",
      "6750\n",
      "Epoch 420, Loss: 0.7571595109127186\n",
      "Test Loss after Epoch 420: 0.9061787922382355\n",
      "6750\n",
      "Epoch 421, Loss: 0.7564176726517854\n",
      "Test Loss after Epoch 421: 0.8545036528110505\n",
      "6750\n",
      "Epoch 422, Loss: 0.7557986503000612\n",
      "Test Loss after Epoch 422: 0.8610282907485962\n",
      "6750\n",
      "Epoch 423, Loss: 0.7560498443356267\n",
      "Test Loss after Epoch 423: 0.8953625276088715\n",
      "6750\n",
      "Epoch 424, Loss: 0.7574574307865567\n",
      "Test Loss after Epoch 424: 0.885839528799057\n",
      "6750\n",
      "Epoch 425, Loss: 0.7580796000869185\n",
      "Test Loss after Epoch 425: 0.874479572057724\n",
      "6750\n",
      "Epoch 426, Loss: 0.756123430358039\n",
      "Test Loss after Epoch 426: 0.8643844003677368\n",
      "6750\n",
      "Epoch 427, Loss: 0.7572656907328853\n",
      "Test Loss after Epoch 427: 0.8679537374973297\n",
      "6750\n",
      "Epoch 428, Loss: 0.7551555627187093\n",
      "Test Loss after Epoch 428: 0.8601868858337403\n",
      "6750\n",
      "Epoch 429, Loss: 0.7560555771015308\n",
      "Test Loss after Epoch 429: 0.8786828534603119\n",
      "6750\n",
      "Epoch 430, Loss: 0.7534773576347916\n",
      "Test Loss after Epoch 430: 0.8629567422866822\n",
      "6750\n",
      "Epoch 431, Loss: 0.7556001286330046\n",
      "Test Loss after Epoch 431: 0.874334489107132\n",
      "6750\n",
      "Epoch 432, Loss: 0.7538293288902\n",
      "Test Loss after Epoch 432: 0.8596505315303803\n",
      "6750\n",
      "Epoch 433, Loss: 0.753823828626562\n",
      "Test Loss after Epoch 433: 0.859741913318634\n",
      "6750\n",
      "Epoch 434, Loss: 0.752494794121495\n",
      "Test Loss after Epoch 434: 0.8554752042293549\n",
      "6750\n",
      "Epoch 435, Loss: 0.753997940769902\n",
      "Test Loss after Epoch 435: 0.8577397680282592\n",
      "6750\n",
      "Epoch 436, Loss: 0.7573847873652423\n",
      "Test Loss after Epoch 436: 0.8912752695083618\n",
      "6750\n",
      "Epoch 437, Loss: 0.7551817728678385\n",
      "Test Loss after Epoch 437: 0.8754312198162079\n",
      "6750\n",
      "Epoch 438, Loss: 0.7553610605310511\n",
      "Test Loss after Epoch 438: 0.8736589827537536\n",
      "6750\n",
      "Epoch 439, Loss: 0.7545666430967826\n",
      "Test Loss after Epoch 439: 0.8669972586631775\n",
      "6750\n",
      "Epoch 440, Loss: 0.7530201508204142\n",
      "Test Loss after Epoch 440: 0.8597942469120026\n",
      "6750\n",
      "Epoch 441, Loss: 0.7538144403386999\n",
      "Test Loss after Epoch 441: 0.8479832668304443\n",
      "6750\n",
      "Epoch 442, Loss: 0.7531347532802158\n",
      "Test Loss after Epoch 442: 0.8604386012554168\n",
      "6750\n",
      "Epoch 443, Loss: 0.7519081705587881\n",
      "Test Loss after Epoch 443: 0.8909991846084595\n",
      "6750\n",
      "Epoch 444, Loss: 0.7491954001673946\n",
      "Test Loss after Epoch 444: 0.8532319395542145\n",
      "6750\n",
      "Epoch 445, Loss: 0.7496114078451086\n",
      "Test Loss after Epoch 445: 0.8554480798244476\n",
      "6750\n",
      "Epoch 446, Loss: 0.7505681342195581\n",
      "Test Loss after Epoch 446: 0.8585550141334534\n",
      "6750\n",
      "Epoch 447, Loss: 0.7510846429047762\n",
      "Test Loss after Epoch 447: 0.8661550767421723\n",
      "6750\n",
      "Epoch 448, Loss: 0.7519920631867868\n",
      "Test Loss after Epoch 448: 0.8521385102272033\n",
      "6750\n",
      "Epoch 449, Loss: 0.7503532440044262\n",
      "Test Loss after Epoch 449: 0.8629120197296143\n",
      "6750\n",
      "Epoch 450, Loss: 0.7523828151490953\n",
      "Test Loss after Epoch 450: 0.8635723121166229\n",
      "6750\n",
      "Epoch 451, Loss: 0.7520722872945997\n",
      "Test Loss after Epoch 451: 0.8759792449474335\n",
      "6750\n",
      "Epoch 452, Loss: 0.7497356924834074\n",
      "Test Loss after Epoch 452: 0.8801259748935699\n",
      "6750\n",
      "Epoch 453, Loss: 0.7493863385341786\n",
      "Test Loss after Epoch 453: 0.8914609272480011\n",
      "6750\n",
      "Epoch 454, Loss: 0.7489405793790465\n",
      "Test Loss after Epoch 454: 0.8764144053459167\n",
      "6750\n",
      "Epoch 455, Loss: 0.7475784302994057\n",
      "Test Loss after Epoch 455: 0.8810893220901489\n",
      "6750\n",
      "Epoch 456, Loss: 0.7519453579054939\n",
      "Test Loss after Epoch 456: 0.8727743356227875\n",
      "6750\n",
      "Epoch 457, Loss: 0.7518007547767074\n",
      "Test Loss after Epoch 457: 0.8701888720989227\n",
      "6750\n",
      "Epoch 458, Loss: 0.7514815518414533\n",
      "Test Loss after Epoch 458: 0.9005182168483734\n",
      "6750\n",
      "Epoch 459, Loss: 0.7512192102185002\n",
      "Test Loss after Epoch 459: 0.8978672695159912\n",
      "6750\n",
      "Epoch 460, Loss: 0.7510264574510079\n",
      "Test Loss after Epoch 460: 0.8855716433525086\n",
      "6750\n",
      "Epoch 461, Loss: 0.7493697990488123\n",
      "Test Loss after Epoch 461: 0.8743831152915955\n",
      "6750\n",
      "Epoch 462, Loss: 0.7503814376725091\n",
      "Test Loss after Epoch 462: 0.8765090711116791\n",
      "6750\n",
      "Epoch 463, Loss: 0.7467564829367178\n",
      "Test Loss after Epoch 463: 0.8586757519245147\n",
      "6750\n",
      "Epoch 464, Loss: 0.7474225966312267\n",
      "Test Loss after Epoch 464: 0.8621767947673797\n",
      "6750\n",
      "Epoch 465, Loss: 0.7465059621598985\n",
      "Test Loss after Epoch 465: 0.8647852072715759\n",
      "6750\n",
      "Epoch 466, Loss: 0.7499364399026941\n",
      "Test Loss after Epoch 466: 0.8814306497573853\n",
      "6750\n",
      "Epoch 467, Loss: 0.7498695589171516\n",
      "Test Loss after Epoch 467: 0.8617829451560974\n",
      "6750\n",
      "Epoch 468, Loss: 0.7497678475733156\n",
      "Test Loss after Epoch 468: 0.8606941957473755\n",
      "6750\n",
      "Epoch 469, Loss: 0.7490828595338044\n",
      "Test Loss after Epoch 469: 0.8666491320133209\n",
      "6750\n",
      "Epoch 470, Loss: 0.7487830310221072\n",
      "Test Loss after Epoch 470: 0.8742404265403747\n",
      "6750\n",
      "Epoch 471, Loss: 0.749837116294437\n",
      "Test Loss after Epoch 471: 0.8599108703136444\n",
      "6750\n",
      "Epoch 472, Loss: 0.7479348984471074\n",
      "Test Loss after Epoch 472: 0.8756061663627625\n",
      "6750\n",
      "Epoch 473, Loss: 0.7461213120531153\n",
      "Test Loss after Epoch 473: 0.8601364390850067\n",
      "6750\n",
      "Epoch 474, Loss: 0.7493561526934306\n",
      "Test Loss after Epoch 474: 0.8659061942100524\n",
      "6750\n",
      "Epoch 475, Loss: 0.7477266891797384\n",
      "Test Loss after Epoch 475: 0.860457266330719\n",
      "6750\n",
      "Epoch 476, Loss: 0.7458272890338191\n",
      "Test Loss after Epoch 476: 0.8618057434558868\n",
      "6750\n",
      "Epoch 477, Loss: 0.7469999416845816\n",
      "Test Loss after Epoch 477: 0.8536811952590942\n",
      "6750\n",
      "Epoch 478, Loss: 0.7452521538911042\n",
      "Test Loss after Epoch 478: 0.8459012084007264\n",
      "6750\n",
      "Epoch 479, Loss: 0.7449522404847322\n",
      "Test Loss after Epoch 479: 0.8559234972000123\n",
      "6750\n",
      "Epoch 480, Loss: 0.7453260818410803\n",
      "Test Loss after Epoch 480: 0.8420088648796081\n",
      "6750\n",
      "Epoch 481, Loss: 0.7483907262837445\n",
      "Test Loss after Epoch 481: 0.8513989334106445\n",
      "6750\n",
      "Epoch 482, Loss: 0.745833063920339\n",
      "Test Loss after Epoch 482: 0.839399373292923\n",
      "6750\n",
      "Epoch 483, Loss: 0.7480377797903838\n",
      "Test Loss after Epoch 483: 0.8610322585105896\n",
      "6750\n",
      "Epoch 484, Loss: 0.7488106189303928\n",
      "Test Loss after Epoch 484: 0.8399180052280426\n",
      "6750\n",
      "Epoch 485, Loss: 0.7465613874329461\n",
      "Test Loss after Epoch 485: 0.8384476392269135\n",
      "6750\n",
      "Epoch 486, Loss: 0.7478450033399794\n",
      "Test Loss after Epoch 486: 0.8473180341720581\n",
      "6750\n",
      "Epoch 487, Loss: 0.7445057797961765\n",
      "Test Loss after Epoch 487: 0.8533666365146637\n",
      "6750\n",
      "Epoch 488, Loss: 0.7445459015281112\n",
      "Test Loss after Epoch 488: 0.8622803966999054\n",
      "6750\n",
      "Epoch 489, Loss: 0.745471575860624\n",
      "Test Loss after Epoch 489: 0.8455118522644043\n",
      "6750\n",
      "Epoch 490, Loss: 0.7439378765424093\n",
      "Test Loss after Epoch 490: 0.8475383589267731\n",
      "6750\n",
      "Epoch 491, Loss: 0.7450725830572623\n",
      "Test Loss after Epoch 491: 0.855065083026886\n",
      "6750\n",
      "Epoch 492, Loss: 0.7451368998774776\n",
      "Test Loss after Epoch 492: 0.845268814086914\n",
      "6750\n",
      "Epoch 493, Loss: 0.7467559085775305\n",
      "Test Loss after Epoch 493: 0.8469084265232086\n",
      "6750\n",
      "Epoch 494, Loss: 0.7434502822204873\n",
      "Test Loss after Epoch 494: 0.8475939915180206\n",
      "6750\n",
      "Epoch 495, Loss: 0.7436179964454086\n",
      "Test Loss after Epoch 495: 0.8579115505218506\n",
      "6750\n",
      "Epoch 496, Loss: 0.745127746882262\n",
      "Test Loss after Epoch 496: 0.8567414619922638\n",
      "6750\n",
      "Epoch 497, Loss: 0.7436601650096752\n",
      "Test Loss after Epoch 497: 0.855422200679779\n",
      "6750\n",
      "Epoch 498, Loss: 0.7447824722925822\n",
      "Test Loss after Epoch 498: 0.8547769153118133\n",
      "6750\n",
      "Epoch 499, Loss: 0.7429356877892106\n",
      "Test Loss after Epoch 499: 0.8553923728466034\n",
      "6750\n",
      "Epoch 500, Loss: 0.7435010658899943\n",
      "Test Loss after Epoch 500: 0.8661047370433808\n",
      "6750\n",
      "Epoch 501, Loss: 0.744611909142247\n",
      "Test Loss after Epoch 501: 0.8642624249458313\n",
      "6750\n",
      "Epoch 502, Loss: 0.7431130556707028\n",
      "Test Loss after Epoch 502: 0.8599954209327698\n",
      "6750\n",
      "Epoch 503, Loss: 0.7442510520087348\n",
      "Test Loss after Epoch 503: 0.8537801101207733\n",
      "6750\n",
      "Epoch 504, Loss: 0.74316590084853\n",
      "Test Loss after Epoch 504: 0.8700677223205566\n",
      "6750\n",
      "Epoch 505, Loss: 0.7455909679200914\n",
      "Test Loss after Epoch 505: 0.8520492868423462\n",
      "6750\n",
      "Epoch 506, Loss: 0.7443660451041327\n",
      "Test Loss after Epoch 506: 0.843324214220047\n",
      "6750\n",
      "Epoch 507, Loss: 0.7440989587571886\n",
      "Test Loss after Epoch 507: 0.8587950441837311\n",
      "6750\n",
      "Epoch 508, Loss: 0.7438177435486405\n",
      "Test Loss after Epoch 508: 0.8534143815040588\n",
      "6750\n",
      "Epoch 509, Loss: 0.7433989140722487\n",
      "Test Loss after Epoch 509: 0.8602978103160858\n",
      "6750\n",
      "Epoch 510, Loss: 0.7421588795626605\n",
      "Test Loss after Epoch 510: 0.8535444133281708\n",
      "6750\n",
      "Epoch 511, Loss: 0.7441467595806828\n",
      "Test Loss after Epoch 511: 0.855621460199356\n",
      "6750\n",
      "Epoch 512, Loss: 0.743598639594184\n",
      "Test Loss after Epoch 512: 0.859669038772583\n",
      "6750\n",
      "Epoch 513, Loss: 0.7431579659426654\n",
      "Test Loss after Epoch 513: 0.8534091830253601\n",
      "6750\n",
      "Epoch 514, Loss: 0.7439676099883186\n",
      "Test Loss after Epoch 514: 0.8477126142978668\n",
      "6750\n",
      "Epoch 515, Loss: 0.7436310120158726\n",
      "Test Loss after Epoch 515: 0.8508916711807251\n",
      "6750\n",
      "Epoch 516, Loss: 0.7429996212323506\n",
      "Test Loss after Epoch 516: 0.8442128200531006\n",
      "6750\n",
      "Epoch 517, Loss: 0.7423722509984617\n",
      "Test Loss after Epoch 517: 0.840169130563736\n",
      "6750\n",
      "Epoch 518, Loss: 0.7421935896343655\n",
      "Test Loss after Epoch 518: 0.844363095998764\n",
      "6750\n",
      "Epoch 519, Loss: 0.742445587793986\n",
      "Test Loss after Epoch 519: 0.8391314506530761\n",
      "6750\n",
      "Epoch 520, Loss: 0.7417422408704405\n",
      "Test Loss after Epoch 520: 0.8440234725475311\n",
      "6750\n",
      "Epoch 521, Loss: 0.7439920234326963\n",
      "Test Loss after Epoch 521: 0.8519534525871277\n",
      "6750\n",
      "Epoch 522, Loss: 0.7409006156391568\n",
      "Test Loss after Epoch 522: 0.8411967272758484\n",
      "6750\n",
      "Epoch 523, Loss: 0.7405649969489486\n",
      "Test Loss after Epoch 523: 0.84014359998703\n",
      "6750\n",
      "Epoch 524, Loss: 0.7412631830992522\n",
      "Test Loss after Epoch 524: 0.8444647052288056\n",
      "6750\n",
      "Epoch 525, Loss: 0.740558836195204\n",
      "Test Loss after Epoch 525: 0.8472715756893158\n",
      "6750\n",
      "Epoch 526, Loss: 0.7421704588113007\n",
      "Test Loss after Epoch 526: 0.8554998364448547\n",
      "6750\n",
      "Epoch 527, Loss: 0.7421088959905836\n",
      "Test Loss after Epoch 527: 0.8376504929065705\n",
      "6750\n",
      "Epoch 528, Loss: 0.7424852349316632\n",
      "Test Loss after Epoch 528: 0.8390422177314758\n",
      "6750\n",
      "Epoch 529, Loss: 0.7416886437204149\n",
      "Test Loss after Epoch 529: 0.8596579463481903\n",
      "6750\n",
      "Epoch 530, Loss: 0.7423969488143921\n",
      "Test Loss after Epoch 530: 0.8457946050167083\n",
      "6750\n",
      "Epoch 531, Loss: 0.7418796200752258\n",
      "Test Loss after Epoch 531: 0.8428829078674316\n",
      "6750\n",
      "Epoch 532, Loss: 0.7406863991772688\n",
      "Test Loss after Epoch 532: 0.8450154359340668\n",
      "6750\n",
      "Epoch 533, Loss: 0.7410034794277615\n",
      "Test Loss after Epoch 533: 0.8409053115844727\n",
      "6750\n",
      "Epoch 534, Loss: 0.7419838236172994\n",
      "Test Loss after Epoch 534: 0.8385866253376008\n",
      "6750\n",
      "Epoch 535, Loss: 0.7412659529050192\n",
      "Test Loss after Epoch 535: 0.8396993532180786\n",
      "6750\n",
      "Epoch 536, Loss: 0.7414317757641827\n",
      "Test Loss after Epoch 536: 0.8415493309497833\n",
      "6750\n",
      "Epoch 537, Loss: 0.7402924172436749\n",
      "Test Loss after Epoch 537: 0.8377163875102996\n",
      "6750\n",
      "Epoch 538, Loss: 0.7403905609448751\n",
      "Test Loss after Epoch 538: 0.8470779118537903\n",
      "6750\n",
      "Epoch 539, Loss: 0.7421576383731984\n",
      "Test Loss after Epoch 539: 0.8401063370704651\n",
      "6750\n",
      "Epoch 540, Loss: 0.7404133774086281\n",
      "Test Loss after Epoch 540: 0.8331969246864319\n",
      "6750\n",
      "Epoch 541, Loss: 0.7424167264655784\n",
      "Test Loss after Epoch 541: 0.8452992219924926\n",
      "6750\n",
      "Epoch 542, Loss: 0.7417568375446179\n",
      "Test Loss after Epoch 542: 0.8478090944290161\n",
      "6750\n",
      "Epoch 543, Loss: 0.740787683363314\n",
      "Test Loss after Epoch 543: 0.8462428884506226\n",
      "6750\n",
      "Epoch 544, Loss: 0.7402203023698595\n",
      "Test Loss after Epoch 544: 0.8480930840969085\n",
      "6750\n",
      "Epoch 545, Loss: 0.741506077059993\n",
      "Test Loss after Epoch 545: 0.8388165199756622\n",
      "6750\n",
      "Epoch 546, Loss: 0.7408274685188576\n",
      "Test Loss after Epoch 546: 0.8381172840595246\n",
      "6750\n",
      "Epoch 547, Loss: 0.7410055257302743\n",
      "Test Loss after Epoch 547: 0.8431147544384002\n",
      "6750\n",
      "Epoch 548, Loss: 0.7405585390550119\n",
      "Test Loss after Epoch 548: 0.8359202785491944\n",
      "6750\n",
      "Epoch 549, Loss: 0.7406058266604388\n",
      "Test Loss after Epoch 549: 0.8471891713142395\n",
      "6750\n",
      "Epoch 550, Loss: 0.7398365419352496\n",
      "Test Loss after Epoch 550: 0.8434773063659669\n",
      "6750\n",
      "Epoch 551, Loss: 0.7426753953121327\n",
      "Test Loss after Epoch 551: 0.8381883428096771\n",
      "6750\n",
      "Epoch 552, Loss: 0.7405044237419411\n",
      "Test Loss after Epoch 552: 0.8569923913478852\n",
      "6750\n",
      "Epoch 553, Loss: 0.7407778955388952\n",
      "Test Loss after Epoch 553: 0.8412048447132111\n",
      "6750\n",
      "Epoch 554, Loss: 0.7411756905273155\n",
      "Test Loss after Epoch 554: 0.8333053143024445\n",
      "6750\n",
      "Epoch 555, Loss: 0.7403928194929053\n",
      "Test Loss after Epoch 555: 0.8430354461669922\n",
      "6750\n",
      "Epoch 556, Loss: 0.7399284105300903\n",
      "Test Loss after Epoch 556: 0.8462905766963958\n",
      "6750\n",
      "Epoch 557, Loss: 0.7393264670195403\n",
      "Test Loss after Epoch 557: 0.8477940316200256\n",
      "6750\n",
      "Epoch 558, Loss: 0.7392467116073326\n",
      "Test Loss after Epoch 558: 0.8387023227214814\n",
      "6750\n",
      "Epoch 559, Loss: 0.740248460010246\n",
      "Test Loss after Epoch 559: 0.8359389469623566\n",
      "6750\n",
      "Epoch 560, Loss: 0.7392731872841164\n",
      "Test Loss after Epoch 560: 0.8393453307151795\n",
      "6750\n",
      "Epoch 561, Loss: 0.7405139901549728\n",
      "Test Loss after Epoch 561: 0.8398880689144135\n",
      "6750\n",
      "Epoch 562, Loss: 0.7409180560112\n",
      "Test Loss after Epoch 562: 0.8403155794143676\n",
      "6750\n",
      "Epoch 563, Loss: 0.7393903204070197\n",
      "Test Loss after Epoch 563: 0.8412061221599579\n",
      "6750\n",
      "Epoch 564, Loss: 0.7403086399502224\n",
      "Test Loss after Epoch 564: 0.8420325226783753\n",
      "6750\n",
      "Epoch 565, Loss: 0.7402643517211631\n",
      "Test Loss after Epoch 565: 0.8359237833023071\n",
      "6750\n",
      "Epoch 566, Loss: 0.7390326642283687\n",
      "Test Loss after Epoch 566: 0.8418511726856232\n",
      "6750\n",
      "Epoch 567, Loss: 0.7397481026649475\n",
      "Test Loss after Epoch 567: 0.8435746750831604\n",
      "6750\n",
      "Epoch 568, Loss: 0.7411149201922946\n",
      "Test Loss after Epoch 568: 0.8411637742519379\n",
      "6750\n",
      "Epoch 569, Loss: 0.7387168491328204\n",
      "Test Loss after Epoch 569: 0.8448814942836761\n",
      "6750\n",
      "Epoch 570, Loss: 0.738100705853215\n",
      "Test Loss after Epoch 570: 0.8444105653762818\n",
      "6750\n",
      "Epoch 571, Loss: 0.7398750304116143\n",
      "Test Loss after Epoch 571: 0.8632679102420807\n",
      "6750\n",
      "Epoch 572, Loss: 0.7381904591807613\n",
      "Test Loss after Epoch 572: 0.8755529208183288\n",
      "6750\n",
      "Epoch 573, Loss: 0.7395723303159077\n",
      "Test Loss after Epoch 573: 0.8522791390419007\n",
      "6750\n",
      "Epoch 574, Loss: 0.7370459105350353\n",
      "Test Loss after Epoch 574: 0.8547096338272094\n",
      "6750\n",
      "Epoch 575, Loss: 0.7390956034836945\n",
      "Test Loss after Epoch 575: 0.8498080823421478\n",
      "6750\n",
      "Epoch 576, Loss: 0.7398791980036983\n",
      "Test Loss after Epoch 576: 0.8457192914485931\n",
      "6750\n",
      "Epoch 577, Loss: 0.7389322172094275\n",
      "Test Loss after Epoch 577: 0.8487935013771057\n",
      "6750\n",
      "Epoch 578, Loss: 0.7393874131308662\n",
      "Test Loss after Epoch 578: 0.8448288023471833\n",
      "6750\n",
      "Epoch 579, Loss: 0.7387257955869039\n",
      "Test Loss after Epoch 579: 0.8404451003074646\n",
      "6750\n",
      "Epoch 580, Loss: 0.7390443251574481\n",
      "Test Loss after Epoch 580: 0.8485591166019439\n",
      "6750\n",
      "Epoch 581, Loss: 0.7381701212812353\n",
      "Test Loss after Epoch 581: 0.8435931904315949\n",
      "6750\n",
      "Epoch 582, Loss: 0.7377289136427421\n",
      "Test Loss after Epoch 582: 0.8421269166469574\n",
      "6750\n",
      "Epoch 583, Loss: 0.738225068586844\n",
      "Test Loss after Epoch 583: 0.8453662252426147\n",
      "6750\n",
      "Epoch 584, Loss: 0.7393865861716094\n",
      "Test Loss after Epoch 584: 0.8440458862781525\n",
      "6750\n",
      "Epoch 585, Loss: 0.7375887471481606\n",
      "Test Loss after Epoch 585: 0.8413186078071594\n",
      "6750\n",
      "Epoch 586, Loss: 0.7382500564963729\n",
      "Test Loss after Epoch 586: 0.8541515686511993\n",
      "6750\n",
      "Epoch 587, Loss: 0.739163263603493\n",
      "Test Loss after Epoch 587: 0.8560082986354828\n",
      "6750\n",
      "Epoch 588, Loss: 0.7381646101209852\n",
      "Test Loss after Epoch 588: 0.863754677772522\n",
      "6750\n",
      "Epoch 589, Loss: 0.7377505966469093\n",
      "Test Loss after Epoch 589: 0.8561385910511017\n",
      "6750\n",
      "Epoch 590, Loss: 0.7372954345985695\n",
      "Test Loss after Epoch 590: 0.8392773640155792\n",
      "6750\n",
      "Epoch 591, Loss: 0.738533831137198\n",
      "Test Loss after Epoch 591: 0.8404052486419678\n",
      "6750\n",
      "Epoch 592, Loss: 0.7372579050770512\n",
      "Test Loss after Epoch 592: 0.8490402231216431\n",
      "6750\n",
      "Epoch 593, Loss: 0.7370358273541486\n",
      "Test Loss after Epoch 593: 0.8455551555156707\n",
      "6750\n",
      "Epoch 594, Loss: 0.7366977919649195\n",
      "Test Loss after Epoch 594: 0.8402348308563232\n",
      "6750\n",
      "Epoch 595, Loss: 0.7383962655420656\n",
      "Test Loss after Epoch 595: 0.8415657389163971\n",
      "Rolling back to best model from epoch 540\n",
      "Best test loss: 0.8331969246864319\n",
      "6750\n",
      "Epoch 596, Loss: 0.7409964897720902\n",
      "Test Loss after Epoch 596: 0.8441305391788483\n",
      "6750\n",
      "Epoch 597, Loss: 0.7382460181624801\n",
      "Test Loss after Epoch 597: 0.8432524614334106\n",
      "6750\n",
      "Epoch 598, Loss: 0.7379417429500156\n",
      "Test Loss after Epoch 598: 0.8393707313537597\n",
      "6750\n",
      "Epoch 599, Loss: 0.7380096891544483\n",
      "Test Loss after Epoch 599: 0.8446879754066468\n",
      "6750\n",
      "Epoch 600, Loss: 0.7382630746276291\n",
      "Test Loss after Epoch 600: 0.838479501247406\n",
      "6750\n",
      "Epoch 601, Loss: 0.740439333597819\n",
      "Test Loss after Epoch 601: 0.8421533987522125\n",
      "6750\n",
      "Epoch 602, Loss: 0.7386409447811269\n",
      "Test Loss after Epoch 602: 0.8467757303714752\n",
      "6750\n",
      "Epoch 603, Loss: 0.7385868350488168\n",
      "Test Loss after Epoch 603: 0.8457860472202301\n",
      "6750\n",
      "Epoch 604, Loss: 0.7383071697199786\n",
      "Test Loss after Epoch 604: 0.8502789194583893\n",
      "6750\n",
      "Epoch 605, Loss: 0.7373343595045584\n",
      "Test Loss after Epoch 605: 0.8429998450279236\n",
      "6750\n",
      "Epoch 606, Loss: 0.7385816157658894\n",
      "Test Loss after Epoch 606: 0.8370207147598266\n",
      "6750\n",
      "Epoch 607, Loss: 0.7392589705961722\n",
      "Test Loss after Epoch 607: 0.8423202505111694\n",
      "6750\n",
      "Epoch 608, Loss: 0.7377926472734522\n",
      "Test Loss after Epoch 608: 0.8360638909339905\n",
      "6750\n",
      "Epoch 609, Loss: 0.7384012532940617\n",
      "Test Loss after Epoch 609: 0.8399366219043731\n",
      "6750\n",
      "Epoch 610, Loss: 0.7391481900568362\n",
      "Test Loss after Epoch 610: 0.8393841581344604\n",
      "6750\n",
      "Epoch 611, Loss: 0.7380652389173155\n",
      "Test Loss after Epoch 611: 0.837997888803482\n",
      "6750\n",
      "Epoch 612, Loss: 0.739220777352651\n",
      "Test Loss after Epoch 612: 0.8464192633628845\n",
      "6750\n",
      "Epoch 613, Loss: 0.7378677655149389\n",
      "Test Loss after Epoch 613: 0.838713344335556\n",
      "6750\n",
      "Epoch 614, Loss: 0.738068263442428\n",
      "Test Loss after Epoch 614: 0.8362967460155487\n",
      "6750\n",
      "Epoch 615, Loss: 0.7387832612814726\n",
      "Test Loss after Epoch 615: 0.8378075106143952\n",
      "6750\n",
      "Epoch 616, Loss: 0.7405619147795218\n",
      "Test Loss after Epoch 616: 0.8358208441734314\n",
      "6750\n",
      "Epoch 617, Loss: 0.7397688519159953\n",
      "Test Loss after Epoch 617: 0.8380782389640808\n",
      "6750\n",
      "Epoch 618, Loss: 0.7381208638085259\n",
      "Test Loss after Epoch 618: 0.8466887228488922\n",
      "6750\n",
      "Epoch 619, Loss: 0.7402689971040797\n",
      "Test Loss after Epoch 619: 0.8387093033790588\n",
      "6750\n",
      "Epoch 620, Loss: 0.7389731789871499\n",
      "Test Loss after Epoch 620: 0.8390414929389953\n",
      "6750\n",
      "Epoch 621, Loss: 0.7394037840984485\n",
      "Test Loss after Epoch 621: 0.840442007780075\n",
      "6750\n",
      "Epoch 622, Loss: 0.7387366358968946\n",
      "Test Loss after Epoch 622: 0.8371058597564697\n",
      "6750\n",
      "Epoch 623, Loss: 0.7382759476414433\n",
      "Test Loss after Epoch 623: 0.8425418677330017\n",
      "6750\n",
      "Epoch 624, Loss: 0.7371710105295535\n",
      "Test Loss after Epoch 624: 0.8389610085487366\n",
      "6750\n",
      "Epoch 625, Loss: 0.7373980399414345\n",
      "Test Loss after Epoch 625: 0.8353118410110474\n",
      "6750\n",
      "Epoch 626, Loss: 0.739144548062925\n",
      "Test Loss after Epoch 626: 0.8416962776184082\n",
      "6750\n",
      "Epoch 627, Loss: 0.7383604429562887\n",
      "Test Loss after Epoch 627: 0.8441384830474854\n",
      "6750\n",
      "Epoch 628, Loss: 0.7384182340127451\n",
      "Test Loss after Epoch 628: 0.8444351944923401\n",
      "6750\n",
      "Epoch 629, Loss: 0.737587040812881\n",
      "Test Loss after Epoch 629: 0.855536473274231\n",
      "6750\n",
      "Epoch 630, Loss: 0.7358289748297797\n",
      "Test Loss after Epoch 630: 0.8426773324012756\n",
      "6750\n",
      "Epoch 631, Loss: 0.7373710776435004\n",
      "Test Loss after Epoch 631: 0.843481654882431\n",
      "6750\n",
      "Epoch 632, Loss: 0.7396847230063545\n",
      "Test Loss after Epoch 632: 0.8509085171222687\n",
      "6750\n",
      "Epoch 633, Loss: 0.7380796042018467\n",
      "Test Loss after Epoch 633: 0.8372216186523438\n",
      "6750\n",
      "Epoch 634, Loss: 0.7378807854829011\n",
      "Test Loss after Epoch 634: 0.8390464468002319\n",
      "6750\n",
      "Epoch 635, Loss: 0.7376171391275194\n",
      "Test Loss after Epoch 635: 0.8471028096675873\n",
      "6750\n",
      "Epoch 636, Loss: 0.7386857050083302\n",
      "Test Loss after Epoch 636: 0.8346903932094574\n",
      "6750\n",
      "Epoch 637, Loss: 0.7372866694662306\n",
      "Test Loss after Epoch 637: 0.834090656042099\n",
      "6750\n",
      "Epoch 638, Loss: 0.7367990155396638\n",
      "Test Loss after Epoch 638: 0.8445014991760253\n",
      "6750\n",
      "Epoch 639, Loss: 0.7387464269708705\n",
      "Test Loss after Epoch 639: 0.8431066162586213\n",
      "6750\n",
      "Epoch 640, Loss: 0.7374067706178736\n",
      "Test Loss after Epoch 640: 0.8439538638591766\n",
      "6750\n",
      "Epoch 641, Loss: 0.7384227766284236\n",
      "Test Loss after Epoch 641: 0.8487343242168427\n",
      "6750\n",
      "Epoch 642, Loss: 0.7390664184358385\n",
      "Test Loss after Epoch 642: 0.8494819617271423\n",
      "6750\n",
      "Epoch 643, Loss: 0.7380203429681284\n",
      "Test Loss after Epoch 643: 0.8442795577049256\n",
      "6750\n",
      "Epoch 644, Loss: 0.738009567031154\n",
      "Test Loss after Epoch 644: 0.8448899374008179\n",
      "6750\n",
      "Epoch 645, Loss: 0.7364068755220484\n",
      "Test Loss after Epoch 645: 0.8515039446353913\n",
      "6750\n",
      "Epoch 646, Loss: 0.737075524436103\n",
      "Test Loss after Epoch 646: 0.8585556225776673\n",
      "6750\n",
      "Epoch 647, Loss: 0.7359804551513107\n",
      "Test Loss after Epoch 647: 0.8555778250694275\n",
      "6750\n",
      "Epoch 648, Loss: 0.7368670877527308\n",
      "Test Loss after Epoch 648: 0.8490085461139679\n",
      "6750\n",
      "Epoch 649, Loss: 0.7346508423840558\n",
      "Test Loss after Epoch 649: 0.8483221735954285\n",
      "6750\n",
      "Epoch 650, Loss: 0.7368399281148558\n",
      "Test Loss after Epoch 650: 0.8445567915439606\n",
      "6750\n",
      "Epoch 651, Loss: 0.7369839455639875\n",
      "Test Loss after Epoch 651: 0.8493084650039673\n",
      "6750\n",
      "Epoch 652, Loss: 0.7368816314096804\n",
      "Test Loss after Epoch 652: 0.8460584697723389\n",
      "6750\n",
      "Epoch 653, Loss: 0.7368066078115393\n",
      "Test Loss after Epoch 653: 0.8511069343090057\n",
      "6750\n",
      "Epoch 654, Loss: 0.736443555549339\n",
      "Test Loss after Epoch 654: 0.8486364395618439\n",
      "6750\n",
      "Epoch 655, Loss: 0.7370393287694013\n",
      "Test Loss after Epoch 655: 0.8450133728981019\n",
      "6750\n",
      "Epoch 656, Loss: 0.7372117553287082\n",
      "Test Loss after Epoch 656: 0.8486186518669129\n",
      "6750\n",
      "Epoch 657, Loss: 0.7372518443531461\n",
      "Test Loss after Epoch 657: 0.8438112246990204\n",
      "6750\n",
      "Epoch 658, Loss: 0.7370520311461555\n",
      "Test Loss after Epoch 658: 0.8456724555492401\n",
      "6750\n",
      "Epoch 659, Loss: 0.7357717313060054\n",
      "Test Loss after Epoch 659: 0.8486039304733276\n",
      "6750\n",
      "Epoch 660, Loss: 0.7363222504545142\n",
      "Test Loss after Epoch 660: 0.8345616044998169\n",
      "6750\n",
      "Epoch 661, Loss: 0.7372041967003434\n",
      "Test Loss after Epoch 661: 0.8354704990386963\n",
      "6750\n",
      "Epoch 662, Loss: 0.7354740870970267\n",
      "Test Loss after Epoch 662: 0.8441244947910309\n",
      "6750\n",
      "Epoch 663, Loss: 0.7354910686104386\n",
      "Test Loss after Epoch 663: 0.8445636551380158\n",
      "6750\n",
      "Epoch 664, Loss: 0.7364210229803014\n",
      "Test Loss after Epoch 664: 0.843803808927536\n",
      "6750\n",
      "Epoch 665, Loss: 0.7358528040250143\n",
      "Test Loss after Epoch 665: 0.8423886651992798\n",
      "6750\n",
      "Epoch 666, Loss: 0.7360455757953502\n",
      "Test Loss after Epoch 666: 0.8338045971393585\n",
      "6750\n",
      "Epoch 667, Loss: 0.737156823794047\n",
      "Test Loss after Epoch 667: 0.8339760022163392\n",
      "6750\n",
      "Epoch 668, Loss: 0.7362811641693116\n",
      "Test Loss after Epoch 668: 0.8374317467212677\n",
      "6750\n",
      "Epoch 669, Loss: 0.7355687032452336\n",
      "Test Loss after Epoch 669: 0.8348893570899963\n",
      "6750\n",
      "Epoch 670, Loss: 0.737168602236995\n",
      "Test Loss after Epoch 670: 0.8385265789031983\n",
      "6750\n",
      "Epoch 671, Loss: 0.7361639182479294\n",
      "Test Loss after Epoch 671: 0.8386322259902954\n",
      "6750\n",
      "Epoch 672, Loss: 0.7355752556059095\n",
      "Test Loss after Epoch 672: 0.8372309007644654\n",
      "Rolling back to best model from epoch 540\n",
      "Best test loss: 0.8331969246864319\n",
      "6750\n",
      "Epoch 673, Loss: 0.742753636430811\n",
      "Test Loss after Epoch 673: 0.8471616237163544\n",
      "6750\n",
      "Epoch 674, Loss: 0.7391255512590762\n",
      "Test Loss after Epoch 674: 0.8373452799320221\n",
      "6750\n",
      "Epoch 675, Loss: 0.7379722080054106\n",
      "Test Loss after Epoch 675: 0.8377625980377197\n",
      "6750\n",
      "Epoch 676, Loss: 0.7389867066630611\n",
      "Test Loss after Epoch 676: 0.8398717107772827\n",
      "6750\n",
      "Epoch 677, Loss: 0.7390113597269411\n",
      "Test Loss after Epoch 677: 0.8323429775238037\n",
      "6750\n",
      "Epoch 678, Loss: 0.7388869529653479\n",
      "Test Loss after Epoch 678: 0.836544028043747\n",
      "6750\n",
      "Epoch 679, Loss: 0.7395813356858713\n",
      "Test Loss after Epoch 679: 0.8440010778903961\n",
      "6750\n",
      "Epoch 680, Loss: 0.7392874932818942\n",
      "Test Loss after Epoch 680: 0.8364912428855896\n",
      "6750\n",
      "Epoch 681, Loss: 0.7381486753887601\n",
      "Test Loss after Epoch 681: 0.8317550828456879\n",
      "6750\n",
      "Epoch 682, Loss: 0.7399780142042371\n",
      "Test Loss after Epoch 682: 0.837750988483429\n",
      "6750\n",
      "Epoch 683, Loss: 0.7393454409352055\n",
      "Test Loss after Epoch 683: 0.8284571015834808\n",
      "6750\n",
      "Epoch 684, Loss: 0.7398138775648894\n",
      "Test Loss after Epoch 684: 0.8256458504199982\n",
      "6750\n",
      "Epoch 685, Loss: 0.7373685031290408\n",
      "Test Loss after Epoch 685: 0.8344990234375\n",
      "6750\n",
      "Epoch 686, Loss: 0.7372334217142176\n",
      "Test Loss after Epoch 686: 0.8321267399787903\n",
      "6750\n",
      "Epoch 687, Loss: 0.7380368165969848\n",
      "Test Loss after Epoch 687: 0.8316698169708252\n",
      "6750\n",
      "Epoch 688, Loss: 0.7360634982144391\n",
      "Test Loss after Epoch 688: 0.8446654832363129\n",
      "6750\n",
      "Epoch 689, Loss: 0.737995528433058\n",
      "Test Loss after Epoch 689: 0.8361046905517578\n",
      "6750\n",
      "Epoch 690, Loss: 0.7372563407862628\n",
      "Test Loss after Epoch 690: 0.8343304657936096\n",
      "6750\n",
      "Epoch 691, Loss: 0.7363659359260841\n",
      "Test Loss after Epoch 691: 0.8376639728546142\n",
      "6750\n",
      "Epoch 692, Loss: 0.7361438854358814\n",
      "Test Loss after Epoch 692: 0.8383560757637024\n",
      "6750\n",
      "Epoch 693, Loss: 0.7377128029046235\n",
      "Test Loss after Epoch 693: 0.844310638666153\n",
      "6750\n",
      "Epoch 694, Loss: 0.7385073918766446\n",
      "Test Loss after Epoch 694: 0.8399063153266907\n",
      "6750\n",
      "Epoch 695, Loss: 0.7380097466751381\n",
      "Test Loss after Epoch 695: 0.838560115814209\n",
      "6750\n",
      "Epoch 696, Loss: 0.7372107701831394\n",
      "Test Loss after Epoch 696: 0.8495472688674927\n",
      "6750\n",
      "Epoch 697, Loss: 0.7383460028259843\n",
      "Test Loss after Epoch 697: 0.8482365026473999\n",
      "6750\n",
      "Epoch 698, Loss: 0.7379132772021824\n",
      "Test Loss after Epoch 698: 0.8402311680316925\n",
      "6750\n",
      "Epoch 699, Loss: 0.737069760799408\n",
      "Test Loss after Epoch 699: 0.8470699405670166\n",
      "6750\n",
      "Epoch 700, Loss: 0.7362083131648876\n",
      "Test Loss after Epoch 700: 0.8441016967296601\n",
      "6750\n",
      "Epoch 701, Loss: 0.736708252535926\n",
      "Test Loss after Epoch 701: 0.8510077977180481\n",
      "6750\n",
      "Epoch 702, Loss: 0.7372988097226179\n",
      "Test Loss after Epoch 702: 0.8479332752227783\n",
      "6750\n",
      "Epoch 703, Loss: 0.737463725566864\n",
      "Test Loss after Epoch 703: 0.8382322239875794\n",
      "6750\n",
      "Epoch 704, Loss: 0.7373282658612287\n",
      "Test Loss after Epoch 704: 0.843426566362381\n",
      "6750\n",
      "Epoch 705, Loss: 0.737801886399587\n",
      "Test Loss after Epoch 705: 0.8368823902606964\n",
      "6750\n",
      "Epoch 706, Loss: 0.7388559731024283\n",
      "Test Loss after Epoch 706: 0.8370569450855255\n",
      "6750\n",
      "Epoch 707, Loss: 0.7376448069678413\n",
      "Test Loss after Epoch 707: 0.8413090224266052\n",
      "6750\n",
      "Epoch 708, Loss: 0.7364080545813949\n",
      "Test Loss after Epoch 708: 0.8439656591415405\n",
      "6750\n",
      "Epoch 709, Loss: 0.7364954178068372\n",
      "Test Loss after Epoch 709: 0.8369316372871399\n",
      "6750\n",
      "Epoch 710, Loss: 0.7370467574861315\n",
      "Test Loss after Epoch 710: 0.8418530941009521\n",
      "6750\n",
      "Epoch 711, Loss: 0.7378701644296999\n",
      "Test Loss after Epoch 711: 0.8483369262218475\n",
      "6750\n",
      "Epoch 712, Loss: 0.7377785101113497\n",
      "Test Loss after Epoch 712: 0.8488724243640899\n",
      "6750\n",
      "Epoch 713, Loss: 0.7357904415130615\n",
      "Test Loss after Epoch 713: 0.8422718195915222\n",
      "6750\n",
      "Epoch 714, Loss: 0.7360277216699388\n",
      "Test Loss after Epoch 714: 0.8478949174880982\n",
      "6750\n",
      "Epoch 715, Loss: 0.7353628540392275\n",
      "Test Loss after Epoch 715: 0.8448340294361114\n",
      "6750\n",
      "Epoch 716, Loss: 0.7377133534572743\n",
      "Test Loss after Epoch 716: 0.8406860830783844\n",
      "6750\n",
      "Epoch 717, Loss: 0.7359441562228732\n",
      "Test Loss after Epoch 717: 0.8473431503772736\n",
      "6750\n",
      "Epoch 718, Loss: 0.73670454454422\n",
      "Test Loss after Epoch 718: 0.8453883047103882\n",
      "6750\n",
      "Epoch 719, Loss: 0.7372852567390159\n",
      "Test Loss after Epoch 719: 0.8448138093948364\n",
      "6750\n",
      "Epoch 720, Loss: 0.7358814116054111\n",
      "Test Loss after Epoch 720: 0.8446918795108795\n",
      "6750\n",
      "Epoch 721, Loss: 0.7367306028825266\n",
      "Test Loss after Epoch 721: 0.8384434688091278\n",
      "6750\n",
      "Epoch 722, Loss: 0.736710677853337\n",
      "Test Loss after Epoch 722: 0.8383183081150055\n",
      "6750\n",
      "Epoch 723, Loss: 0.7361213381731951\n",
      "Test Loss after Epoch 723: 0.8471628544330597\n",
      "6750\n",
      "Epoch 724, Loss: 0.7372845231338784\n",
      "Test Loss after Epoch 724: 0.849185929775238\n",
      "6750\n",
      "Epoch 725, Loss: 0.7365459831379078\n",
      "Test Loss after Epoch 725: 0.8477203149795532\n",
      "6750\n",
      "Epoch 726, Loss: 0.7374138783702144\n",
      "Test Loss after Epoch 726: 0.8466210269927978\n",
      "6750\n",
      "Epoch 727, Loss: 0.7371571006598296\n",
      "Test Loss after Epoch 727: 0.8498014655113221\n",
      "6750\n",
      "Epoch 728, Loss: 0.7357458693539655\n",
      "Test Loss after Epoch 728: 0.8487560930252075\n",
      "6750\n",
      "Epoch 729, Loss: 0.7357064830285531\n",
      "Test Loss after Epoch 729: 0.8503453366756439\n",
      "6750\n",
      "Epoch 730, Loss: 0.7361478719358091\n",
      "Test Loss after Epoch 730: 0.8469598004817963\n",
      "6750\n",
      "Epoch 731, Loss: 0.7359090770792078\n",
      "Test Loss after Epoch 731: 0.8384377987384796\n",
      "6750\n",
      "Epoch 732, Loss: 0.7363448106801068\n",
      "Test Loss after Epoch 732: 0.8434704642295837\n",
      "6750\n",
      "Epoch 733, Loss: 0.7356474498466209\n",
      "Test Loss after Epoch 733: 0.8496584806442261\n",
      "6750\n",
      "Epoch 734, Loss: 0.7356817184730813\n",
      "Test Loss after Epoch 734: 0.8386321496963501\n",
      "6750\n",
      "Epoch 735, Loss: 0.7348703187306722\n",
      "Test Loss after Epoch 735: 0.8405962734222412\n",
      "6750\n",
      "Epoch 736, Loss: 0.7359924636770178\n",
      "Test Loss after Epoch 736: 0.8424523735046386\n",
      "6750\n",
      "Epoch 737, Loss: 0.7366888803552698\n",
      "Test Loss after Epoch 737: 0.8387233545780182\n",
      "6750\n",
      "Epoch 738, Loss: 0.7354143634548894\n",
      "Test Loss after Epoch 738: 0.8414494740962982\n",
      "6750\n",
      "Epoch 739, Loss: 0.7361216251408612\n",
      "Test Loss after Epoch 739: 0.8444898331165314\n",
      "6750\n",
      "Epoch 740, Loss: 0.7356471741464403\n",
      "Test Loss after Epoch 740: 0.8439146273136139\n",
      "6750\n",
      "Epoch 741, Loss: 0.736682363986969\n",
      "Test Loss after Epoch 741: 0.8460896317958831\n",
      "6750\n",
      "Epoch 742, Loss: 0.7361524350378248\n",
      "Test Loss after Epoch 742: 0.8486419358253479\n",
      "6750\n",
      "Epoch 743, Loss: 0.7349036112008271\n",
      "Test Loss after Epoch 743: 0.8494322566986084\n",
      "6750\n",
      "Epoch 744, Loss: 0.7342967106324655\n",
      "Test Loss after Epoch 744: 0.8493415188789367\n",
      "6750\n",
      "Epoch 745, Loss: 0.7358027363000093\n",
      "Test Loss after Epoch 745: 0.8525793719291687\n",
      "6750\n",
      "Epoch 746, Loss: 0.7374582561740168\n",
      "Test Loss after Epoch 746: 0.8498056783676148\n",
      "6750\n",
      "Epoch 747, Loss: 0.7365903889691389\n",
      "Test Loss after Epoch 747: 0.844506769657135\n",
      "6750\n",
      "Epoch 748, Loss: 0.7358191692564222\n",
      "Test Loss after Epoch 748: 0.8539356279373169\n",
      "6750\n",
      "Epoch 749, Loss: 0.7347995300822788\n",
      "Test Loss after Epoch 749: 0.8508292806148529\n",
      "6750\n",
      "Epoch 750, Loss: 0.7346683983272977\n",
      "Test Loss after Epoch 750: 0.8464878704547882\n",
      "6750\n",
      "Epoch 751, Loss: 0.735261259538156\n",
      "Test Loss after Epoch 751: 0.8476095778942108\n",
      "6750\n",
      "Epoch 752, Loss: 0.7353349438243442\n",
      "Test Loss after Epoch 752: 0.8481881999969483\n",
      "6750\n",
      "Epoch 753, Loss: 0.7350896923100507\n",
      "Test Loss after Epoch 753: 0.8415159947872162\n",
      "6750\n",
      "Epoch 754, Loss: 0.7355457697444492\n",
      "Test Loss after Epoch 754: 0.841954127073288\n",
      "6750\n",
      "Epoch 755, Loss: 0.7348667663998074\n",
      "Test Loss after Epoch 755: 0.8408851873874664\n",
      "6750\n",
      "Epoch 756, Loss: 0.7343106468518575\n",
      "Test Loss after Epoch 756: 0.8461456363201141\n",
      "6750\n",
      "Epoch 757, Loss: 0.734926688529827\n",
      "Test Loss after Epoch 757: 0.8514761621952057\n",
      "6750\n",
      "Epoch 758, Loss: 0.7348989230085302\n",
      "Test Loss after Epoch 758: 0.8487564599514008\n",
      "6750\n",
      "Epoch 759, Loss: 0.7344726713321827\n",
      "Test Loss after Epoch 759: 0.8438838064670563\n",
      "6750\n",
      "Epoch 760, Loss: 0.7339430447684394\n",
      "Test Loss after Epoch 760: 0.8410787527561188\n",
      "6750\n",
      "Epoch 761, Loss: 0.7364974107565703\n",
      "Test Loss after Epoch 761: 0.8391232359409332\n",
      "6750\n",
      "Epoch 762, Loss: 0.7352365675149141\n",
      "Test Loss after Epoch 762: 0.8442332062721253\n",
      "6750\n",
      "Epoch 763, Loss: 0.7344311646885342\n",
      "Test Loss after Epoch 763: 0.8417092070579529\n",
      "6750\n",
      "Epoch 764, Loss: 0.7342822267214457\n",
      "Test Loss after Epoch 764: 0.8415592505931854\n",
      "6750\n",
      "Epoch 765, Loss: 0.7346362924399199\n",
      "Test Loss after Epoch 765: 0.8442704758644104\n",
      "6750\n",
      "Epoch 766, Loss: 0.735228056024622\n",
      "Test Loss after Epoch 766: 0.8416156609058381\n",
      "6750\n",
      "Epoch 767, Loss: 0.7338744175522416\n",
      "Test Loss after Epoch 767: 0.8420086748600006\n",
      "6750\n",
      "Epoch 768, Loss: 0.7339615278773838\n",
      "Test Loss after Epoch 768: 0.8404086346626282\n",
      "6750\n",
      "Epoch 769, Loss: 0.7333102871930157\n",
      "Test Loss after Epoch 769: 0.8454482491016388\n",
      "6750\n",
      "Epoch 770, Loss: 0.7331735438064293\n",
      "Test Loss after Epoch 770: 0.8566979591846466\n",
      "6750\n",
      "Epoch 771, Loss: 0.7348320670127869\n",
      "Test Loss after Epoch 771: 0.8491631243228912\n",
      "6750\n",
      "Epoch 772, Loss: 0.7355560038707875\n",
      "Test Loss after Epoch 772: 0.8443240756988526\n",
      "6750\n",
      "Epoch 773, Loss: 0.736267079865491\n",
      "Test Loss after Epoch 773: 0.8459105367660522\n",
      "6750\n",
      "Epoch 774, Loss: 0.7344694011299698\n",
      "Test Loss after Epoch 774: 0.8433688993453979\n",
      "6750\n",
      "Epoch 775, Loss: 0.7343021295158951\n",
      "Test Loss after Epoch 775: 0.8411945600509644\n",
      "6750\n",
      "Epoch 776, Loss: 0.7345735571472733\n",
      "Test Loss after Epoch 776: 0.838257618188858\n",
      "6750\n",
      "Epoch 777, Loss: 0.7353912572684111\n",
      "Test Loss after Epoch 777: 0.8415050330162048\n",
      "6750\n",
      "Epoch 778, Loss: 0.7334929589165582\n",
      "Test Loss after Epoch 778: 0.8467583279609681\n",
      "6750\n",
      "Epoch 779, Loss: 0.7342187206656845\n",
      "Test Loss after Epoch 779: 0.8498094766139984\n",
      "6750\n",
      "Epoch 780, Loss: 0.7347387488683065\n",
      "Test Loss after Epoch 780: 0.8503578262329101\n",
      "6750\n",
      "Epoch 781, Loss: 0.7355202267964681\n",
      "Test Loss after Epoch 781: 0.848951984167099\n",
      "6750\n",
      "Epoch 782, Loss: 0.7340997926570751\n",
      "Test Loss after Epoch 782: 0.8444507377147674\n",
      "6750\n",
      "Epoch 783, Loss: 0.7347305900079233\n",
      "Test Loss after Epoch 783: 0.8421740441322326\n",
      "6750\n",
      "Epoch 784, Loss: 0.7361518396448206\n",
      "Test Loss after Epoch 784: 0.8425489566326141\n",
      "6750\n",
      "Epoch 785, Loss: 0.7341308428976271\n",
      "Test Loss after Epoch 785: 0.8433093957901001\n",
      "6750\n",
      "Epoch 786, Loss: 0.7345212086748194\n",
      "Test Loss after Epoch 786: 0.8437798273563385\n",
      "6750\n",
      "Epoch 787, Loss: 0.7334942049980163\n",
      "Test Loss after Epoch 787: 0.8473652579784393\n",
      "6750\n",
      "Epoch 788, Loss: 0.7333200334796199\n",
      "Test Loss after Epoch 788: 0.8503849375247955\n",
      "6750\n",
      "Epoch 789, Loss: 0.7342929963712339\n",
      "Test Loss after Epoch 789: 0.851435016155243\n",
      "6750\n",
      "Epoch 790, Loss: 0.7335260260546649\n",
      "Test Loss after Epoch 790: 0.8583028633594513\n",
      "6750\n",
      "Epoch 791, Loss: 0.7330594315705476\n",
      "Test Loss after Epoch 791: 0.8565498037338257\n",
      "6750\n",
      "Epoch 792, Loss: 0.7337196046864545\n",
      "Test Loss after Epoch 792: 0.8462928609848023\n",
      "Rolling back to best model from epoch 684\n",
      "Best test loss: 0.8256458504199982\n",
      "6750\n",
      "Epoch 793, Loss: 0.7366466452987106\n",
      "Test Loss after Epoch 793: 0.831847615480423\n",
      "6750\n",
      "Epoch 794, Loss: 0.7342457280158997\n",
      "Test Loss after Epoch 794: 0.8343242433071136\n",
      "6750\n",
      "Epoch 795, Loss: 0.7356037437474287\n",
      "Test Loss after Epoch 795: 0.834025856256485\n",
      "6750\n",
      "Epoch 796, Loss: 0.7355185063680013\n",
      "Test Loss after Epoch 796: 0.8385212867259979\n",
      "6750\n",
      "Epoch 797, Loss: 0.7358799064777516\n",
      "Test Loss after Epoch 797: 0.8408220992088318\n",
      "6750\n",
      "Epoch 798, Loss: 0.7365948308485526\n",
      "Test Loss after Epoch 798: 0.8433585929870605\n",
      "6750\n",
      "Epoch 799, Loss: 0.7349284845811349\n",
      "Test Loss after Epoch 799: 0.8385435843467712\n",
      "6750\n",
      "Epoch 800, Loss: 0.735643982163182\n",
      "Test Loss after Epoch 800: 0.8360545434951783\n",
      "6750\n",
      "Epoch 801, Loss: 0.7381468289693197\n",
      "Test Loss after Epoch 801: 0.8357792637348175\n",
      "6750\n",
      "Epoch 802, Loss: 0.7375902417324207\n",
      "Test Loss after Epoch 802: 0.8403393812179566\n",
      "6750\n",
      "Epoch 803, Loss: 0.7373626228791696\n",
      "Test Loss after Epoch 803: 0.8373248825073242\n",
      "6750\n",
      "Epoch 804, Loss: 0.7372123884978118\n",
      "Test Loss after Epoch 804: 0.8340442507266999\n",
      "6750\n",
      "Epoch 805, Loss: 0.7366335118258441\n",
      "Test Loss after Epoch 805: 0.8373467628955841\n",
      "6750\n",
      "Epoch 806, Loss: 0.7383015764201128\n",
      "Test Loss after Epoch 806: 0.8374305560588836\n",
      "6750\n",
      "Epoch 807, Loss: 0.7368267516383419\n",
      "Test Loss after Epoch 807: 0.8484374868869782\n",
      "6750\n",
      "Epoch 808, Loss: 0.7363857981540539\n",
      "Test Loss after Epoch 808: 0.8458173265457153\n",
      "6750\n",
      "Epoch 809, Loss: 0.7353032781283061\n",
      "Test Loss after Epoch 809: 0.8404408850669861\n",
      "6750\n",
      "Epoch 810, Loss: 0.7367901512781779\n",
      "Test Loss after Epoch 810: 0.8374473261833191\n",
      "6750\n",
      "Epoch 811, Loss: 0.7374302928182814\n",
      "Test Loss after Epoch 811: 0.8350655519962311\n",
      "6750\n",
      "Epoch 812, Loss: 0.7362449430889554\n",
      "Test Loss after Epoch 812: 0.8411106276512146\n",
      "6750\n",
      "Epoch 813, Loss: 0.7368773452617504\n",
      "Test Loss after Epoch 813: 0.8392539823055267\n",
      "6750\n",
      "Epoch 814, Loss: 0.7361335654082122\n",
      "Test Loss after Epoch 814: 0.8412377743721008\n",
      "6750\n",
      "Epoch 815, Loss: 0.7362825927381162\n",
      "Test Loss after Epoch 815: 0.8543804728984833\n",
      "6750\n",
      "Epoch 816, Loss: 0.7379078288608127\n",
      "Test Loss after Epoch 816: 0.8544191472530365\n",
      "6750\n",
      "Epoch 817, Loss: 0.737266612317827\n",
      "Test Loss after Epoch 817: 0.8479698550701141\n",
      "6750\n",
      "Epoch 818, Loss: 0.7363404140295806\n",
      "Test Loss after Epoch 818: 0.8451770129203796\n",
      "6750\n",
      "Epoch 819, Loss: 0.7349110131793551\n",
      "Test Loss after Epoch 819: 0.8459937674999237\n",
      "6750\n",
      "Epoch 820, Loss: 0.7374497154553731\n",
      "Test Loss after Epoch 820: 0.8444665937423707\n",
      "6750\n",
      "Epoch 821, Loss: 0.7382039766664859\n",
      "Test Loss after Epoch 821: 0.844031299829483\n",
      "6750\n",
      "Epoch 822, Loss: 0.7357450064906368\n",
      "Test Loss after Epoch 822: 0.8417156610488892\n",
      "6750\n",
      "Epoch 823, Loss: 0.7359403911873146\n",
      "Test Loss after Epoch 823: 0.8364029605388641\n",
      "6750\n",
      "Epoch 824, Loss: 0.7352404814472905\n",
      "Test Loss after Epoch 824: 0.8415735833644867\n",
      "6750\n",
      "Epoch 825, Loss: 0.7352243905950475\n",
      "Test Loss after Epoch 825: 0.8451196637153625\n",
      "6750\n",
      "Epoch 826, Loss: 0.7368464839899982\n",
      "Test Loss after Epoch 826: 0.8388396801948548\n",
      "6750\n",
      "Epoch 827, Loss: 0.7361079193221198\n",
      "Test Loss after Epoch 827: 0.836526602268219\n",
      "6750\n",
      "Epoch 828, Loss: 0.7368200553611473\n",
      "Test Loss after Epoch 828: 0.8370490546226501\n",
      "6750\n",
      "Epoch 829, Loss: 0.7360189800968877\n",
      "Test Loss after Epoch 829: 0.842376415014267\n",
      "6750\n",
      "Epoch 830, Loss: 0.7360938209251121\n",
      "Test Loss after Epoch 830: 0.8453698389530182\n",
      "6750\n",
      "Epoch 831, Loss: 0.7364248274167379\n",
      "Test Loss after Epoch 831: 0.8495554320812225\n",
      "6750\n",
      "Epoch 832, Loss: 0.7373848210793954\n",
      "Test Loss after Epoch 832: 0.841053814649582\n",
      "6750\n",
      "Epoch 833, Loss: 0.7363128349162914\n",
      "Test Loss after Epoch 833: 0.8399005913734436\n",
      "6750\n",
      "Epoch 834, Loss: 0.7344238911558081\n",
      "Test Loss after Epoch 834: 0.8496065812110901\n",
      "6750\n",
      "Epoch 835, Loss: 0.7352191616694133\n",
      "Test Loss after Epoch 835: 0.846468849182129\n",
      "6750\n",
      "Epoch 836, Loss: 0.7364733443790012\n",
      "Test Loss after Epoch 836: 0.8469223523139954\n",
      "6750\n",
      "Epoch 837, Loss: 0.7366690118577746\n",
      "Test Loss after Epoch 837: 0.8483532922267913\n",
      "6750\n",
      "Epoch 838, Loss: 0.7353871908541079\n",
      "Test Loss after Epoch 838: 0.8424835450649262\n",
      "6750\n",
      "Epoch 839, Loss: 0.7338836304876539\n",
      "Test Loss after Epoch 839: 0.8414780631065368\n",
      "6750\n",
      "Epoch 840, Loss: 0.7340303814852679\n",
      "Test Loss after Epoch 840: 0.84277623295784\n",
      "6750\n",
      "Epoch 841, Loss: 0.7350848048351429\n",
      "Test Loss after Epoch 841: 0.8435654292106628\n",
      "6750\n",
      "Epoch 842, Loss: 0.7342413941136113\n",
      "Test Loss after Epoch 842: 0.8407467682361602\n",
      "6750\n",
      "Epoch 843, Loss: 0.7345457664065891\n",
      "Test Loss after Epoch 843: 0.8407901384830475\n",
      "6750\n",
      "Epoch 844, Loss: 0.7353806715364809\n",
      "Test Loss after Epoch 844: 0.8438881363868713\n",
      "6750\n",
      "Epoch 845, Loss: 0.7346364296807183\n",
      "Test Loss after Epoch 845: 0.8447107739448547\n",
      "6750\n",
      "Epoch 846, Loss: 0.7359867120495549\n",
      "Test Loss after Epoch 846: 0.8439986374378204\n",
      "6750\n",
      "Epoch 847, Loss: 0.7343102927561159\n",
      "Test Loss after Epoch 847: 0.8454577226638794\n",
      "6750\n",
      "Epoch 848, Loss: 0.7350017596880595\n",
      "Test Loss after Epoch 848: 0.8382764933109284\n",
      "6750\n",
      "Epoch 849, Loss: 0.73407803440094\n",
      "Test Loss after Epoch 849: 0.8389333217144013\n",
      "6750\n",
      "Epoch 850, Loss: 0.7332043452969303\n",
      "Test Loss after Epoch 850: 0.8462903406620026\n",
      "6750\n",
      "Epoch 851, Loss: 0.7356036369888871\n",
      "Test Loss after Epoch 851: 0.8509564590454102\n",
      "6750\n",
      "Epoch 852, Loss: 0.7355681872191252\n",
      "Test Loss after Epoch 852: 0.8425390961170196\n",
      "6750\n",
      "Epoch 853, Loss: 0.7348335433182893\n",
      "Test Loss after Epoch 853: 0.8455206229686737\n",
      "6750\n",
      "Epoch 854, Loss: 0.7337709142720258\n",
      "Test Loss after Epoch 854: 0.841985229253769\n",
      "6750\n",
      "Epoch 855, Loss: 0.733046560658349\n",
      "Test Loss after Epoch 855: 0.8433991713523865\n",
      "6750\n",
      "Epoch 856, Loss: 0.7354424818356832\n",
      "Test Loss after Epoch 856: 0.8464807558059693\n",
      "6750\n",
      "Epoch 857, Loss: 0.7334141119851006\n",
      "Test Loss after Epoch 857: 0.842770265340805\n",
      "6750\n",
      "Epoch 858, Loss: 0.7345983569886949\n",
      "Test Loss after Epoch 858: 0.8416666173934937\n",
      "6750\n",
      "Epoch 859, Loss: 0.7333953156118039\n",
      "Test Loss after Epoch 859: 0.8405375616550446\n",
      "6750\n",
      "Epoch 860, Loss: 0.7331405099586205\n",
      "Test Loss after Epoch 860: 0.8430729062557221\n",
      "6750\n",
      "Epoch 861, Loss: 0.7362924378006547\n",
      "Test Loss after Epoch 861: 0.8437381427288055\n",
      "6750\n",
      "Epoch 862, Loss: 0.7345193795451411\n",
      "Test Loss after Epoch 862: 0.8549980795383454\n",
      "6750\n",
      "Epoch 863, Loss: 0.7348983322602731\n",
      "Test Loss after Epoch 863: 0.8448929150104523\n",
      "6750\n",
      "Epoch 864, Loss: 0.7346895448720013\n",
      "Test Loss after Epoch 864: 0.8419798555374145\n",
      "6750\n",
      "Epoch 865, Loss: 0.7339765264016611\n",
      "Test Loss after Epoch 865: 0.8409562206268311\n",
      "6750\n",
      "Epoch 866, Loss: 0.7348437955114576\n",
      "Test Loss after Epoch 866: 0.8497913188934326\n",
      "6750\n",
      "Epoch 867, Loss: 0.7334910514972828\n",
      "Test Loss after Epoch 867: 0.84934636759758\n",
      "6750\n",
      "Epoch 868, Loss: 0.7336272064668161\n",
      "Test Loss after Epoch 868: 0.8423322033882141\n",
      "6750\n",
      "Epoch 869, Loss: 0.7344296630046986\n",
      "Test Loss after Epoch 869: 0.8458197903633118\n",
      "6750\n",
      "Epoch 870, Loss: 0.7341591814888848\n",
      "Test Loss after Epoch 870: 0.852425422668457\n",
      "6750\n",
      "Epoch 871, Loss: 0.7345371294198213\n",
      "Test Loss after Epoch 871: 0.8525875720977784\n",
      "6750\n",
      "Epoch 872, Loss: 0.734334538406796\n",
      "Test Loss after Epoch 872: 0.8620270335674286\n",
      "6750\n",
      "Epoch 873, Loss: 0.7339978425767687\n",
      "Test Loss after Epoch 873: 0.8714970512390137\n",
      "6750\n",
      "Epoch 874, Loss: 0.7348002104935822\n",
      "Test Loss after Epoch 874: 0.8545387814044952\n",
      "6750\n",
      "Epoch 875, Loss: 0.734006500420747\n",
      "Test Loss after Epoch 875: 0.8409303646087647\n",
      "6750\n",
      "Epoch 876, Loss: 0.7338908354441325\n",
      "Test Loss after Epoch 876: 0.8441720395088196\n",
      "6750\n",
      "Epoch 877, Loss: 0.7330770968154625\n",
      "Test Loss after Epoch 877: 0.8441654777526856\n",
      "6750\n",
      "Epoch 878, Loss: 0.7334657456786544\n",
      "Test Loss after Epoch 878: 0.8459656701087952\n",
      "6750\n",
      "Epoch 879, Loss: 0.7330856236175255\n",
      "Test Loss after Epoch 879: 0.8539409365653992\n",
      "6750\n",
      "Epoch 880, Loss: 0.7338572181595696\n",
      "Test Loss after Epoch 880: 0.8522118434906006\n",
      "6750\n",
      "Epoch 881, Loss: 0.7341198384850114\n",
      "Test Loss after Epoch 881: 0.8485901319980621\n",
      "6750\n",
      "Epoch 882, Loss: 0.7333430346912808\n",
      "Test Loss after Epoch 882: 0.8602680704593658\n",
      "6750\n",
      "Epoch 883, Loss: 0.7336332908206515\n",
      "Test Loss after Epoch 883: 0.8615014379024506\n",
      "6750\n",
      "Epoch 884, Loss: 0.7335377336784645\n",
      "Test Loss after Epoch 884: 0.8512738647460938\n",
      "6750\n",
      "Epoch 885, Loss: 0.732503757070612\n",
      "Test Loss after Epoch 885: 0.8504499056339264\n",
      "6750\n",
      "Epoch 886, Loss: 0.7331985829318012\n",
      "Test Loss after Epoch 886: 0.8566215677261353\n",
      "6750\n",
      "Epoch 887, Loss: 0.7351429117697257\n",
      "Test Loss after Epoch 887: 0.8568214733600616\n",
      "6750\n",
      "Epoch 888, Loss: 0.7326537963549296\n",
      "Test Loss after Epoch 888: 0.8544062490463257\n",
      "6750\n",
      "Epoch 889, Loss: 0.7325577039365415\n",
      "Test Loss after Epoch 889: 0.860331640958786\n",
      "6750\n",
      "Epoch 890, Loss: 0.7342418765845122\n",
      "Test Loss after Epoch 890: 0.8629133992195129\n",
      "6750\n",
      "Epoch 891, Loss: 0.7343841474674366\n",
      "Test Loss after Epoch 891: 0.8570295448303222\n",
      "6750\n",
      "Epoch 892, Loss: 0.7339639288407785\n",
      "Test Loss after Epoch 892: 0.8530607843399047\n",
      "6750\n",
      "Epoch 893, Loss: 0.7330417045487297\n",
      "Test Loss after Epoch 893: 0.8521726117134094\n",
      "6750\n",
      "Epoch 894, Loss: 0.7329713380071852\n",
      "Test Loss after Epoch 894: 0.8553693821430206\n",
      "6750\n",
      "Epoch 895, Loss: 0.7348054313483061\n",
      "Test Loss after Epoch 895: 0.8556564526557923\n",
      "6750\n",
      "Epoch 896, Loss: 0.7336724298971671\n",
      "Test Loss after Epoch 896: 0.8566691846847534\n",
      "6750\n",
      "Epoch 897, Loss: 0.734165032280816\n",
      "Test Loss after Epoch 897: 0.8535993504524231\n",
      "6750\n",
      "Epoch 898, Loss: 0.732334984532109\n",
      "Test Loss after Epoch 898: 0.8504821701049805\n",
      "6750\n",
      "Epoch 899, Loss: 0.7324598688902678\n",
      "Test Loss after Epoch 899: 0.8510604126453399\n",
      "6750\n",
      "Epoch 900, Loss: 0.7329026016835813\n",
      "Test Loss after Epoch 900: 0.8563039600849152\n",
      "6750\n",
      "Epoch 901, Loss: 0.7336790744286996\n",
      "Test Loss after Epoch 901: 0.8563951444625855\n",
      "6750\n",
      "Epoch 902, Loss: 0.7335466063287523\n",
      "Test Loss after Epoch 902: 0.8518984162807465\n",
      "6750\n",
      "Epoch 903, Loss: 0.7324494933022393\n",
      "Test Loss after Epoch 903: 0.8521590142250061\n",
      "6750\n",
      "Epoch 904, Loss: 0.7332831596974974\n",
      "Test Loss after Epoch 904: 0.8538856101036072\n",
      "6750\n",
      "Epoch 905, Loss: 0.7331877195040385\n",
      "Test Loss after Epoch 905: 0.8573663642406464\n",
      "6750\n",
      "Epoch 906, Loss: 0.7338815294548318\n",
      "Test Loss after Epoch 906: 0.8586288802623748\n",
      "6750\n",
      "Epoch 907, Loss: 0.7329576775939376\n",
      "Test Loss after Epoch 907: 0.8569424023628235\n",
      "6750\n",
      "Epoch 908, Loss: 0.7319690204549719\n",
      "Test Loss after Epoch 908: 0.855552413225174\n",
      "6750\n",
      "Epoch 909, Loss: 0.7323955088897988\n",
      "Test Loss after Epoch 909: 0.8502719712257385\n",
      "6750\n",
      "Epoch 910, Loss: 0.7325199886781198\n",
      "Test Loss after Epoch 910: 0.8474264929294586\n",
      "6750\n",
      "Epoch 911, Loss: 0.7321603939444931\n",
      "Test Loss after Epoch 911: 0.846778336763382\n",
      "6750\n",
      "Epoch 912, Loss: 0.7328988194465638\n",
      "Test Loss after Epoch 912: 0.8507887327671051\n",
      "6750\n",
      "Epoch 913, Loss: 0.7322735385718169\n",
      "Test Loss after Epoch 913: 0.8523769357204437\n",
      "6750\n",
      "Epoch 914, Loss: 0.7324490765642236\n",
      "Test Loss after Epoch 914: 0.8537342748641967\n",
      "6750\n",
      "Epoch 915, Loss: 0.7322667948404948\n",
      "Test Loss after Epoch 915: 0.8528875176906585\n",
      "6750\n",
      "Epoch 916, Loss: 0.732549982742027\n",
      "Test Loss after Epoch 916: 0.8497139227390289\n",
      "6750\n",
      "Epoch 917, Loss: 0.7327265172887731\n",
      "Test Loss after Epoch 917: 0.846321444272995\n",
      "6750\n",
      "Epoch 918, Loss: 0.7326087301572164\n",
      "Test Loss after Epoch 918: 0.849696528673172\n",
      "6750\n",
      "Epoch 919, Loss: 0.7323924321245264\n",
      "Test Loss after Epoch 919: 0.8509721565246582\n",
      "6750\n",
      "Epoch 920, Loss: 0.7317391005445409\n",
      "Test Loss after Epoch 920: 0.8519786508083343\n",
      "6750\n",
      "Epoch 921, Loss: 0.7324845579112017\n",
      "Test Loss after Epoch 921: 0.8551840987205506\n",
      "6750\n",
      "Epoch 922, Loss: 0.7322106827629937\n",
      "Test Loss after Epoch 922: 0.8573653426170349\n",
      "6750\n",
      "Epoch 923, Loss: 0.7321913573830217\n",
      "Test Loss after Epoch 923: 0.8609758772850037\n",
      "6750\n",
      "Epoch 924, Loss: 0.7322789360152351\n",
      "Test Loss after Epoch 924: 0.8500346405506134\n",
      "6750\n",
      "Epoch 925, Loss: 0.732920086560426\n",
      "Test Loss after Epoch 925: 0.8470565686225892\n",
      "6750\n",
      "Epoch 926, Loss: 0.732616552317584\n",
      "Test Loss after Epoch 926: 0.8483043541908264\n",
      "6750\n",
      "Epoch 927, Loss: 0.7332452115306148\n",
      "Test Loss after Epoch 927: 0.8465133357048035\n",
      "6750\n",
      "Epoch 928, Loss: 0.733325788144712\n",
      "Test Loss after Epoch 928: 0.8418118381500244\n",
      "6750\n",
      "Epoch 929, Loss: 0.732400986318235\n",
      "Test Loss after Epoch 929: 0.8502912871837616\n",
      "6750\n",
      "Epoch 930, Loss: 0.7330164291946977\n",
      "Test Loss after Epoch 930: 0.859610237121582\n",
      "6750\n",
      "Epoch 931, Loss: 0.7339647519146955\n",
      "Test Loss after Epoch 931: 0.8558942778110504\n",
      "6750\n",
      "Epoch 932, Loss: 0.7325010561236629\n",
      "Test Loss after Epoch 932: 0.8526294298171997\n",
      "6750\n",
      "Epoch 933, Loss: 0.73313097415147\n",
      "Test Loss after Epoch 933: 0.8485493228435517\n",
      "6750\n",
      "Epoch 934, Loss: 0.7326265621715122\n",
      "Test Loss after Epoch 934: 0.8426098716259003\n",
      "6750\n",
      "Epoch 935, Loss: 0.7322202380498251\n",
      "Test Loss after Epoch 935: 0.8491902215480804\n",
      "6750\n",
      "Epoch 936, Loss: 0.7329406949149238\n",
      "Test Loss after Epoch 936: 0.851677759885788\n",
      "6750\n",
      "Epoch 937, Loss: 0.7331374633047316\n",
      "Test Loss after Epoch 937: 0.8502337946891785\n",
      "6750\n",
      "Epoch 938, Loss: 0.7318235513192636\n",
      "Test Loss after Epoch 938: 0.8460090198516845\n",
      "6750\n",
      "Epoch 939, Loss: 0.7328920324996666\n",
      "Test Loss after Epoch 939: 0.8415646727085113\n",
      "6750\n",
      "Epoch 940, Loss: 0.7316439248014379\n",
      "Test Loss after Epoch 940: 0.847943466424942\n",
      "6750\n",
      "Epoch 941, Loss: 0.732716027436433\n",
      "Test Loss after Epoch 941: 0.8492040057182312\n",
      "6750\n",
      "Epoch 942, Loss: 0.732075980981191\n",
      "Test Loss after Epoch 942: 0.8485212345123291\n",
      "6750\n",
      "Epoch 943, Loss: 0.7325342997798213\n",
      "Test Loss after Epoch 943: 0.8449654858112335\n",
      "6750\n",
      "Epoch 944, Loss: 0.731366448437726\n",
      "Test Loss after Epoch 944: 0.8478090393543244\n",
      "Rolling back to best model from epoch 684\n",
      "Best test loss: 0.8256458504199982\n",
      "6750\n",
      "Epoch 945, Loss: 0.738304358959198\n",
      "Test Loss after Epoch 945: 0.8346597361564636\n",
      "6750\n",
      "Epoch 946, Loss: 0.7372030560352184\n",
      "Test Loss after Epoch 946: 0.8335353186130524\n",
      "6750\n",
      "Epoch 947, Loss: 0.7354461352383649\n",
      "Test Loss after Epoch 947: 0.8300759432315826\n",
      "6750\n",
      "Epoch 948, Loss: 0.737402010652754\n",
      "Test Loss after Epoch 948: 0.8426558210849762\n",
      "6750\n",
      "Epoch 949, Loss: 0.7361413497748198\n",
      "Test Loss after Epoch 949: 0.8397320358753204\n",
      "6750\n",
      "Epoch 950, Loss: 0.7347611062085186\n",
      "Test Loss after Epoch 950: 0.83546968460083\n",
      "6750\n",
      "Epoch 951, Loss: 0.7371510971387227\n",
      "Test Loss after Epoch 951: 0.833393969297409\n",
      "6750\n",
      "Epoch 952, Loss: 0.7356286770855939\n",
      "Test Loss after Epoch 952: 0.8353464002609253\n",
      "6750\n",
      "Epoch 953, Loss: 0.735992919144807\n",
      "Test Loss after Epoch 953: 0.8373490314483643\n",
      "6750\n",
      "Epoch 954, Loss: 0.7347317825776559\n",
      "Test Loss after Epoch 954: 0.8359222311973572\n",
      "6750\n",
      "Epoch 955, Loss: 0.735461435565242\n",
      "Test Loss after Epoch 955: 0.8370785491466523\n",
      "6750\n",
      "Epoch 956, Loss: 0.736070558583295\n",
      "Test Loss after Epoch 956: 0.8395821549892426\n",
      "6750\n",
      "Epoch 957, Loss: 0.7364207909372118\n",
      "Test Loss after Epoch 957: 0.8384431056976318\n",
      "6750\n",
      "Epoch 958, Loss: 0.7355625544477392\n",
      "Test Loss after Epoch 958: 0.8513593742847443\n",
      "6750\n",
      "Epoch 959, Loss: 0.7354491637371204\n",
      "Test Loss after Epoch 959: 0.8517734844684601\n",
      "6750\n",
      "Epoch 960, Loss: 0.7350666071397287\n",
      "Test Loss after Epoch 960: 0.845023013830185\n",
      "6750\n",
      "Epoch 961, Loss: 0.7351466913399873\n",
      "Test Loss after Epoch 961: 0.8460701332092285\n",
      "6750\n",
      "Epoch 962, Loss: 0.7348850423494975\n",
      "Test Loss after Epoch 962: 0.8438620913028717\n",
      "6750\n",
      "Epoch 963, Loss: 0.7357842226911474\n",
      "Test Loss after Epoch 963: 0.8390033161640167\n",
      "6750\n",
      "Epoch 964, Loss: 0.7351269622908698\n",
      "Test Loss after Epoch 964: 0.8363894810676574\n",
      "6750\n",
      "Epoch 965, Loss: 0.7340857131569474\n",
      "Test Loss after Epoch 965: 0.8345769536495209\n",
      "6750\n",
      "Epoch 966, Loss: 0.7353767578336927\n",
      "Test Loss after Epoch 966: 0.8369213993549347\n",
      "6750\n",
      "Epoch 967, Loss: 0.7346870126724243\n",
      "Test Loss after Epoch 967: 0.8400942053794861\n",
      "6750\n",
      "Epoch 968, Loss: 0.7355651114605091\n",
      "Test Loss after Epoch 968: 0.8408955850601196\n",
      "6750\n",
      "Epoch 969, Loss: 0.7337920962263037\n",
      "Test Loss after Epoch 969: 0.8335841917991638\n",
      "6750\n",
      "Epoch 970, Loss: 0.7350956750975715\n",
      "Test Loss after Epoch 970: 0.8353612315654755\n",
      "6750\n",
      "Epoch 971, Loss: 0.7351513696070071\n",
      "Test Loss after Epoch 971: 0.8427153878211975\n",
      "6750\n",
      "Epoch 972, Loss: 0.7362977164586385\n",
      "Test Loss after Epoch 972: 0.842631938457489\n",
      "6750\n",
      "Epoch 973, Loss: 0.7350095237449363\n",
      "Test Loss after Epoch 973: 0.8448113162517548\n",
      "6750\n",
      "Epoch 974, Loss: 0.7344905928859005\n",
      "Test Loss after Epoch 974: 0.8473240702152253\n",
      "6750\n",
      "Epoch 975, Loss: 0.7354161609013875\n",
      "Test Loss after Epoch 975: 0.8490678038597107\n",
      "6750\n",
      "Epoch 976, Loss: 0.7356658271506981\n",
      "Test Loss after Epoch 976: 0.8498068263530731\n",
      "6750\n",
      "Epoch 977, Loss: 0.7356543509165446\n",
      "Test Loss after Epoch 977: 0.8573593201637268\n",
      "6750\n",
      "Epoch 978, Loss: 0.733727497877898\n",
      "Test Loss after Epoch 978: 0.8478237397670746\n",
      "6750\n",
      "Epoch 979, Loss: 0.7356106321723372\n",
      "Test Loss after Epoch 979: 0.8468152594566345\n",
      "6750\n",
      "Epoch 980, Loss: 0.7347278082988881\n",
      "Test Loss after Epoch 980: 0.8476480901241302\n",
      "6750\n",
      "Epoch 981, Loss: 0.7350683715078565\n",
      "Test Loss after Epoch 981: 0.8467517342567444\n",
      "6750\n",
      "Epoch 982, Loss: 0.7339615607085052\n",
      "Test Loss after Epoch 982: 0.8501530377864838\n",
      "6750\n",
      "Epoch 983, Loss: 0.7345380581573204\n",
      "Test Loss after Epoch 983: 0.8497595705986023\n",
      "6750\n",
      "Epoch 984, Loss: 0.7342631916999817\n",
      "Test Loss after Epoch 984: 0.846076821565628\n",
      "6750\n",
      "Epoch 985, Loss: 0.7331922825354117\n",
      "Test Loss after Epoch 985: 0.8468169708251954\n",
      "6750\n",
      "Epoch 986, Loss: 0.7351580136263812\n",
      "Test Loss after Epoch 986: 0.8416696603298187\n",
      "6750\n",
      "Epoch 987, Loss: 0.7355400154678909\n",
      "Test Loss after Epoch 987: 0.8394273416996002\n",
      "6750\n",
      "Epoch 988, Loss: 0.7344519220634743\n",
      "Test Loss after Epoch 988: 0.8431067953109741\n",
      "6750\n",
      "Epoch 989, Loss: 0.7342051991709957\n",
      "Test Loss after Epoch 989: 0.841057005405426\n",
      "6750\n",
      "Epoch 990, Loss: 0.7333321284541378\n",
      "Test Loss after Epoch 990: 0.8404451541900635\n",
      "6750\n",
      "Epoch 991, Loss: 0.7364419984287686\n",
      "Test Loss after Epoch 991: 0.8416011106967926\n",
      "6750\n",
      "Epoch 992, Loss: 0.734475389904446\n",
      "Test Loss after Epoch 992: 0.8471047384738922\n",
      "6750\n",
      "Epoch 993, Loss: 0.7353192786110772\n",
      "Test Loss after Epoch 993: 0.8531479737758636\n",
      "6750\n",
      "Epoch 994, Loss: 0.7358394906079327\n",
      "Test Loss after Epoch 994: 0.8429912807941436\n",
      "6750\n",
      "Epoch 995, Loss: 0.7350054136558816\n",
      "Test Loss after Epoch 995: 0.8476721184253693\n",
      "6750\n",
      "Epoch 996, Loss: 0.7351359430595681\n",
      "Test Loss after Epoch 996: 0.85324613571167\n",
      "6750\n",
      "Epoch 997, Loss: 0.7347494653949032\n",
      "Test Loss after Epoch 997: 0.8563429591655731\n",
      "6750\n",
      "Epoch 998, Loss: 0.734324759748247\n",
      "Test Loss after Epoch 998: 0.8487526016235352\n",
      "6750\n",
      "Epoch 999, Loss: 0.7335927087995741\n",
      "Test Loss after Epoch 999: 0.8592323768138885\n",
      "6750\n",
      "Epoch 1000, Loss: 0.7339600734887299\n",
      "Test Loss after Epoch 1000: 0.8576372776031495\n",
      "6750\n",
      "Epoch 1001, Loss: 0.7339802885055542\n",
      "Test Loss after Epoch 1001: 0.8492007420063019\n",
      "6750\n",
      "Epoch 1002, Loss: 0.7345761436356438\n",
      "Test Loss after Epoch 1002: 0.8477253544330597\n",
      "6750\n",
      "Epoch 1003, Loss: 0.7340879443486532\n",
      "Test Loss after Epoch 1003: 0.8459499833583832\n",
      "6750\n",
      "Epoch 1004, Loss: 0.7346056497891744\n",
      "Test Loss after Epoch 1004: 0.8448867948055268\n",
      "6750\n",
      "Epoch 1005, Loss: 0.7333486617759422\n",
      "Test Loss after Epoch 1005: 0.842944687128067\n",
      "6750\n",
      "Epoch 1006, Loss: 0.7343429609934489\n",
      "Test Loss after Epoch 1006: 0.8420057735443115\n",
      "6750\n",
      "Epoch 1007, Loss: 0.7329761128602205\n",
      "Test Loss after Epoch 1007: 0.8405359997749329\n",
      "6750\n",
      "Epoch 1008, Loss: 0.7341165044219405\n",
      "Test Loss after Epoch 1008: 0.8380726683139801\n",
      "6750\n",
      "Epoch 1009, Loss: 0.7340199989742703\n",
      "Test Loss after Epoch 1009: 0.8411917238235473\n",
      "6750\n",
      "Epoch 1010, Loss: 0.7334389730736062\n",
      "Test Loss after Epoch 1010: 0.844001984834671\n",
      "6750\n",
      "Epoch 1011, Loss: 0.7330900063338103\n",
      "Test Loss after Epoch 1011: 0.838908136844635\n",
      "6750\n",
      "Epoch 1012, Loss: 0.7324224216319897\n",
      "Test Loss after Epoch 1012: 0.8415575473308563\n",
      "6750\n",
      "Epoch 1013, Loss: 0.7329218167375635\n",
      "Test Loss after Epoch 1013: 0.8416416730880737\n",
      "6750\n",
      "Epoch 1014, Loss: 0.7330246446750782\n",
      "Test Loss after Epoch 1014: 0.8432721729278565\n",
      "6750\n",
      "Epoch 1015, Loss: 0.7329872923603764\n",
      "Test Loss after Epoch 1015: 0.8516601107120514\n",
      "6750\n",
      "Epoch 1016, Loss: 0.733147349163338\n",
      "Test Loss after Epoch 1016: 0.8434425642490387\n",
      "6750\n",
      "Epoch 1017, Loss: 0.7326069946465669\n",
      "Test Loss after Epoch 1017: 0.8377336673736572\n",
      "6750\n",
      "Epoch 1018, Loss: 0.732617733973044\n",
      "Test Loss after Epoch 1018: 0.8359816901683808\n",
      "6750\n",
      "Epoch 1019, Loss: 0.7338392405509949\n",
      "Test Loss after Epoch 1019: 0.8329361052513122\n",
      "6750\n",
      "Epoch 1020, Loss: 0.7333503075705634\n",
      "Test Loss after Epoch 1020: 0.8291289570331574\n",
      "6750\n",
      "Epoch 1021, Loss: 0.7348480896773162\n",
      "Test Loss after Epoch 1021: 0.8343042998313904\n",
      "6750\n",
      "Epoch 1022, Loss: 0.7335273420369184\n",
      "Test Loss after Epoch 1022: 0.8432334523200988\n",
      "6750\n",
      "Epoch 1023, Loss: 0.7346136491033766\n",
      "Test Loss after Epoch 1023: 0.8392023138999939\n",
      "6750\n",
      "Epoch 1024, Loss: 0.7339128934365732\n",
      "Test Loss after Epoch 1024: 0.835713947057724\n",
      "6750\n",
      "Epoch 1025, Loss: 0.7341110263753821\n",
      "Test Loss after Epoch 1025: 0.8349364168643951\n",
      "6750\n",
      "Epoch 1026, Loss: 0.7328925638905278\n",
      "Test Loss after Epoch 1026: 0.8340308928489685\n",
      "6750\n",
      "Epoch 1027, Loss: 0.7325593635594403\n",
      "Test Loss after Epoch 1027: 0.8343086442947387\n",
      "6750\n",
      "Epoch 1028, Loss: 0.7324407313664755\n",
      "Test Loss after Epoch 1028: 0.8415224406719207\n",
      "6750\n",
      "Epoch 1029, Loss: 0.7333615869769343\n",
      "Test Loss after Epoch 1029: 0.8423105008602142\n",
      "6750\n",
      "Epoch 1030, Loss: 0.7327656356670238\n",
      "Test Loss after Epoch 1030: 0.8386490921974182\n",
      "6750\n",
      "Epoch 1031, Loss: 0.733645954185062\n",
      "Test Loss after Epoch 1031: 0.841437292098999\n",
      "6750\n",
      "Epoch 1032, Loss: 0.7343580644748829\n",
      "Test Loss after Epoch 1032: 0.8434091637134552\n",
      "6750\n",
      "Epoch 1033, Loss: 0.7334964081622936\n",
      "Test Loss after Epoch 1033: 0.8415163431167603\n",
      "6750\n",
      "Epoch 1034, Loss: 0.7333230403087757\n",
      "Test Loss after Epoch 1034: 0.847023983001709\n",
      "6750\n",
      "Epoch 1035, Loss: 0.7339002409687748\n",
      "Test Loss after Epoch 1035: 0.8448408102989197\n",
      "6750\n",
      "Epoch 1036, Loss: 0.7333737570974562\n",
      "Test Loss after Epoch 1036: 0.846480851650238\n",
      "6750\n",
      "Epoch 1037, Loss: 0.7326817591631855\n",
      "Test Loss after Epoch 1037: 0.8487882721424103\n",
      "6750\n",
      "Epoch 1038, Loss: 0.7327040175861782\n",
      "Test Loss after Epoch 1038: 0.8422897701263428\n",
      "6750\n",
      "Epoch 1039, Loss: 0.7324503985334325\n",
      "Test Loss after Epoch 1039: 0.840420001745224\n",
      "6750\n",
      "Epoch 1040, Loss: 0.7325141068387915\n",
      "Test Loss after Epoch 1040: 0.8455683703422546\n",
      "6750\n",
      "Epoch 1041, Loss: 0.7336698732729311\n",
      "Test Loss after Epoch 1041: 0.8525827474594116\n",
      "6750\n",
      "Epoch 1042, Loss: 0.7334944298532274\n",
      "Test Loss after Epoch 1042: 0.8441263020038605\n",
      "6750\n",
      "Epoch 1043, Loss: 0.7337817583967138\n",
      "Test Loss after Epoch 1043: 0.8439837651252746\n",
      "6750\n",
      "Epoch 1044, Loss: 0.7323389118689078\n",
      "Test Loss after Epoch 1044: 0.8477682957649231\n",
      "6750\n",
      "Epoch 1045, Loss: 0.7323872697971485\n",
      "Test Loss after Epoch 1045: 0.8523992452621459\n",
      "6750\n",
      "Epoch 1046, Loss: 0.7346849491684525\n",
      "Test Loss after Epoch 1046: 0.8447190971374512\n",
      "6750\n",
      "Epoch 1047, Loss: 0.73369359087061\n",
      "Test Loss after Epoch 1047: 0.8370472557544708\n",
      "6750\n",
      "Epoch 1048, Loss: 0.7329076601487619\n",
      "Test Loss after Epoch 1048: 0.8354793851375579\n",
      "6750\n",
      "Epoch 1049, Loss: 0.7330403943591648\n",
      "Test Loss after Epoch 1049: 0.8359436931610107\n",
      "6750\n",
      "Epoch 1050, Loss: 0.7323682333098518\n",
      "Test Loss after Epoch 1050: 0.8368635351657867\n",
      "6750\n",
      "Epoch 1051, Loss: 0.7323800835609436\n",
      "Test Loss after Epoch 1051: 0.8384395055770874\n",
      "6750\n",
      "Epoch 1052, Loss: 0.7325187155758893\n",
      "Test Loss after Epoch 1052: 0.8359088380336761\n",
      "6750\n",
      "Epoch 1053, Loss: 0.7320090437641851\n",
      "Test Loss after Epoch 1053: 0.8329484293460846\n",
      "6750\n",
      "Epoch 1054, Loss: 0.7319448562728034\n",
      "Test Loss after Epoch 1054: 0.8345163516998291\n",
      "6750\n",
      "Epoch 1055, Loss: 0.731378308614095\n",
      "Test Loss after Epoch 1055: 0.8356445682048798\n",
      "6750\n",
      "Epoch 1056, Loss: 0.7328564132054647\n",
      "Test Loss after Epoch 1056: 0.8365888319015503\n",
      "6750\n",
      "Epoch 1057, Loss: 0.7323774769924305\n",
      "Test Loss after Epoch 1057: 0.84020561003685\n",
      "6750\n",
      "Epoch 1058, Loss: 0.7324338267291034\n",
      "Test Loss after Epoch 1058: 0.8366922338008881\n",
      "6750\n",
      "Epoch 1059, Loss: 0.7330382406623275\n",
      "Test Loss after Epoch 1059: 0.8374706788063049\n",
      "6750\n",
      "Epoch 1060, Loss: 0.7320108043352763\n",
      "Test Loss after Epoch 1060: 0.8451463191509246\n",
      "6750\n",
      "Epoch 1061, Loss: 0.7323779952967608\n",
      "Test Loss after Epoch 1061: 0.8423088366985321\n",
      "4500\n",
      "Epoch 1062, Loss: 0.7281990247302585\n",
      "Test Loss after Epoch 1062: 0.8378565392494202\n",
      "4500\n",
      "Epoch 1063, Loss: 0.7287631562021044\n",
      "Test Loss after Epoch 1063: 0.8363269703388214\n",
      "4500\n",
      "Epoch 1064, Loss: 0.7281316590574053\n",
      "Test Loss after Epoch 1064: 0.8384996683597564\n",
      "4500\n",
      "Epoch 1065, Loss: 0.7289734098116557\n",
      "Test Loss after Epoch 1065: 0.8438151090145111\n",
      "4500\n",
      "Epoch 1066, Loss: 0.7287089126374986\n",
      "Test Loss after Epoch 1066: 0.8458125927448272\n",
      "4500\n",
      "Epoch 1067, Loss: 0.7282175614304013\n",
      "Test Loss after Epoch 1067: 0.8484231433868408\n",
      "4500\n",
      "Epoch 1068, Loss: 0.7287326087156931\n",
      "Test Loss after Epoch 1068: 0.8490034742355347\n",
      "4500\n",
      "Epoch 1069, Loss: 0.729159652656979\n",
      "Test Loss after Epoch 1069: 0.8505980989933014\n",
      "4500\n",
      "Epoch 1070, Loss: 0.7291102984481388\n",
      "Test Loss after Epoch 1070: 0.8548525981903076\n",
      "4500\n",
      "Epoch 1071, Loss: 0.7282687669065263\n",
      "Test Loss after Epoch 1071: 0.8537741849422454\n",
      "4500\n",
      "Epoch 1072, Loss: 0.729024096303516\n",
      "Test Loss after Epoch 1072: 0.8516637828350068\n",
      "4500\n",
      "Epoch 1073, Loss: 0.7281669149133894\n",
      "Test Loss after Epoch 1073: 0.8512395143508911\n",
      "4500\n",
      "Epoch 1074, Loss: 0.7277890707386865\n",
      "Test Loss after Epoch 1074: 0.8527988216876984\n",
      "4500\n",
      "Epoch 1075, Loss: 0.7298454885482788\n",
      "Test Loss after Epoch 1075: 0.8522098696231842\n",
      "4500\n",
      "Epoch 1076, Loss: 0.7290990810129377\n",
      "Test Loss after Epoch 1076: 0.8518220067024231\n",
      "4500\n",
      "Epoch 1077, Loss: 0.7292610159450107\n",
      "Test Loss after Epoch 1077: 0.8519147202968598\n",
      "4500\n",
      "Epoch 1078, Loss: 0.727877941555447\n",
      "Test Loss after Epoch 1078: 0.851855926990509\n",
      "4500\n",
      "Epoch 1079, Loss: 0.7290954759650761\n",
      "Test Loss after Epoch 1079: 0.851085967540741\n",
      "4500\n",
      "Epoch 1080, Loss: 0.7285506263838873\n",
      "Test Loss after Epoch 1080: 0.853301596403122\n",
      "4500\n",
      "Epoch 1081, Loss: 0.7283552971680959\n",
      "Test Loss after Epoch 1081: 0.8563674006462098\n",
      "4500\n",
      "Epoch 1082, Loss: 0.7297210759586759\n",
      "Test Loss after Epoch 1082: 0.8564475238323211\n",
      "4500\n",
      "Epoch 1083, Loss: 0.7293159301016066\n",
      "Test Loss after Epoch 1083: 0.8546886913776398\n",
      "4500\n",
      "Epoch 1084, Loss: 0.7292850069999695\n",
      "Test Loss after Epoch 1084: 0.8542111270427704\n",
      "4500\n",
      "Epoch 1085, Loss: 0.7283664616213904\n",
      "Test Loss after Epoch 1085: 0.8553329637050628\n",
      "4500\n",
      "Epoch 1086, Loss: 0.7291522496011522\n",
      "Test Loss after Epoch 1086: 0.856414546251297\n",
      "4500\n",
      "Epoch 1087, Loss: 0.7279764155546824\n",
      "Test Loss after Epoch 1087: 0.8597437562942505\n",
      "4500\n",
      "Epoch 1088, Loss: 0.7296843224366506\n",
      "Test Loss after Epoch 1088: 0.8546731572151184\n",
      "4500\n",
      "Epoch 1089, Loss: 0.728169427924686\n",
      "Test Loss after Epoch 1089: 0.852585523366928\n",
      "4500\n",
      "Epoch 1090, Loss: 0.7279754251374139\n",
      "Test Loss after Epoch 1090: 0.8550672469139099\n",
      "4500\n",
      "Epoch 1091, Loss: 0.7288242643939125\n",
      "Test Loss after Epoch 1091: 0.8536154942512513\n",
      "4500\n",
      "Epoch 1092, Loss: 0.728808456076516\n",
      "Test Loss after Epoch 1092: 0.853119835615158\n",
      "4500\n",
      "Epoch 1093, Loss: 0.72854008491834\n",
      "Test Loss after Epoch 1093: 0.8562105975151062\n",
      "4500\n",
      "Epoch 1094, Loss: 0.7282748761971791\n",
      "Test Loss after Epoch 1094: 0.8623448719978333\n",
      "4500\n",
      "Epoch 1095, Loss: 0.728588613960478\n",
      "Test Loss after Epoch 1095: 0.8658390896320343\n",
      "4500\n",
      "Epoch 1096, Loss: 0.7285222469700707\n",
      "Test Loss after Epoch 1096: 0.8570486464500428\n",
      "4500\n",
      "Epoch 1097, Loss: 0.7277936985757616\n",
      "Test Loss after Epoch 1097: 0.8508700537681579\n",
      "4500\n",
      "Epoch 1098, Loss: 0.7281035916805267\n",
      "Test Loss after Epoch 1098: 0.8478086788654328\n",
      "4500\n",
      "Epoch 1099, Loss: 0.7286997422642179\n",
      "Test Loss after Epoch 1099: 0.8443134036064148\n",
      "4500\n",
      "Epoch 1100, Loss: 0.7288040515316857\n",
      "Test Loss after Epoch 1100: 0.8458035094738007\n",
      "4500\n",
      "Epoch 1101, Loss: 0.7277776454024845\n",
      "Test Loss after Epoch 1101: 0.852533236503601\n",
      "4500\n",
      "Epoch 1102, Loss: 0.7288191648059421\n",
      "Test Loss after Epoch 1102: 0.852247526884079\n",
      "4500\n",
      "Epoch 1103, Loss: 0.7288440948327383\n",
      "Test Loss after Epoch 1103: 0.8469347453117371\n",
      "4500\n",
      "Epoch 1104, Loss: 0.7279711053901249\n",
      "Test Loss after Epoch 1104: 0.8476370429992676\n",
      "4500\n",
      "Epoch 1105, Loss: 0.7274778903325398\n",
      "Test Loss after Epoch 1105: 0.8513135159015656\n",
      "4500\n",
      "Epoch 1106, Loss: 0.7284324283070035\n",
      "Test Loss after Epoch 1106: 0.848878466129303\n",
      "4500\n",
      "Epoch 1107, Loss: 0.7272787547641331\n",
      "Test Loss after Epoch 1107: 0.8494659595489502\n",
      "4500\n",
      "Epoch 1108, Loss: 0.7273341955873701\n",
      "Test Loss after Epoch 1108: 0.8499166748523712\n",
      "4500\n",
      "Epoch 1109, Loss: 0.7274998158084022\n",
      "Test Loss after Epoch 1109: 0.8509942348003388\n",
      "4500\n",
      "Epoch 1110, Loss: 0.727937938981586\n",
      "Test Loss after Epoch 1110: 0.8554952864646912\n",
      "4500\n",
      "Epoch 1111, Loss: 0.7287457845740848\n",
      "Test Loss after Epoch 1111: 0.8539214386940003\n",
      "4500\n",
      "Epoch 1112, Loss: 0.7292522258493636\n",
      "Test Loss after Epoch 1112: 0.8522773396968841\n",
      "4500\n",
      "Epoch 1113, Loss: 0.727878768020206\n",
      "Test Loss after Epoch 1113: 0.8529407262802124\n",
      "4500\n",
      "Epoch 1114, Loss: 0.7282404648198022\n",
      "Test Loss after Epoch 1114: 0.851452998638153\n",
      "4500\n",
      "Epoch 1115, Loss: 0.729334005329344\n",
      "Test Loss after Epoch 1115: 0.8522809598445892\n",
      "4500\n",
      "Epoch 1116, Loss: 0.727230553123686\n",
      "Test Loss after Epoch 1116: 0.8545719332695008\n",
      "4500\n",
      "Epoch 1117, Loss: 0.727776014857822\n",
      "Test Loss after Epoch 1117: 0.8533685426712037\n",
      "4500\n",
      "Epoch 1118, Loss: 0.7270067765977648\n",
      "Test Loss after Epoch 1118: 0.848475605726242\n",
      "4500\n",
      "Epoch 1119, Loss: 0.7281644175317552\n",
      "Test Loss after Epoch 1119: 0.8505625321865081\n",
      "4500\n",
      "Epoch 1120, Loss: 0.7294154812494914\n",
      "Test Loss after Epoch 1120: 0.8539687669277192\n",
      "4500\n",
      "Epoch 1121, Loss: 0.7281660876539019\n",
      "Test Loss after Epoch 1121: 0.8558517558574676\n",
      "4500\n",
      "Epoch 1122, Loss: 0.7279874488247765\n",
      "Test Loss after Epoch 1122: 0.8577455718517304\n",
      "4500\n",
      "Epoch 1123, Loss: 0.7285708509286245\n",
      "Test Loss after Epoch 1123: 0.8556605319976807\n",
      "4500\n",
      "Epoch 1124, Loss: 0.7281875072320302\n",
      "Test Loss after Epoch 1124: 0.8511125192642212\n",
      "4500\n",
      "Epoch 1125, Loss: 0.7278423924446106\n",
      "Test Loss after Epoch 1125: 0.8468212559223175\n",
      "4500\n",
      "Epoch 1126, Loss: 0.7286064355903201\n",
      "Test Loss after Epoch 1126: 0.8447288544178009\n",
      "4500\n",
      "Epoch 1127, Loss: 0.7271731361018287\n",
      "Test Loss after Epoch 1127: 0.8434598026275635\n",
      "4500\n",
      "Epoch 1128, Loss: 0.728207196633021\n",
      "Test Loss after Epoch 1128: 0.8463216474056244\n",
      "4500\n",
      "Epoch 1129, Loss: 0.7276033273537954\n",
      "Test Loss after Epoch 1129: 0.850306057214737\n",
      "4500\n",
      "Epoch 1130, Loss: 0.7278399888674418\n",
      "Test Loss after Epoch 1130: 0.8571312932968139\n",
      "4500\n",
      "Epoch 1131, Loss: 0.7278205414083269\n",
      "Test Loss after Epoch 1131: 0.86088019323349\n",
      "4500\n",
      "Epoch 1132, Loss: 0.7277924209965599\n",
      "Test Loss after Epoch 1132: 0.8599916334152222\n",
      "4500\n",
      "Epoch 1133, Loss: 0.7282894332938724\n",
      "Test Loss after Epoch 1133: 0.8522731282711029\n",
      "4500\n",
      "Epoch 1134, Loss: 0.7281678362687429\n",
      "Test Loss after Epoch 1134: 0.8460951247215271\n",
      "4500\n",
      "Epoch 1135, Loss: 0.7278031639258067\n",
      "Test Loss after Epoch 1135: 0.8401042928695679\n",
      "4500\n",
      "Epoch 1136, Loss: 0.7286853971216414\n",
      "Test Loss after Epoch 1136: 0.8362547976970672\n",
      "4500\n",
      "Epoch 1137, Loss: 0.7285280361705356\n",
      "Test Loss after Epoch 1137: 0.8389281742572784\n",
      "4500\n",
      "Epoch 1138, Loss: 0.7275619581805335\n",
      "Test Loss after Epoch 1138: 0.8461047194004059\n",
      "4500\n",
      "Epoch 1139, Loss: 0.7271528646416134\n",
      "Test Loss after Epoch 1139: 0.8518976354598999\n",
      "4500\n",
      "Epoch 1140, Loss: 0.7277726383474138\n",
      "Test Loss after Epoch 1140: 0.8516875829696655\n",
      "4500\n",
      "Epoch 1141, Loss: 0.7283757256401909\n",
      "Test Loss after Epoch 1141: 0.8484920582771301\n",
      "4500\n",
      "Epoch 1142, Loss: 0.7279826293256547\n",
      "Test Loss after Epoch 1142: 0.8467833631038666\n",
      "4500\n",
      "Epoch 1143, Loss: 0.7279545594056447\n",
      "Test Loss after Epoch 1143: 0.8456374826431274\n",
      "4500\n",
      "Epoch 1144, Loss: 0.7278766137229071\n",
      "Test Loss after Epoch 1144: 0.8422273316383362\n",
      "4500\n",
      "Epoch 1145, Loss: 0.7277788382371266\n",
      "Test Loss after Epoch 1145: 0.8397616758346558\n",
      "4500\n",
      "Epoch 1146, Loss: 0.7277873538335164\n",
      "Test Loss after Epoch 1146: 0.8428055558204651\n",
      "4500\n",
      "Epoch 1147, Loss: 0.7286367538505131\n",
      "Test Loss after Epoch 1147: 0.8444296655654907\n",
      "4500\n",
      "Epoch 1148, Loss: 0.7283009289635552\n",
      "Test Loss after Epoch 1148: 0.8438640961647034\n",
      "4500\n",
      "Epoch 1149, Loss: 0.7278841249412961\n",
      "Test Loss after Epoch 1149: 0.846787802696228\n",
      "4500\n",
      "Epoch 1150, Loss: 0.7280184201664395\n",
      "Test Loss after Epoch 1150: 0.8489499387741088\n",
      "4500\n",
      "Epoch 1151, Loss: 0.7282227839893765\n",
      "Test Loss after Epoch 1151: 0.8486275632381439\n",
      "4500\n",
      "Epoch 1152, Loss: 0.7272405033641391\n",
      "Test Loss after Epoch 1152: 0.8519911930561066\n",
      "4500\n",
      "Epoch 1153, Loss: 0.7280131727854411\n",
      "Test Loss after Epoch 1153: 0.8504021501541138\n",
      "4500\n",
      "Epoch 1154, Loss: 0.7284492739041646\n",
      "Test Loss after Epoch 1154: 0.8460885326862335\n",
      "4500\n",
      "Epoch 1155, Loss: 0.7282364575333066\n",
      "Test Loss after Epoch 1155: 0.8456803710460663\n",
      "4500\n",
      "Epoch 1156, Loss: 0.7282609503269195\n",
      "Test Loss after Epoch 1156: 0.846795441865921\n",
      "4500\n",
      "Epoch 1157, Loss: 0.7275814333491856\n",
      "Test Loss after Epoch 1157: 0.8499214570522309\n",
      "4500\n",
      "Epoch 1158, Loss: 0.727756533410814\n",
      "Test Loss after Epoch 1158: 0.8548823070526123\n",
      "Rolling back to best model from epoch 684\n",
      "Best test loss: 0.8256458504199982\n",
      "4500\n",
      "Epoch 1159, Loss: 0.7320517545806037\n",
      "Test Loss after Epoch 1159: 0.8303033664226532\n",
      "4500\n",
      "Epoch 1160, Loss: 0.7307827831374274\n",
      "Test Loss after Epoch 1160: 0.8342908520698548\n",
      "4500\n",
      "Epoch 1161, Loss: 0.7293315521081288\n",
      "Test Loss after Epoch 1161: 0.8354895303249359\n",
      "4500\n",
      "Epoch 1162, Loss: 0.7306163160800934\n",
      "Test Loss after Epoch 1162: 0.8356322054862976\n",
      "4500\n",
      "Epoch 1163, Loss: 0.7305968112680647\n",
      "Test Loss after Epoch 1163: 0.8367781093120575\n",
      "4500\n",
      "Epoch 1164, Loss: 0.7297884553273519\n",
      "Test Loss after Epoch 1164: 0.8376894941329956\n",
      "4500\n",
      "Epoch 1165, Loss: 0.7301387727525499\n",
      "Test Loss after Epoch 1165: 0.8391099634170532\n",
      "4500\n",
      "Epoch 1166, Loss: 0.7306753623220655\n",
      "Test Loss after Epoch 1166: 0.8388268995285034\n",
      "4500\n",
      "Epoch 1167, Loss: 0.730307718594869\n",
      "Test Loss after Epoch 1167: 0.8379245910644532\n",
      "4500\n",
      "Epoch 1168, Loss: 0.7304461693233913\n",
      "Test Loss after Epoch 1168: 0.8382697649002075\n",
      "4500\n",
      "Epoch 1169, Loss: 0.7308068958918253\n",
      "Test Loss after Epoch 1169: 0.8405122821331024\n",
      "4500\n",
      "Epoch 1170, Loss: 0.7309502257506053\n",
      "Test Loss after Epoch 1170: 0.8414293668270111\n",
      "4500\n",
      "Epoch 1171, Loss: 0.7302730096181234\n",
      "Test Loss after Epoch 1171: 0.8412692270278931\n",
      "4500\n",
      "Epoch 1172, Loss: 0.7305390554004245\n",
      "Test Loss after Epoch 1172: 0.8405181818008423\n",
      "4500\n",
      "Epoch 1173, Loss: 0.7296395386324989\n",
      "Test Loss after Epoch 1173: 0.8397285790443421\n",
      "4500\n",
      "Epoch 1174, Loss: 0.7295074168046316\n",
      "Test Loss after Epoch 1174: 0.8410973336696624\n",
      "4500\n",
      "Epoch 1175, Loss: 0.7304088963402642\n",
      "Test Loss after Epoch 1175: 0.8466117947101593\n",
      "4500\n",
      "Epoch 1176, Loss: 0.7301436222129398\n",
      "Test Loss after Epoch 1176: 0.8524233617782593\n",
      "4500\n",
      "Epoch 1177, Loss: 0.7293761976824866\n",
      "Test Loss after Epoch 1177: 0.8522704420089722\n",
      "4500\n",
      "Epoch 1178, Loss: 0.7312490654521518\n",
      "Test Loss after Epoch 1178: 0.8501533291339874\n",
      "4500\n",
      "Epoch 1179, Loss: 0.7304425276915232\n",
      "Test Loss after Epoch 1179: 0.8485995135307312\n",
      "900 4500 500\n",
      "4500\n",
      "Epoch 1, Loss: 4.029203168869018\n",
      "Test Loss after Epoch 1: 3.720374140739441\n",
      "4500\n",
      "Epoch 2, Loss: 3.6893582978778414\n",
      "Test Loss after Epoch 2: 3.4840511178970335\n",
      "4500\n",
      "Epoch 3, Loss: 3.50750014061398\n",
      "Test Loss after Epoch 3: 3.398538837432861\n",
      "4500\n",
      "Epoch 4, Loss: 3.4070007898542616\n",
      "Test Loss after Epoch 4: 3.342926727294922\n",
      "4500\n",
      "Epoch 5, Loss: 3.3425433739556207\n",
      "Test Loss after Epoch 5: 3.297032629966736\n",
      "4500\n",
      "Epoch 6, Loss: 3.2959136080212064\n",
      "Test Loss after Epoch 6: 3.2592595624923706\n",
      "4500\n",
      "Epoch 7, Loss: 3.2494726543426515\n",
      "Test Loss after Epoch 7: 3.202731622695923\n",
      "4500\n",
      "Epoch 8, Loss: 3.1851287483639186\n",
      "Test Loss after Epoch 8: 3.085440128326416\n",
      "4500\n",
      "Epoch 9, Loss: 3.0869954909218684\n",
      "Test Loss after Epoch 9: 2.909603784561157\n",
      "4500\n",
      "Epoch 10, Loss: 2.970970368385315\n",
      "Test Loss after Epoch 10: 2.851137396812439\n",
      "4500\n",
      "Epoch 11, Loss: 2.86199635081821\n",
      "Test Loss after Epoch 11: 2.7633482065200807\n",
      "4500\n",
      "Epoch 12, Loss: 2.770978933546278\n",
      "Test Loss after Epoch 12: 2.698466381072998\n",
      "4500\n",
      "Epoch 13, Loss: 2.6904165711932713\n",
      "Test Loss after Epoch 13: 2.659331501960754\n",
      "4500\n",
      "Epoch 14, Loss: 2.619074675877889\n",
      "Test Loss after Epoch 14: 2.598860701560974\n",
      "4500\n",
      "Epoch 15, Loss: 2.552967382642958\n",
      "Test Loss after Epoch 15: 2.574405415534973\n",
      "4500\n",
      "Epoch 16, Loss: 2.493637454032898\n",
      "Test Loss after Epoch 16: 2.5438060474395754\n",
      "4500\n",
      "Epoch 17, Loss: 2.4444136385387845\n",
      "Test Loss after Epoch 17: 2.5164531412124633\n",
      "4500\n",
      "Epoch 18, Loss: 2.393760520829095\n",
      "Test Loss after Epoch 18: 2.4947921028137205\n",
      "4500\n",
      "Epoch 19, Loss: 2.349451780954997\n",
      "Test Loss after Epoch 19: 2.4692990951538087\n",
      "4500\n",
      "Epoch 20, Loss: 2.313517092704773\n",
      "Test Loss after Epoch 20: 2.4466221504211427\n",
      "4500\n",
      "Epoch 21, Loss: 2.2769491367340087\n",
      "Test Loss after Epoch 21: 2.4206398630142214\n",
      "4500\n",
      "Epoch 22, Loss: 2.2414122483995227\n",
      "Test Loss after Epoch 22: 2.394228094100952\n",
      "4500\n",
      "Epoch 23, Loss: 2.2123268099890816\n",
      "Test Loss after Epoch 23: 2.362840003967285\n",
      "4500\n",
      "Epoch 24, Loss: 2.1835121120876737\n",
      "Test Loss after Epoch 24: 2.3391706790924074\n",
      "4500\n",
      "Epoch 25, Loss: 2.15141641998291\n",
      "Test Loss after Epoch 25: 2.309955695152283\n",
      "4500\n",
      "Epoch 26, Loss: 2.1252506913079157\n",
      "Test Loss after Epoch 26: 2.280372447967529\n",
      "4500\n",
      "Epoch 27, Loss: 2.1015192424986098\n",
      "Test Loss after Epoch 27: 2.2515918226242064\n",
      "4500\n",
      "Epoch 28, Loss: 2.075991726557414\n",
      "Test Loss after Epoch 28: 2.233640587806702\n",
      "4500\n",
      "Epoch 29, Loss: 2.051493038283454\n",
      "Test Loss after Epoch 29: 2.201652689933777\n",
      "4500\n",
      "Epoch 30, Loss: 2.025886311319139\n",
      "Test Loss after Epoch 30: 2.183894599914551\n",
      "4500\n",
      "Epoch 31, Loss: 2.0024151264296637\n",
      "Test Loss after Epoch 31: 2.154221972465515\n",
      "4500\n",
      "Epoch 32, Loss: 1.9772714704937404\n",
      "Test Loss after Epoch 32: 2.1397634859085084\n",
      "4500\n",
      "Epoch 33, Loss: 1.9596796383857726\n",
      "Test Loss after Epoch 33: 2.123739740371704\n",
      "4500\n",
      "Epoch 34, Loss: 1.930161452240414\n",
      "Test Loss after Epoch 34: 2.10606000995636\n",
      "4500\n",
      "Epoch 35, Loss: 1.9110118593639798\n",
      "Test Loss after Epoch 35: 2.0951402673721313\n",
      "4500\n",
      "Epoch 36, Loss: 1.8917025875515407\n",
      "Test Loss after Epoch 36: 2.06672545003891\n",
      "4500\n",
      "Epoch 37, Loss: 1.8683454958068\n",
      "Test Loss after Epoch 37: 2.050194400310516\n",
      "4500\n",
      "Epoch 38, Loss: 1.8484716036054822\n",
      "Test Loss after Epoch 38: 2.047100495815277\n",
      "4500\n",
      "Epoch 39, Loss: 1.8252555344899495\n",
      "Test Loss after Epoch 39: 2.019189703464508\n",
      "4500\n",
      "Epoch 40, Loss: 1.799566556983524\n",
      "Test Loss after Epoch 40: 2.024648006916046\n",
      "4500\n",
      "Epoch 41, Loss: 1.7795076721509298\n",
      "Test Loss after Epoch 41: 2.001840549468994\n",
      "4500\n",
      "Epoch 42, Loss: 1.7606476362016465\n",
      "Test Loss after Epoch 42: 1.9946935806274415\n",
      "4500\n",
      "Epoch 43, Loss: 1.7393224727842542\n",
      "Test Loss after Epoch 43: 1.9649985785484314\n",
      "4500\n",
      "Epoch 44, Loss: 1.7124285092353821\n",
      "Test Loss after Epoch 44: 1.9544162197113037\n",
      "4500\n",
      "Epoch 45, Loss: 1.688099880748325\n",
      "Test Loss after Epoch 45: 1.9151404886245726\n",
      "4500\n",
      "Epoch 46, Loss: 1.6672681856685214\n",
      "Test Loss after Epoch 46: 1.900928141117096\n",
      "4500\n",
      "Epoch 47, Loss: 1.636421471118927\n",
      "Test Loss after Epoch 47: 1.8472027325630187\n",
      "4500\n",
      "Epoch 48, Loss: 1.6095601740413241\n",
      "Test Loss after Epoch 48: 1.8085866470336913\n",
      "4500\n",
      "Epoch 49, Loss: 1.5854311316808065\n",
      "Test Loss after Epoch 49: 1.7677942881584168\n",
      "4500\n",
      "Epoch 50, Loss: 1.5542935616175333\n",
      "Test Loss after Epoch 50: 1.7109624061584472\n",
      "4500\n",
      "Epoch 51, Loss: 1.5303699158562554\n",
      "Test Loss after Epoch 51: 1.6750232753753662\n",
      "4500\n",
      "Epoch 52, Loss: 1.5032690382533602\n",
      "Test Loss after Epoch 52: 1.6139928030967712\n",
      "4500\n",
      "Epoch 53, Loss: 1.4775350830290053\n",
      "Test Loss after Epoch 53: 1.5857423028945923\n",
      "4500\n",
      "Epoch 54, Loss: 1.459498684088389\n",
      "Test Loss after Epoch 54: 1.521049280166626\n",
      "4500\n",
      "Epoch 55, Loss: 1.434712604628669\n",
      "Test Loss after Epoch 55: 1.4973903851509094\n",
      "4500\n",
      "Epoch 56, Loss: 1.417554818312327\n",
      "Test Loss after Epoch 56: 1.455949999809265\n",
      "4500\n",
      "Epoch 57, Loss: 1.3957546047634548\n",
      "Test Loss after Epoch 57: 1.4343588142395018\n",
      "4500\n",
      "Epoch 58, Loss: 1.3670946901639303\n",
      "Test Loss after Epoch 58: 1.3933250942230224\n",
      "4500\n",
      "Epoch 59, Loss: 1.3532019787364535\n",
      "Test Loss after Epoch 59: 1.3689952187538148\n",
      "4500\n",
      "Epoch 60, Loss: 1.3324402525689867\n",
      "Test Loss after Epoch 60: 1.3471349444389342\n",
      "4500\n",
      "Epoch 61, Loss: 1.3062620958752102\n",
      "Test Loss after Epoch 61: 1.3250142908096314\n",
      "4500\n",
      "Epoch 62, Loss: 1.284794028494093\n",
      "Test Loss after Epoch 62: 1.2992942261695861\n",
      "4500\n",
      "Epoch 63, Loss: 1.268677644835578\n",
      "Test Loss after Epoch 63: 1.2810216598510742\n",
      "4500\n",
      "Epoch 64, Loss: 1.2421287922859192\n",
      "Test Loss after Epoch 64: 1.2546153812408447\n",
      "4500\n",
      "Epoch 65, Loss: 1.2258420089615716\n",
      "Test Loss after Epoch 65: 1.2527754459381104\n",
      "4500\n",
      "Epoch 66, Loss: 1.209615692668491\n",
      "Test Loss after Epoch 66: 1.2211904311180115\n",
      "4500\n",
      "Epoch 67, Loss: 1.1902876336839463\n",
      "Test Loss after Epoch 67: 1.2198328647613526\n",
      "4500\n",
      "Epoch 68, Loss: 1.1741590258280437\n",
      "Test Loss after Epoch 68: 1.207346417427063\n",
      "4500\n",
      "Epoch 69, Loss: 1.157207385381063\n",
      "Test Loss after Epoch 69: 1.191535915851593\n",
      "4500\n",
      "Epoch 70, Loss: 1.1412917566829257\n",
      "Test Loss after Epoch 70: 1.1814089150428773\n",
      "4500\n",
      "Epoch 71, Loss: 1.1198264673550924\n",
      "Test Loss after Epoch 71: 1.1660149412155152\n",
      "4500\n",
      "Epoch 72, Loss: 1.1109284998046027\n",
      "Test Loss after Epoch 72: 1.1584542922973633\n",
      "4500\n",
      "Epoch 73, Loss: 1.0961628223525153\n",
      "Test Loss after Epoch 73: 1.1419447765350341\n",
      "4500\n",
      "Epoch 74, Loss: 1.0788894362449646\n",
      "Test Loss after Epoch 74: 1.125094922065735\n",
      "4500\n",
      "Epoch 75, Loss: 1.0692468011114333\n",
      "Test Loss after Epoch 75: 1.1230131831169128\n",
      "4500\n",
      "Epoch 76, Loss: 1.0616237172550624\n",
      "Test Loss after Epoch 76: 1.1149520502090453\n",
      "4500\n",
      "Epoch 77, Loss: 1.0468667322264777\n",
      "Test Loss after Epoch 77: 1.1221975283622743\n",
      "4500\n",
      "Epoch 78, Loss: 1.0339031360944113\n",
      "Test Loss after Epoch 78: 1.1155574340820313\n",
      "4500\n",
      "Epoch 79, Loss: 1.0235228504604763\n",
      "Test Loss after Epoch 79: 1.1019688854217529\n",
      "4500\n",
      "Epoch 80, Loss: 1.0155840942064922\n",
      "Test Loss after Epoch 80: 1.0953238911628722\n",
      "4500\n",
      "Epoch 81, Loss: 1.0068847307629056\n",
      "Test Loss after Epoch 81: 1.0837418246269226\n",
      "4500\n",
      "Epoch 82, Loss: 0.9978809656302134\n",
      "Test Loss after Epoch 82: 1.086176703929901\n",
      "4500\n",
      "Epoch 83, Loss: 0.9880418338510725\n",
      "Test Loss after Epoch 83: 1.0764144477844237\n",
      "4500\n",
      "Epoch 84, Loss: 0.9795826163556841\n",
      "Test Loss after Epoch 84: 1.0597420125007628\n",
      "4500\n",
      "Epoch 85, Loss: 0.971766668425666\n",
      "Test Loss after Epoch 85: 1.053698588848114\n",
      "4500\n",
      "Epoch 86, Loss: 0.9620282413164775\n",
      "Test Loss after Epoch 86: 1.0507109756469726\n",
      "4500\n",
      "Epoch 87, Loss: 0.9545378531879849\n",
      "Test Loss after Epoch 87: 1.0355856909751893\n",
      "4500\n",
      "Epoch 88, Loss: 0.9469781273470984\n",
      "Test Loss after Epoch 88: 1.036498941421509\n",
      "4500\n",
      "Epoch 89, Loss: 0.9393988664150238\n",
      "Test Loss after Epoch 89: 1.0342448964118958\n",
      "4500\n",
      "Epoch 90, Loss: 0.932521682050493\n",
      "Test Loss after Epoch 90: 1.0235002303123475\n",
      "4500\n",
      "Epoch 91, Loss: 0.927908402495914\n",
      "Test Loss after Epoch 91: 1.0013346858024597\n",
      "4500\n",
      "Epoch 92, Loss: 0.9247729133235084\n",
      "Test Loss after Epoch 92: 0.9977045295238495\n",
      "4500\n",
      "Epoch 93, Loss: 0.9175564897060394\n",
      "Test Loss after Epoch 93: 0.9902462191581726\n",
      "4500\n",
      "Epoch 94, Loss: 0.908769924322764\n",
      "Test Loss after Epoch 94: 0.9985066409111023\n",
      "4500\n",
      "Epoch 95, Loss: 0.9050806420644124\n",
      "Test Loss after Epoch 95: 1.0014146938323976\n",
      "4500\n",
      "Epoch 96, Loss: 0.900312327755822\n",
      "Test Loss after Epoch 96: 0.9846845507621765\n",
      "4500\n",
      "Epoch 97, Loss: 0.8960066013601091\n",
      "Test Loss after Epoch 97: 0.9861757867336273\n",
      "4500\n",
      "Epoch 98, Loss: 0.890358621570799\n",
      "Test Loss after Epoch 98: 0.9690278327465057\n",
      "4500\n",
      "Epoch 99, Loss: 0.8832047927644517\n",
      "Test Loss after Epoch 99: 0.9770168759822846\n",
      "4500\n",
      "Epoch 100, Loss: 0.8790689854092069\n",
      "Test Loss after Epoch 100: 0.9572767119407654\n",
      "4500\n",
      "Epoch 101, Loss: 0.8757121582296159\n",
      "Test Loss after Epoch 101: 0.9575305049419403\n",
      "4500\n",
      "Epoch 102, Loss: 0.8723637173175812\n",
      "Test Loss after Epoch 102: 0.9496331052780151\n",
      "4500\n",
      "Epoch 103, Loss: 0.8677268976635403\n",
      "Test Loss after Epoch 103: 0.9450046465396881\n",
      "4500\n",
      "Epoch 104, Loss: 0.8607776216136085\n",
      "Test Loss after Epoch 104: 0.9367494668960571\n",
      "4500\n",
      "Epoch 105, Loss: 0.8592290456295013\n",
      "Test Loss after Epoch 105: 0.9508500835895538\n",
      "4500\n",
      "Epoch 106, Loss: 0.8547863297727373\n",
      "Test Loss after Epoch 106: 0.9569624264240265\n",
      "4500\n",
      "Epoch 107, Loss: 0.8538672642707825\n",
      "Test Loss after Epoch 107: 0.9540248174667358\n",
      "4500\n",
      "Epoch 108, Loss: 0.8520893923441569\n",
      "Test Loss after Epoch 108: 0.9508310227394104\n",
      "4500\n",
      "Epoch 109, Loss: 0.845380271964603\n",
      "Test Loss after Epoch 109: 0.9368618292808533\n",
      "4500\n",
      "Epoch 110, Loss: 0.8502715709209442\n",
      "Test Loss after Epoch 110: 0.9316955459117889\n",
      "4500\n",
      "Epoch 111, Loss: 0.8409742374949986\n",
      "Test Loss after Epoch 111: 0.9471955692768097\n",
      "4500\n",
      "Epoch 112, Loss: 0.8466322934627533\n",
      "Test Loss after Epoch 112: 0.9471976163387299\n",
      "4500\n",
      "Epoch 113, Loss: 0.8422918121284909\n",
      "Test Loss after Epoch 113: 0.9400757513046265\n",
      "4500\n",
      "Epoch 114, Loss: 0.8423788260618845\n",
      "Test Loss after Epoch 114: 0.9281209838390351\n",
      "4500\n",
      "Epoch 115, Loss: 0.83703657690684\n",
      "Test Loss after Epoch 115: 0.914453439950943\n",
      "4500\n",
      "Epoch 116, Loss: 0.8366014824708303\n",
      "Test Loss after Epoch 116: 0.913793747663498\n",
      "4500\n",
      "Epoch 117, Loss: 0.8339531740877364\n",
      "Test Loss after Epoch 117: 0.9210743579864502\n",
      "4500\n",
      "Epoch 118, Loss: 0.8294826458825005\n",
      "Test Loss after Epoch 118: 0.9310766861438752\n",
      "4500\n",
      "Epoch 119, Loss: 0.8287002542018891\n",
      "Test Loss after Epoch 119: 0.9221502676010132\n",
      "4500\n",
      "Epoch 120, Loss: 0.8271403961446551\n",
      "Test Loss after Epoch 120: 0.9301119546890259\n",
      "4500\n",
      "Epoch 121, Loss: 0.8227329793506198\n",
      "Test Loss after Epoch 121: 0.9419270443916321\n",
      "4500\n",
      "Epoch 122, Loss: 0.8256512752109104\n",
      "Test Loss after Epoch 122: 0.9210554728507996\n",
      "4500\n",
      "Epoch 123, Loss: 0.8236858931117588\n",
      "Test Loss after Epoch 123: 0.9219942092895508\n",
      "4500\n",
      "Epoch 124, Loss: 0.8180615485774146\n",
      "Test Loss after Epoch 124: 0.9263811450004578\n",
      "4500\n",
      "Epoch 125, Loss: 0.8159325438605415\n",
      "Test Loss after Epoch 125: 0.9058560223579407\n",
      "4500\n",
      "Epoch 126, Loss: 0.8158988681369358\n",
      "Test Loss after Epoch 126: 0.908200487613678\n",
      "4500\n",
      "Epoch 127, Loss: 0.8147912584145864\n",
      "Test Loss after Epoch 127: 0.9123475337028504\n",
      "4500\n",
      "Epoch 128, Loss: 0.8138442614343431\n",
      "Test Loss after Epoch 128: 0.9015452420711517\n",
      "4500\n",
      "Epoch 129, Loss: 0.8118984526263343\n",
      "Test Loss after Epoch 129: 0.904058671951294\n",
      "4500\n",
      "Epoch 130, Loss: 0.8059336334334479\n",
      "Test Loss after Epoch 130: 0.9057077815532685\n",
      "4500\n",
      "Epoch 131, Loss: 0.8093680387602912\n",
      "Test Loss after Epoch 131: 0.9059662663936615\n",
      "4500\n",
      "Epoch 132, Loss: 0.8072668816248576\n",
      "Test Loss after Epoch 132: 0.899837857246399\n",
      "4500\n",
      "Epoch 133, Loss: 0.8087138132519193\n",
      "Test Loss after Epoch 133: 0.8999998779296875\n",
      "4500\n",
      "Epoch 134, Loss: 0.805653157790502\n",
      "Test Loss after Epoch 134: 0.8897501242160797\n",
      "4500\n",
      "Epoch 135, Loss: 0.8036159156958262\n",
      "Test Loss after Epoch 135: 0.8964631168842315\n",
      "4500\n",
      "Epoch 136, Loss: 0.804781523042255\n",
      "Test Loss after Epoch 136: 0.899561495065689\n",
      "4500\n",
      "Epoch 137, Loss: 0.8056342400974698\n",
      "Test Loss after Epoch 137: 0.9049293475151062\n",
      "4500\n",
      "Epoch 138, Loss: 0.8033000360859764\n",
      "Test Loss after Epoch 138: 0.9066901321411133\n",
      "4500\n",
      "Epoch 139, Loss: 0.8016776917775472\n",
      "Test Loss after Epoch 139: 0.8987036163806915\n",
      "4500\n",
      "Epoch 140, Loss: 0.8017729627026452\n",
      "Test Loss after Epoch 140: 0.9059733994007111\n",
      "4500\n",
      "Epoch 141, Loss: 0.798329702032937\n",
      "Test Loss after Epoch 141: 0.9119844098091126\n",
      "4500\n",
      "Epoch 142, Loss: 0.799064272430208\n",
      "Test Loss after Epoch 142: 0.8987668483257294\n",
      "4500\n",
      "Epoch 143, Loss: 0.8001321593390571\n",
      "Test Loss after Epoch 143: 0.899213063955307\n",
      "4500\n",
      "Epoch 144, Loss: 0.7961627053154839\n",
      "Test Loss after Epoch 144: 0.9103489511013031\n",
      "4500\n",
      "Epoch 145, Loss: 0.7940188372135162\n",
      "Test Loss after Epoch 145: 0.8904946990013123\n",
      "4500\n",
      "Epoch 146, Loss: 0.7982379732661777\n",
      "Test Loss after Epoch 146: 0.891200442314148\n",
      "4500\n",
      "Epoch 147, Loss: 0.7942705951266819\n",
      "Test Loss after Epoch 147: 0.9087213504314423\n",
      "4500\n",
      "Epoch 148, Loss: 0.7967439942889744\n",
      "Test Loss after Epoch 148: 0.8956489474773407\n",
      "4500\n",
      "Epoch 149, Loss: 0.7915031014018589\n",
      "Test Loss after Epoch 149: 0.8915804302692414\n",
      "4500\n",
      "Epoch 150, Loss: 0.794896674686008\n",
      "Test Loss after Epoch 150: 0.9104860427379609\n",
      "4500\n",
      "Epoch 151, Loss: 0.7957798477013905\n",
      "Test Loss after Epoch 151: 0.8982005188465119\n",
      "4500\n",
      "Epoch 152, Loss: 0.7906201391220092\n",
      "Test Loss after Epoch 152: 0.8743087086677551\n",
      "4500\n",
      "Epoch 153, Loss: 0.7905833968586392\n",
      "Test Loss after Epoch 153: 0.8802480099201202\n",
      "4500\n",
      "Epoch 154, Loss: 0.7888655569818285\n",
      "Test Loss after Epoch 154: 0.8903870160579681\n",
      "4500\n",
      "Epoch 155, Loss: 0.7896393373542362\n",
      "Test Loss after Epoch 155: 0.8755564007759095\n",
      "4500\n",
      "Epoch 156, Loss: 0.7918364139927758\n",
      "Test Loss after Epoch 156: 0.8800794081687927\n",
      "4500\n",
      "Epoch 157, Loss: 0.7865052239894867\n",
      "Test Loss after Epoch 157: 0.8908168992996216\n",
      "4500\n",
      "Epoch 158, Loss: 0.7865870952606201\n",
      "Test Loss after Epoch 158: 0.8831527931690216\n",
      "4500\n",
      "Epoch 159, Loss: 0.7884283011224534\n",
      "Test Loss after Epoch 159: 0.8801711668968201\n",
      "4500\n",
      "Epoch 160, Loss: 0.7868942507108052\n",
      "Test Loss after Epoch 160: 0.8830262100696564\n",
      "4500\n",
      "Epoch 161, Loss: 0.7839127802054088\n",
      "Test Loss after Epoch 161: 0.8829619925022125\n",
      "4500\n",
      "Epoch 162, Loss: 0.7848733774291144\n",
      "Test Loss after Epoch 162: 0.8921030328273774\n",
      "4500\n",
      "Epoch 163, Loss: 0.7845021568934123\n",
      "Test Loss after Epoch 163: 0.8833698468208313\n",
      "4500\n",
      "Epoch 164, Loss: 0.7833441065947214\n",
      "Test Loss after Epoch 164: 0.8847643616199493\n",
      "4500\n",
      "Epoch 165, Loss: 0.7830976085133022\n",
      "Test Loss after Epoch 165: 0.8714635379314423\n",
      "4500\n",
      "Epoch 166, Loss: 0.7829615508450403\n",
      "Test Loss after Epoch 166: 0.8779047765731811\n",
      "4500\n",
      "Epoch 167, Loss: 0.7810052586131626\n",
      "Test Loss after Epoch 167: 0.8847032978534698\n",
      "4500\n",
      "Epoch 168, Loss: 0.7814457527001699\n",
      "Test Loss after Epoch 168: 0.8832878830432892\n",
      "4500\n",
      "Epoch 169, Loss: 0.7799339330726199\n",
      "Test Loss after Epoch 169: 0.8859250621795655\n",
      "4500\n",
      "Epoch 170, Loss: 0.7825070359971789\n",
      "Test Loss after Epoch 170: 0.8812098155021667\n",
      "4500\n",
      "Epoch 171, Loss: 0.7787039396233029\n",
      "Test Loss after Epoch 171: 0.8836011052131653\n",
      "4500\n",
      "Epoch 172, Loss: 0.7813342077996995\n",
      "Test Loss after Epoch 172: 0.8929457936286926\n",
      "4500\n",
      "Epoch 173, Loss: 0.78106551861763\n",
      "Test Loss after Epoch 173: 0.8894150309562683\n",
      "4500\n",
      "Epoch 174, Loss: 0.7780539271831513\n",
      "Test Loss after Epoch 174: 0.8919877967834473\n",
      "4500\n",
      "Epoch 175, Loss: 0.7782425220277575\n",
      "Test Loss after Epoch 175: 0.8910115048885345\n",
      "4500\n",
      "Epoch 176, Loss: 0.7804328483740489\n",
      "Test Loss after Epoch 176: 0.8811079227924347\n",
      "4500\n",
      "Epoch 177, Loss: 0.7790163146389856\n",
      "Test Loss after Epoch 177: 0.8848359718322754\n",
      "4500\n",
      "Epoch 178, Loss: 0.7779881100389693\n",
      "Test Loss after Epoch 178: 0.8849479174613952\n",
      "4500\n",
      "Epoch 179, Loss: 0.7767314922014872\n",
      "Test Loss after Epoch 179: 0.8983249657154083\n",
      "4500\n",
      "Epoch 180, Loss: 0.7771292723814647\n",
      "Test Loss after Epoch 180: 0.9004202198982239\n",
      "4500\n",
      "Epoch 181, Loss: 0.7758435800870259\n",
      "Test Loss after Epoch 181: 0.8932401316165924\n",
      "4500\n",
      "Epoch 182, Loss: 0.7781075147787729\n",
      "Test Loss after Epoch 182: 0.8951599359512329\n",
      "4500\n",
      "Epoch 183, Loss: 0.7771871226893531\n",
      "Test Loss after Epoch 183: 0.8957016236782074\n",
      "4500\n",
      "Epoch 184, Loss: 0.7757807993094127\n",
      "Test Loss after Epoch 184: 0.8875364434719085\n",
      "4500\n",
      "Epoch 185, Loss: 0.7745155996746487\n",
      "Test Loss after Epoch 185: 0.8821591970920563\n",
      "Rolling back to best model from epoch 165\n",
      "Best test loss: 0.8714635379314423\n",
      "4500\n",
      "Epoch 186, Loss: 0.780329685555564\n",
      "Test Loss after Epoch 186: 0.8773402395248413\n",
      "4500\n",
      "Epoch 187, Loss: 0.7785229576428732\n",
      "Test Loss after Epoch 187: 0.8773525724411011\n",
      "4500\n",
      "Epoch 188, Loss: 0.7799364472760094\n",
      "Test Loss after Epoch 188: 0.8753279674053193\n",
      "4500\n",
      "Epoch 189, Loss: 0.7808344876236386\n",
      "Test Loss after Epoch 189: 0.8803688421249389\n",
      "4500\n",
      "Epoch 190, Loss: 0.7752013082504272\n",
      "Test Loss after Epoch 190: 0.8759636323451996\n",
      "4500\n",
      "Epoch 191, Loss: 0.7793027452362908\n",
      "Test Loss after Epoch 191: 0.8775722546577454\n",
      "4500\n",
      "Epoch 192, Loss: 0.7747963171799978\n",
      "Test Loss after Epoch 192: 0.8857275614738465\n",
      "4500\n",
      "Epoch 193, Loss: 0.7754010694026947\n",
      "Test Loss after Epoch 193: 0.8641177909374237\n",
      "4500\n",
      "Epoch 194, Loss: 0.7790011870331235\n",
      "Test Loss after Epoch 194: 0.8641813986301422\n",
      "4500\n",
      "Epoch 195, Loss: 0.7759557789431678\n",
      "Test Loss after Epoch 195: 0.8793108043670654\n",
      "4500\n",
      "Epoch 196, Loss: 0.777140101618237\n",
      "Test Loss after Epoch 196: 0.8842495474815368\n",
      "4500\n",
      "Epoch 197, Loss: 0.7749562845495012\n",
      "Test Loss after Epoch 197: 0.8877095658779144\n",
      "4500\n",
      "Epoch 198, Loss: 0.7782535055213504\n",
      "Test Loss after Epoch 198: 0.8930384066104889\n",
      "4500\n",
      "Epoch 199, Loss: 0.7741771101951599\n",
      "Test Loss after Epoch 199: 0.8905005621910095\n",
      "4500\n",
      "Epoch 200, Loss: 0.7772761551804013\n",
      "Test Loss after Epoch 200: 0.8881233949661255\n",
      "4500\n",
      "Epoch 201, Loss: 0.7744119709332784\n",
      "Test Loss after Epoch 201: 0.8989581401348115\n",
      "4500\n",
      "Epoch 202, Loss: 0.7785442134539287\n",
      "Test Loss after Epoch 202: 0.8859606568813324\n",
      "4500\n",
      "Epoch 203, Loss: 0.7789054036140441\n",
      "Test Loss after Epoch 203: 0.8856235299110412\n",
      "4500\n",
      "Epoch 204, Loss: 0.7800819291008844\n",
      "Test Loss after Epoch 204: 0.8892559192180634\n",
      "4500\n",
      "Epoch 205, Loss: 0.7745848352909088\n",
      "Test Loss after Epoch 205: 0.8881319057941437\n",
      "4500\n",
      "Epoch 206, Loss: 0.7750643224716186\n",
      "Test Loss after Epoch 206: 0.8850631723403931\n",
      "4500\n",
      "Epoch 207, Loss: 0.7769365155961778\n",
      "Test Loss after Epoch 207: 0.893592549085617\n",
      "4500\n",
      "Epoch 208, Loss: 0.7750064928001827\n",
      "Test Loss after Epoch 208: 0.8800315470695496\n",
      "4500\n",
      "Epoch 209, Loss: 0.777140864610672\n",
      "Test Loss after Epoch 209: 0.8744710643291473\n",
      "4500\n",
      "Epoch 210, Loss: 0.7765887730651432\n",
      "Test Loss after Epoch 210: 0.8760641748905182\n",
      "4500\n",
      "Epoch 211, Loss: 0.7750275468561384\n",
      "Test Loss after Epoch 211: 0.8759298238754273\n",
      "4500\n",
      "Epoch 212, Loss: 0.7741079291767544\n",
      "Test Loss after Epoch 212: 0.8740382051467895\n",
      "4500\n",
      "Epoch 213, Loss: 0.7734994293583763\n",
      "Test Loss after Epoch 213: 0.8632207236289978\n",
      "4500\n",
      "Epoch 214, Loss: 0.7766979280312856\n",
      "Test Loss after Epoch 214: 0.8555399351119995\n",
      "4500\n",
      "Epoch 215, Loss: 0.7758621017403072\n",
      "Test Loss after Epoch 215: 0.8659657270908355\n",
      "4500\n",
      "Epoch 216, Loss: 0.773411278963089\n",
      "Test Loss after Epoch 216: 0.8737389326095581\n",
      "4500\n",
      "Epoch 217, Loss: 0.7729654809633891\n",
      "Test Loss after Epoch 217: 0.8740838084220887\n",
      "4500\n",
      "Epoch 218, Loss: 0.7718583887947931\n",
      "Test Loss after Epoch 218: 0.878657947063446\n",
      "4500\n",
      "Epoch 219, Loss: 0.7712609419027965\n",
      "Test Loss after Epoch 219: 0.8850307049751281\n",
      "4500\n",
      "Epoch 220, Loss: 0.7701932945516374\n",
      "Test Loss after Epoch 220: 0.8813635120391846\n",
      "4500\n",
      "Epoch 221, Loss: 0.7683199888865153\n",
      "Test Loss after Epoch 221: 0.8956521995067597\n",
      "4500\n",
      "Epoch 222, Loss: 0.7678034934997559\n",
      "Test Loss after Epoch 222: 0.8953608138561249\n",
      "4500\n",
      "Epoch 223, Loss: 0.7693710281319088\n",
      "Test Loss after Epoch 223: 0.8844866731166839\n",
      "4500\n",
      "Epoch 224, Loss: 0.7667157233556111\n",
      "Test Loss after Epoch 224: 0.8741651117801666\n",
      "4500\n",
      "Epoch 225, Loss: 0.7691119067139096\n",
      "Test Loss after Epoch 225: 0.8570770554542542\n",
      "4500\n",
      "Epoch 226, Loss: 0.7673156694041358\n",
      "Test Loss after Epoch 226: 0.8631485011577607\n",
      "4500\n",
      "Epoch 227, Loss: 0.7695994572639465\n",
      "Test Loss after Epoch 227: 0.865054622888565\n",
      "4500\n",
      "Epoch 228, Loss: 0.7667444060378604\n",
      "Test Loss after Epoch 228: 0.8657931215763092\n",
      "4500\n",
      "Epoch 229, Loss: 0.7656611983776093\n",
      "Test Loss after Epoch 229: 0.8549418292045593\n",
      "4500\n",
      "Epoch 230, Loss: 0.7669898296727075\n",
      "Test Loss after Epoch 230: 0.8602919626235962\n",
      "4500\n",
      "Epoch 231, Loss: 0.7700766112009684\n",
      "Test Loss after Epoch 231: 0.8767389032840729\n",
      "4500\n",
      "Epoch 232, Loss: 0.7679562287065718\n",
      "Test Loss after Epoch 232: 0.8766342000961304\n",
      "4500\n",
      "Epoch 233, Loss: 0.7677895032829708\n",
      "Test Loss after Epoch 233: 0.8659884860515594\n",
      "4500\n",
      "Epoch 234, Loss: 0.7672391191323599\n",
      "Test Loss after Epoch 234: 0.8690150723457336\n",
      "4500\n",
      "Epoch 235, Loss: 0.7668902812268998\n",
      "Test Loss after Epoch 235: 0.8630094950199128\n",
      "4500\n",
      "Epoch 236, Loss: 0.766038365761439\n",
      "Test Loss after Epoch 236: 0.8617565214633942\n",
      "4500\n",
      "Epoch 237, Loss: 0.7665269486639235\n",
      "Test Loss after Epoch 237: 0.8771240329742431\n",
      "4500\n",
      "Epoch 238, Loss: 0.768696447081036\n",
      "Test Loss after Epoch 238: 0.8778365471363068\n",
      "4500\n",
      "Epoch 239, Loss: 0.7688105975257026\n",
      "Test Loss after Epoch 239: 0.872975982427597\n",
      "4500\n",
      "Epoch 240, Loss: 0.7701022614373101\n",
      "Test Loss after Epoch 240: 0.8743944110870361\n",
      "4500\n",
      "Epoch 241, Loss: 0.7667928138044146\n",
      "Test Loss after Epoch 241: 0.8825424709320069\n",
      "4500\n",
      "Epoch 242, Loss: 0.7662367097801632\n",
      "Test Loss after Epoch 242: 0.869541610956192\n",
      "4500\n",
      "Epoch 243, Loss: 0.7625604344209035\n",
      "Test Loss after Epoch 243: 0.8607803313732147\n",
      "4500\n",
      "Epoch 244, Loss: 0.7652617706192865\n",
      "Test Loss after Epoch 244: 0.8562372102737427\n",
      "4500\n",
      "Epoch 245, Loss: 0.7666654446654849\n",
      "Test Loss after Epoch 245: 0.8627974781990051\n",
      "4500\n",
      "Epoch 246, Loss: 0.7653273167610168\n",
      "Test Loss after Epoch 246: 0.8641618881225586\n",
      "4500\n",
      "Epoch 247, Loss: 0.764431852499644\n",
      "Test Loss after Epoch 247: 0.8693658237457276\n",
      "4500\n",
      "Epoch 248, Loss: 0.7641167219479879\n",
      "Test Loss after Epoch 248: 0.8717856061458588\n",
      "4500\n",
      "Epoch 249, Loss: 0.765836631377538\n",
      "Test Loss after Epoch 249: 0.872830828666687\n",
      "4500\n",
      "Epoch 250, Loss: 0.7661345057487488\n",
      "Test Loss after Epoch 250: 0.8679629788398743\n",
      "4500\n",
      "Epoch 251, Loss: 0.7641092842684851\n",
      "Test Loss after Epoch 251: 0.8666038796901703\n",
      "4500\n",
      "Epoch 252, Loss: 0.763513618045383\n",
      "Test Loss after Epoch 252: 0.8687210721969605\n",
      "4500\n",
      "Epoch 253, Loss: 0.7650065525637733\n",
      "Test Loss after Epoch 253: 0.86196399974823\n",
      "4500\n",
      "Epoch 254, Loss: 0.7622108502917819\n",
      "Test Loss after Epoch 254: 0.8649130103588104\n",
      "4500\n",
      "Epoch 255, Loss: 0.7644808424313864\n",
      "Test Loss after Epoch 255: 0.8799615564346314\n",
      "4500\n",
      "Epoch 256, Loss: 0.7644973805215624\n",
      "Test Loss after Epoch 256: 0.8666042580604553\n",
      "4500\n",
      "Epoch 257, Loss: 0.7641810122860803\n",
      "Test Loss after Epoch 257: 0.8809143657684326\n",
      "Rolling back to best model from epoch 229\n",
      "Best test loss: 0.8549418292045593\n",
      "4500\n",
      "Epoch 258, Loss: 0.7630663311481476\n",
      "Test Loss after Epoch 258: 0.8656828105449677\n",
      "4500\n",
      "Epoch 259, Loss: 0.7650911586549547\n",
      "Test Loss after Epoch 259: 0.8772058961391449\n",
      "4500\n",
      "Epoch 260, Loss: 0.7667209388415018\n",
      "Test Loss after Epoch 260: 0.8888496773242951\n",
      "4500\n",
      "Epoch 261, Loss: 0.7631715907785628\n",
      "Test Loss after Epoch 261: 0.8804851286411285\n",
      "4500\n",
      "Epoch 262, Loss: 0.764396206829283\n",
      "Test Loss after Epoch 262: 0.8772943143844605\n",
      "4500\n",
      "Epoch 263, Loss: 0.7651704040633308\n",
      "Test Loss after Epoch 263: 0.8831194624900818\n",
      "4500\n",
      "Epoch 264, Loss: 0.7648063997427622\n",
      "Test Loss after Epoch 264: 0.8874105875492095\n",
      "4500\n",
      "Epoch 265, Loss: 0.7646822123792436\n",
      "Test Loss after Epoch 265: 0.8857185060977936\n",
      "4500\n",
      "Epoch 266, Loss: 0.7666471877098083\n",
      "Test Loss after Epoch 266: 0.8805961756706238\n",
      "4500\n",
      "Epoch 267, Loss: 0.7677655632230971\n",
      "Test Loss after Epoch 267: 0.8831441326141357\n",
      "4500\n",
      "Epoch 268, Loss: 0.7688073340257009\n",
      "Test Loss after Epoch 268: 0.8734974744319915\n",
      "4500\n",
      "Epoch 269, Loss: 0.7670638792514801\n",
      "Test Loss after Epoch 269: 0.8683137333393097\n",
      "4500\n",
      "Epoch 270, Loss: 0.7652835989793142\n",
      "Test Loss after Epoch 270: 0.866190236568451\n",
      "4500\n",
      "Epoch 271, Loss: 0.7662039862208896\n",
      "Test Loss after Epoch 271: 0.8688732507228851\n",
      "4500\n",
      "Epoch 272, Loss: 0.770389274570677\n",
      "Test Loss after Epoch 272: 0.8684791774749756\n",
      "4500\n",
      "Epoch 273, Loss: 0.7677095867527856\n",
      "Test Loss after Epoch 273: 0.8813064215183258\n",
      "4500\n",
      "Epoch 274, Loss: 0.7669932834572262\n",
      "Test Loss after Epoch 274: 0.8751955168247223\n",
      "4500\n",
      "Epoch 275, Loss: 0.7659461879995134\n",
      "Test Loss after Epoch 275: 0.8893708720207214\n",
      "4500\n",
      "Epoch 276, Loss: 0.7666177053186628\n",
      "Test Loss after Epoch 276: 0.8775698740482331\n",
      "4500\n",
      "Epoch 277, Loss: 0.7685524917973412\n",
      "Test Loss after Epoch 277: 0.8733711593151092\n",
      "4500\n",
      "Epoch 278, Loss: 0.7679509548346202\n",
      "Test Loss after Epoch 278: 0.8755446248054505\n",
      "4500\n",
      "Epoch 279, Loss: 0.7667773077752855\n",
      "Test Loss after Epoch 279: 0.8800078537464142\n",
      "4500\n",
      "Epoch 280, Loss: 0.7694750919077131\n",
      "Test Loss after Epoch 280: 0.8662470860481262\n",
      "4500\n",
      "Epoch 281, Loss: 0.7685646314885881\n",
      "Test Loss after Epoch 281: 0.8643141038417816\n",
      "4500\n",
      "Epoch 282, Loss: 0.7666363018088871\n",
      "Test Loss after Epoch 282: 0.8643127937316895\n",
      "4500\n",
      "Epoch 283, Loss: 0.7661786104308235\n",
      "Test Loss after Epoch 283: 0.8728566727638245\n",
      "4500\n",
      "Epoch 284, Loss: 0.7671859179602729\n",
      "Test Loss after Epoch 284: 0.8735350158214569\n",
      "4500\n",
      "Epoch 285, Loss: 0.7655176841417949\n",
      "Test Loss after Epoch 285: 0.846861079454422\n",
      "4500\n",
      "Epoch 286, Loss: 0.7667695506943597\n",
      "Test Loss after Epoch 286: 0.8640568025112152\n",
      "4500\n",
      "Epoch 287, Loss: 0.7693349537319607\n",
      "Test Loss after Epoch 287: 0.8728947792053222\n",
      "4500\n",
      "Epoch 288, Loss: 0.7659987426069048\n",
      "Test Loss after Epoch 288: 0.8578302359580994\n",
      "4500\n",
      "Epoch 289, Loss: 0.7639306435320112\n",
      "Test Loss after Epoch 289: 0.8637788121700287\n",
      "4500\n",
      "Epoch 290, Loss: 0.7636855777899424\n",
      "Test Loss after Epoch 290: 0.8631815526485443\n",
      "4500\n",
      "Epoch 291, Loss: 0.7650569464100732\n",
      "Test Loss after Epoch 291: 0.8622016997337342\n",
      "4500\n",
      "Epoch 292, Loss: 0.7656743102603488\n",
      "Test Loss after Epoch 292: 0.8648768258094788\n",
      "4500\n",
      "Epoch 293, Loss: 0.7661153478622437\n",
      "Test Loss after Epoch 293: 0.8647488107681275\n",
      "4500\n",
      "Epoch 294, Loss: 0.7674400627613067\n",
      "Test Loss after Epoch 294: 0.8689339072704315\n",
      "4500\n",
      "Epoch 295, Loss: 0.766826140138838\n",
      "Test Loss after Epoch 295: 0.8603073561191559\n",
      "4500\n",
      "Epoch 296, Loss: 0.7647931541601817\n",
      "Test Loss after Epoch 296: 0.8654476249217987\n",
      "4500\n",
      "Epoch 297, Loss: 0.7631911515924665\n",
      "Test Loss after Epoch 297: 0.8688845529556274\n",
      "4500\n",
      "Epoch 298, Loss: 0.7631774810949962\n",
      "Test Loss after Epoch 298: 0.879766568183899\n",
      "4500\n",
      "Epoch 299, Loss: 0.7664897187815772\n",
      "Test Loss after Epoch 299: 0.8671380870342255\n",
      "4500\n",
      "Epoch 300, Loss: 0.7675564587646061\n",
      "Test Loss after Epoch 300: 0.8608930013179779\n",
      "4500\n",
      "Epoch 301, Loss: 0.7644343501991696\n",
      "Test Loss after Epoch 301: 0.8668717019557953\n",
      "4500\n",
      "Epoch 302, Loss: 0.7646542216142018\n",
      "Test Loss after Epoch 302: 0.8775735323429108\n",
      "4500\n",
      "Epoch 303, Loss: 0.7652576565477583\n",
      "Test Loss after Epoch 303: 0.8698460283279419\n",
      "4500\n",
      "Epoch 304, Loss: 0.7670891013675266\n",
      "Test Loss after Epoch 304: 0.8671057794094086\n",
      "4500\n",
      "Epoch 305, Loss: 0.7647496070596906\n",
      "Test Loss after Epoch 305: 0.8689630653858185\n",
      "4500\n",
      "Epoch 306, Loss: 0.7631400821208953\n",
      "Test Loss after Epoch 306: 0.8635939450263977\n",
      "4500\n",
      "Epoch 307, Loss: 0.7659381960762872\n",
      "Test Loss after Epoch 307: 0.861482947587967\n",
      "4500\n",
      "Epoch 308, Loss: 0.764551949236128\n",
      "Test Loss after Epoch 308: 0.8597439148426056\n",
      "4500\n",
      "Epoch 309, Loss: 0.7686501211060418\n",
      "Test Loss after Epoch 309: 0.8540083491802215\n",
      "4500\n",
      "Epoch 310, Loss: 0.7632420216401418\n",
      "Test Loss after Epoch 310: 0.8574480955600738\n",
      "4500\n",
      "Epoch 311, Loss: 0.7653922406567467\n",
      "Test Loss after Epoch 311: 0.8564209520816803\n",
      "4500\n",
      "Epoch 312, Loss: 0.7639676101472642\n",
      "Test Loss after Epoch 312: 0.858471684217453\n",
      "4500\n",
      "Epoch 313, Loss: 0.764275738424725\n",
      "Test Loss after Epoch 313: 0.8787912852764129\n",
      "4500\n",
      "Epoch 314, Loss: 0.762840264108446\n",
      "Test Loss after Epoch 314: 0.8768778579235077\n",
      "4500\n",
      "Epoch 315, Loss: 0.7635960205131107\n",
      "Test Loss after Epoch 315: 0.8610100591182709\n",
      "4500\n",
      "Epoch 316, Loss: 0.7636125211980608\n",
      "Test Loss after Epoch 316: 0.8596197335720063\n",
      "4500\n",
      "Epoch 317, Loss: 0.7635779631137848\n",
      "Test Loss after Epoch 317: 0.8672070379257202\n",
      "4500\n",
      "Epoch 318, Loss: 0.7620090404616462\n",
      "Test Loss after Epoch 318: 0.8662076489925384\n",
      "4500\n",
      "Epoch 319, Loss: 0.764276467482249\n",
      "Test Loss after Epoch 319: 0.8507612934112548\n",
      "4500\n",
      "Epoch 320, Loss: 0.7629742733637492\n",
      "Test Loss after Epoch 320: 0.863425930261612\n",
      "4500\n",
      "Epoch 321, Loss: 0.7632453810373943\n",
      "Test Loss after Epoch 321: 0.8625468964576721\n",
      "4500\n",
      "Epoch 322, Loss: 0.7678681168821123\n",
      "Test Loss after Epoch 322: 0.8657509763240814\n",
      "4500\n",
      "Epoch 323, Loss: 0.7648973553975423\n",
      "Test Loss after Epoch 323: 0.8567194654941559\n",
      "4500\n",
      "Epoch 324, Loss: 0.764637752532959\n",
      "Test Loss after Epoch 324: 0.862456102848053\n",
      "Rolling back to best model from epoch 285\n",
      "Best test loss: 0.846861079454422\n",
      "4500\n",
      "Epoch 325, Loss: 0.7644529368082682\n",
      "Test Loss after Epoch 325: 0.8622330932617187\n",
      "4500\n",
      "Epoch 326, Loss: 0.7602045735518138\n",
      "Test Loss after Epoch 326: 0.8677404720783234\n",
      "4500\n",
      "Epoch 327, Loss: 0.7617929060194227\n",
      "Test Loss after Epoch 327: 0.866910044670105\n",
      "4500\n",
      "Epoch 328, Loss: 0.7624734609391954\n",
      "Test Loss after Epoch 328: 0.8742363359928131\n",
      "4500\n",
      "Epoch 329, Loss: 0.7624754859076606\n",
      "Test Loss after Epoch 329: 0.8754580457210541\n",
      "4500\n",
      "Epoch 330, Loss: 0.7641323967774709\n",
      "Test Loss after Epoch 330: 0.8771147973537445\n",
      "4500\n",
      "Epoch 331, Loss: 0.7671710370911492\n",
      "Test Loss after Epoch 331: 0.8693216507434844\n",
      "4500\n",
      "Epoch 332, Loss: 0.7650206498040093\n",
      "Test Loss after Epoch 332: 0.8698180942535401\n",
      "4500\n",
      "Epoch 333, Loss: 0.7673854331440396\n",
      "Test Loss after Epoch 333: 0.868939605474472\n",
      "4500\n",
      "Epoch 334, Loss: 0.7680517864757114\n",
      "Test Loss after Epoch 334: 0.8653034272193909\n",
      "4500\n",
      "Epoch 335, Loss: 0.766830437792672\n",
      "Test Loss after Epoch 335: 0.8549282214641571\n",
      "9000\n",
      "Epoch 336, Loss: 0.9443036023113462\n",
      "Test Loss after Epoch 336: 0.8305004215240479\n",
      "9000\n",
      "Epoch 337, Loss: 0.8844341498083539\n",
      "Test Loss after Epoch 337: 0.8704414134025573\n",
      "9000\n",
      "Epoch 338, Loss: 0.8683945580455992\n",
      "Test Loss after Epoch 338: 0.8447636468410492\n",
      "9000\n",
      "Epoch 339, Loss: 0.847006820903884\n",
      "Test Loss after Epoch 339: 0.8299140498638153\n",
      "9000\n",
      "Epoch 340, Loss: 0.8391998752885395\n",
      "Test Loss after Epoch 340: 0.8389131991863251\n",
      "9000\n",
      "Epoch 341, Loss: 0.8369140766329235\n",
      "Test Loss after Epoch 341: 0.8378205213546753\n",
      "9000\n",
      "Epoch 342, Loss: 0.828406705684132\n",
      "Test Loss after Epoch 342: 0.8363686680793763\n",
      "9000\n",
      "Epoch 343, Loss: 0.8220081109603246\n",
      "Test Loss after Epoch 343: 0.8444789521694184\n",
      "9000\n",
      "Epoch 344, Loss: 0.8195278146664302\n",
      "Test Loss after Epoch 344: 0.8314606623649597\n",
      "9000\n",
      "Epoch 345, Loss: 0.8154149815109041\n",
      "Test Loss after Epoch 345: 0.8244058611392975\n",
      "9000\n",
      "Epoch 346, Loss: 0.8150301707850562\n",
      "Test Loss after Epoch 346: 0.8360699453353881\n",
      "9000\n",
      "Epoch 347, Loss: 0.813116118868192\n",
      "Test Loss after Epoch 347: 0.8227987418174744\n",
      "9000\n",
      "Epoch 348, Loss: 0.8042095526456833\n",
      "Test Loss after Epoch 348: 0.8257672834396362\n",
      "9000\n",
      "Epoch 349, Loss: 0.8038169248633914\n",
      "Test Loss after Epoch 349: 0.8365745151042938\n",
      "9000\n",
      "Epoch 350, Loss: 0.8039393799304962\n",
      "Test Loss after Epoch 350: 0.8298719794750213\n",
      "9000\n",
      "Epoch 351, Loss: 0.8038290435605578\n",
      "Test Loss after Epoch 351: 0.8304358828067779\n",
      "9000\n",
      "Epoch 352, Loss: 0.7987374482552211\n",
      "Test Loss after Epoch 352: 0.8285464980602264\n",
      "9000\n",
      "Epoch 353, Loss: 0.7996293088065254\n",
      "Test Loss after Epoch 353: 0.8246967248916626\n",
      "9000\n",
      "Epoch 354, Loss: 0.7930396886401706\n",
      "Test Loss after Epoch 354: 0.8341482181549073\n",
      "9000\n",
      "Epoch 355, Loss: 0.7965273597770267\n",
      "Test Loss after Epoch 355: 0.8289430706501008\n",
      "9000\n",
      "Epoch 356, Loss: 0.7929884157180787\n",
      "Test Loss after Epoch 356: 0.8217243890762329\n",
      "9000\n",
      "Epoch 357, Loss: 0.7923952196306653\n",
      "Test Loss after Epoch 357: 0.8246296720504761\n",
      "9000\n",
      "Epoch 358, Loss: 0.7867588012748294\n",
      "Test Loss after Epoch 358: 0.8226928741931915\n",
      "9000\n",
      "Epoch 359, Loss: 0.7870128937297397\n",
      "Test Loss after Epoch 359: 0.8300308606624603\n",
      "9000\n",
      "Epoch 360, Loss: 0.7865927010112339\n",
      "Test Loss after Epoch 360: 0.8284784376621246\n",
      "9000\n",
      "Epoch 361, Loss: 0.7867517608536614\n",
      "Test Loss after Epoch 361: 0.8172821626663208\n",
      "9000\n",
      "Epoch 362, Loss: 0.7860894494056702\n",
      "Test Loss after Epoch 362: 0.8211475858688354\n",
      "9000\n",
      "Epoch 363, Loss: 0.7827003558609221\n",
      "Test Loss after Epoch 363: 0.8203122818470001\n",
      "9000\n",
      "Epoch 364, Loss: 0.7827659262816111\n",
      "Test Loss after Epoch 364: 0.8233241286277772\n",
      "9000\n",
      "Epoch 365, Loss: 0.781069368481636\n",
      "Test Loss after Epoch 365: 0.8211179261207581\n",
      "9000\n",
      "Epoch 366, Loss: 0.7849353299008476\n",
      "Test Loss after Epoch 366: 0.8271343245506286\n",
      "9000\n",
      "Epoch 367, Loss: 0.779662287539906\n",
      "Test Loss after Epoch 367: 0.8159097082614899\n",
      "9000\n",
      "Epoch 368, Loss: 0.7791646827724245\n",
      "Test Loss after Epoch 368: 0.823814611196518\n",
      "9000\n",
      "Epoch 369, Loss: 0.7774598123762343\n",
      "Test Loss after Epoch 369: 0.8328560674190522\n",
      "9000\n",
      "Epoch 370, Loss: 0.7755582231945461\n",
      "Test Loss after Epoch 370: 0.8269355924129486\n",
      "9000\n",
      "Epoch 371, Loss: 0.7792268592516581\n",
      "Test Loss after Epoch 371: 0.8300826601982116\n",
      "9000\n",
      "Epoch 372, Loss: 0.7798973885112339\n",
      "Test Loss after Epoch 372: 0.8184851081371307\n",
      "9000\n",
      "Epoch 373, Loss: 0.7759769813087252\n",
      "Test Loss after Epoch 373: 0.8230899894237518\n",
      "9000\n",
      "Epoch 374, Loss: 0.7767334108220206\n",
      "Test Loss after Epoch 374: 0.820700939655304\n",
      "9000\n",
      "Epoch 375, Loss: 0.774636930624644\n",
      "Test Loss after Epoch 375: 0.8215432987213135\n",
      "9000\n",
      "Epoch 376, Loss: 0.7741855096949471\n",
      "Test Loss after Epoch 376: 0.8231425297260284\n",
      "9000\n",
      "Epoch 377, Loss: 0.771773747616344\n",
      "Test Loss after Epoch 377: 0.8278588969707489\n",
      "9000\n",
      "Epoch 378, Loss: 0.7723663594457838\n",
      "Test Loss after Epoch 378: 0.8287894833087921\n",
      "9000\n",
      "Epoch 379, Loss: 0.773905806488461\n",
      "Test Loss after Epoch 379: 0.8230714716911316\n",
      "9000\n",
      "Epoch 380, Loss: 0.772028875430425\n",
      "Test Loss after Epoch 380: 0.8240717861652375\n",
      "9000\n",
      "Epoch 381, Loss: 0.7709450442790985\n",
      "Test Loss after Epoch 381: 0.8210500507354737\n",
      "9000\n",
      "Epoch 382, Loss: 0.7723617481125725\n",
      "Test Loss after Epoch 382: 0.8218572947978974\n",
      "9000\n",
      "Epoch 383, Loss: 0.7725725521908866\n",
      "Test Loss after Epoch 383: 0.818966403245926\n",
      "9000\n",
      "Epoch 384, Loss: 0.7698296239243614\n",
      "Test Loss after Epoch 384: 0.8238177239894867\n",
      "9000\n",
      "Epoch 385, Loss: 0.7672196510765288\n",
      "Test Loss after Epoch 385: 0.8230580673217773\n",
      "9000\n",
      "Epoch 386, Loss: 0.7681096107694838\n",
      "Test Loss after Epoch 386: 0.8258745851516723\n",
      "9000\n",
      "Epoch 387, Loss: 0.7666941940784454\n",
      "Test Loss after Epoch 387: 0.8278965792655945\n",
      "9000\n",
      "Epoch 388, Loss: 0.7654792965253194\n",
      "Test Loss after Epoch 388: 0.8247897815704346\n",
      "9000\n",
      "Epoch 389, Loss: 0.7649065816932255\n",
      "Test Loss after Epoch 389: 0.8364064927101136\n",
      "9000\n",
      "Epoch 390, Loss: 0.764045895854632\n",
      "Test Loss after Epoch 390: 0.8284468502998352\n",
      "9000\n",
      "Epoch 391, Loss: 0.7645634662310282\n",
      "Test Loss after Epoch 391: 0.8286844217777252\n",
      "9000\n",
      "Epoch 392, Loss: 0.7632538935343425\n",
      "Test Loss after Epoch 392: 0.8225985095500946\n",
      "9000\n",
      "Epoch 393, Loss: 0.7648457008600235\n",
      "Test Loss after Epoch 393: 0.8282839269638061\n",
      "9000\n",
      "Epoch 394, Loss: 0.7634625557528602\n",
      "Test Loss after Epoch 394: 0.8240342240333557\n",
      "9000\n",
      "Epoch 395, Loss: 0.7608858642578125\n",
      "Test Loss after Epoch 395: 0.8230967111587525\n",
      "9000\n",
      "Epoch 396, Loss: 0.7633352840211657\n",
      "Test Loss after Epoch 396: 0.8170489029884338\n",
      "9000\n",
      "Epoch 397, Loss: 0.7630034754938549\n",
      "Test Loss after Epoch 397: 0.8294251410961151\n",
      "9000\n",
      "Epoch 398, Loss: 0.7587983807987637\n",
      "Test Loss after Epoch 398: 0.8288028671741485\n",
      "9000\n",
      "Epoch 399, Loss: 0.7596629256672329\n",
      "Test Loss after Epoch 399: 0.8228830978870392\n",
      "9000\n",
      "Epoch 400, Loss: 0.7591426277558009\n",
      "Test Loss after Epoch 400: 0.8257401125431061\n",
      "9000\n",
      "Epoch 401, Loss: 0.7610411944256888\n",
      "Test Loss after Epoch 401: 0.8220162026882172\n",
      "9000\n",
      "Epoch 402, Loss: 0.7632430035405688\n",
      "Test Loss after Epoch 402: 0.825517894744873\n",
      "9000\n",
      "Epoch 403, Loss: 0.7589449274539948\n",
      "Test Loss after Epoch 403: 0.8236472482681274\n",
      "9000\n",
      "Epoch 404, Loss: 0.7593188014162912\n",
      "Test Loss after Epoch 404: 0.8268932409286499\n",
      "9000\n",
      "Epoch 405, Loss: 0.7566730243629879\n",
      "Test Loss after Epoch 405: 0.8223301873207093\n",
      "9000\n",
      "Epoch 406, Loss: 0.7597111897733476\n",
      "Test Loss after Epoch 406: 0.8222746207714081\n",
      "9000\n",
      "Epoch 407, Loss: 0.7579891048139996\n",
      "Test Loss after Epoch 407: 0.8279988751411438\n",
      "9000\n",
      "Epoch 408, Loss: 0.7592837399906582\n",
      "Test Loss after Epoch 408: 0.8226967401504517\n",
      "9000\n",
      "Epoch 409, Loss: 0.7584532883432177\n",
      "Test Loss after Epoch 409: 0.8215956041812896\n",
      "9000\n",
      "Epoch 410, Loss: 0.7545383111900753\n",
      "Test Loss after Epoch 410: 0.820582926273346\n",
      "9000\n",
      "Epoch 411, Loss: 0.7586795504887899\n",
      "Test Loss after Epoch 411: 0.8181708347797394\n",
      "9000\n",
      "Epoch 412, Loss: 0.7552864955796136\n",
      "Test Loss after Epoch 412: 0.8206823756694793\n",
      "9000\n",
      "Epoch 413, Loss: 0.7557739021248288\n",
      "Test Loss after Epoch 413: 0.8223440337181092\n",
      "9000\n",
      "Epoch 414, Loss: 0.7574688934087753\n",
      "Test Loss after Epoch 414: 0.8258636016845703\n",
      "9000\n",
      "Epoch 415, Loss: 0.7554746898412704\n",
      "Test Loss after Epoch 415: 0.8233414452075958\n",
      "9000\n",
      "Epoch 416, Loss: 0.7552958515617583\n",
      "Test Loss after Epoch 416: 0.8221259729862214\n",
      "9000\n",
      "Epoch 417, Loss: 0.755888748354382\n",
      "Test Loss after Epoch 417: 0.8219424915313721\n",
      "9000\n",
      "Epoch 418, Loss: 0.7544243713617325\n",
      "Test Loss after Epoch 418: 0.8198230764865875\n",
      "9000\n",
      "Epoch 419, Loss: 0.7543950066301558\n",
      "Test Loss after Epoch 419: 0.824435376405716\n",
      "9000\n",
      "Epoch 420, Loss: 0.7545405687623554\n",
      "Test Loss after Epoch 420: 0.8230848743915558\n",
      "9000\n",
      "Epoch 421, Loss: 0.7540298921267191\n",
      "Test Loss after Epoch 421: 0.820959511756897\n",
      "9000\n",
      "Epoch 422, Loss: 0.7537449499103758\n",
      "Test Loss after Epoch 422: 0.8218758528232575\n",
      "Rolling back to best model from epoch 367\n",
      "Best test loss: 0.8159097082614899\n",
      "9000\n",
      "Epoch 423, Loss: 0.7746935850779215\n",
      "Test Loss after Epoch 423: 0.8207001094818115\n",
      "9000\n",
      "Epoch 424, Loss: 0.7694310049480863\n",
      "Test Loss after Epoch 424: 0.8194183261394501\n",
      "9000\n",
      "Epoch 425, Loss: 0.7689089885287814\n",
      "Test Loss after Epoch 425: 0.8242387726306916\n",
      "9000\n",
      "Epoch 426, Loss: 0.7696516021755007\n",
      "Test Loss after Epoch 426: 0.8243424065113067\n",
      "9000\n",
      "Epoch 427, Loss: 0.771082963930236\n",
      "Test Loss after Epoch 427: 0.8240053501129151\n",
      "9000\n",
      "Epoch 428, Loss: 0.7720432800849278\n",
      "Test Loss after Epoch 428: 0.8229887855052948\n",
      "9000\n",
      "Epoch 429, Loss: 0.7709277402162552\n",
      "Test Loss after Epoch 429: 0.8278035378456116\n",
      "9000\n",
      "Epoch 430, Loss: 0.7668860262235005\n",
      "Test Loss after Epoch 430: 0.8329291343688965\n",
      "9000\n",
      "Epoch 431, Loss: 0.7733609039518569\n",
      "Test Loss after Epoch 431: 0.8232481460571289\n",
      "9000\n",
      "Epoch 432, Loss: 0.7692890926334593\n",
      "Test Loss after Epoch 432: 0.8291028227806091\n",
      "9000\n",
      "Epoch 433, Loss: 0.7710536099407408\n",
      "Test Loss after Epoch 433: 0.8248183875083923\n",
      "9000\n",
      "Epoch 434, Loss: 0.7694004212352965\n",
      "Test Loss after Epoch 434: 0.8253677220344543\n",
      "9000\n",
      "Epoch 435, Loss: 0.7655220925278133\n",
      "Test Loss after Epoch 435: 0.8237662446498871\n",
      "9000\n",
      "Epoch 436, Loss: 0.7679508679707845\n",
      "Test Loss after Epoch 436: 0.823808023929596\n",
      "9000\n",
      "Epoch 437, Loss: 0.7665715667141808\n",
      "Test Loss after Epoch 437: 0.8206161003112793\n",
      "9000\n",
      "Epoch 438, Loss: 0.7660456177923415\n",
      "Test Loss after Epoch 438: 0.8154358453750611\n",
      "9000\n",
      "Epoch 439, Loss: 0.764383303364118\n",
      "Test Loss after Epoch 439: 0.8201731081008912\n",
      "9000\n",
      "Epoch 440, Loss: 0.7629448968436983\n",
      "Test Loss after Epoch 440: 0.8252711763381958\n",
      "9000\n",
      "Epoch 441, Loss: 0.7654932118256886\n",
      "Test Loss after Epoch 441: 0.8263726408481598\n",
      "9000\n",
      "Epoch 442, Loss: 0.7660526142517725\n",
      "Test Loss after Epoch 442: 0.8250241143703461\n",
      "9000\n",
      "Epoch 443, Loss: 0.7648461099730598\n",
      "Test Loss after Epoch 443: 0.8224324092864991\n",
      "9000\n",
      "Epoch 444, Loss: 0.7632062263886134\n",
      "Test Loss after Epoch 444: 0.8284103379249573\n",
      "9000\n",
      "Epoch 445, Loss: 0.7611045304934184\n",
      "Test Loss after Epoch 445: 0.8207199645042419\n",
      "9000\n",
      "Epoch 446, Loss: 0.7630084868007236\n",
      "Test Loss after Epoch 446: 0.8266581745147705\n",
      "9000\n",
      "Epoch 447, Loss: 0.7613783965773052\n",
      "Test Loss after Epoch 447: 0.818485057592392\n",
      "9000\n",
      "Epoch 448, Loss: 0.7604028291702271\n",
      "Test Loss after Epoch 448: 0.8245505902767182\n",
      "9000\n",
      "Epoch 449, Loss: 0.7610015872187085\n",
      "Test Loss after Epoch 449: 0.8218836128711701\n",
      "9000\n",
      "Epoch 450, Loss: 0.7596872906552421\n",
      "Test Loss after Epoch 450: 0.8277141139507294\n",
      "9000\n",
      "Epoch 451, Loss: 0.7601271144946417\n",
      "Test Loss after Epoch 451: 0.8243341472148895\n",
      "9000\n",
      "Epoch 452, Loss: 0.7609939977592892\n",
      "Test Loss after Epoch 452: 0.8237791595458984\n",
      "9000\n",
      "Epoch 453, Loss: 0.7604870965878169\n",
      "Test Loss after Epoch 453: 0.8225616178512574\n",
      "9000\n",
      "Epoch 454, Loss: 0.7578317506180869\n",
      "Test Loss after Epoch 454: 0.8226599361896515\n",
      "9000\n",
      "Epoch 455, Loss: 0.7584362547530068\n",
      "Test Loss after Epoch 455: 0.8268000094890594\n",
      "9000\n",
      "Epoch 456, Loss: 0.7558902890814675\n",
      "Test Loss after Epoch 456: 0.8218922061920166\n",
      "9000\n",
      "Epoch 457, Loss: 0.7575326870679855\n",
      "Test Loss after Epoch 457: 0.8279728000164032\n",
      "9000\n",
      "Epoch 458, Loss: 0.7558824234273699\n",
      "Test Loss after Epoch 458: 0.8285492799282074\n",
      "9000\n",
      "Epoch 459, Loss: 0.7576253643433253\n",
      "Test Loss after Epoch 459: 0.8220114758014679\n",
      "9000\n",
      "Epoch 460, Loss: 0.7559156139294306\n",
      "Test Loss after Epoch 460: 0.8235787739753723\n",
      "9000\n",
      "Epoch 461, Loss: 0.758418378300137\n",
      "Test Loss after Epoch 461: 0.8225345199108124\n",
      "9000\n",
      "Epoch 462, Loss: 0.756855679988861\n",
      "Test Loss after Epoch 462: 0.8261914620399475\n",
      "9000\n",
      "Epoch 463, Loss: 0.7548946067094803\n",
      "Test Loss after Epoch 463: 0.8240998146533967\n",
      "9000\n",
      "Epoch 464, Loss: 0.7546845187478596\n",
      "Test Loss after Epoch 464: 0.8187738490104676\n",
      "9000\n",
      "Epoch 465, Loss: 0.7550661307573319\n",
      "Test Loss after Epoch 465: 0.822593496799469\n",
      "9000\n",
      "Epoch 466, Loss: 0.7558914330005646\n",
      "Test Loss after Epoch 466: 0.8234886906147003\n",
      "9000\n",
      "Epoch 467, Loss: 0.7541341007020739\n",
      "Test Loss after Epoch 467: 0.8167572507858276\n",
      "9000\n",
      "Epoch 468, Loss: 0.7548663997650147\n",
      "Test Loss after Epoch 468: 0.8234325160980225\n",
      "9000\n",
      "Epoch 469, Loss: 0.7535514929427041\n",
      "Test Loss after Epoch 469: 0.8204712257385254\n",
      "9000\n",
      "Epoch 470, Loss: 0.7535141455597347\n",
      "Test Loss after Epoch 470: 0.8218987548351288\n",
      "9000\n",
      "Epoch 471, Loss: 0.7564288958708445\n",
      "Test Loss after Epoch 471: 0.8225276591777801\n",
      "9000\n",
      "Epoch 472, Loss: 0.7540528272125456\n",
      "Test Loss after Epoch 472: 0.8305822060108184\n",
      "9000\n",
      "Epoch 473, Loss: 0.753427986211247\n",
      "Test Loss after Epoch 473: 0.8264679327011109\n",
      "9000\n",
      "Epoch 474, Loss: 0.753461591720581\n",
      "Test Loss after Epoch 474: 0.8286033990383148\n",
      "9000\n",
      "Epoch 475, Loss: 0.7527318008608288\n",
      "Test Loss after Epoch 475: 0.8182880029678344\n",
      "9000\n",
      "Epoch 476, Loss: 0.752801091366344\n",
      "Test Loss after Epoch 476: 0.8187526569366456\n",
      "9000\n",
      "Epoch 477, Loss: 0.7515475349293814\n",
      "Test Loss after Epoch 477: 0.8179689733982086\n",
      "9000\n",
      "Epoch 478, Loss: 0.7511962902413474\n",
      "Test Loss after Epoch 478: 0.8224832139015198\n",
      "9000\n",
      "Epoch 479, Loss: 0.7505096028248469\n",
      "Test Loss after Epoch 479: 0.8235843358039856\n",
      "9000\n",
      "Epoch 480, Loss: 0.749597680926323\n",
      "Test Loss after Epoch 480: 0.8193168036937714\n",
      "9000\n",
      "Epoch 481, Loss: 0.7523616359101402\n",
      "Test Loss after Epoch 481: 0.8195671381950378\n",
      "9000\n",
      "Epoch 482, Loss: 0.7521438525650236\n",
      "Test Loss after Epoch 482: 0.8185049200057983\n",
      "9000\n",
      "Epoch 483, Loss: 0.7532294620805317\n",
      "Test Loss after Epoch 483: 0.820838772058487\n",
      "9000\n",
      "Epoch 484, Loss: 0.751570424411032\n",
      "Test Loss after Epoch 484: 0.8177511527538299\n",
      "9000\n",
      "Epoch 485, Loss: 0.7491416541470421\n",
      "Test Loss after Epoch 485: 0.820840526342392\n",
      "9000\n",
      "Epoch 486, Loss: 0.7514186065859265\n",
      "Test Loss after Epoch 486: 0.8170872402191162\n",
      "9000\n",
      "Epoch 487, Loss: 0.7514816637966368\n",
      "Test Loss after Epoch 487: 0.8206069645881653\n",
      "9000\n",
      "Epoch 488, Loss: 0.7506687814527088\n",
      "Test Loss after Epoch 488: 0.8217293736934662\n",
      "9000\n",
      "Epoch 489, Loss: 0.7507049130068885\n",
      "Test Loss after Epoch 489: 0.8261555342674255\n",
      "9000\n",
      "Epoch 490, Loss: 0.7491405263211992\n",
      "Test Loss after Epoch 490: 0.8247740530967712\n",
      "9000\n",
      "Epoch 491, Loss: 0.7514987727271186\n",
      "Test Loss after Epoch 491: 0.8272999665737152\n",
      "9000\n",
      "Epoch 492, Loss: 0.7497378211021424\n",
      "Test Loss after Epoch 492: 0.8283816282749176\n",
      "9000\n",
      "Epoch 493, Loss: 0.7491732256412507\n",
      "Test Loss after Epoch 493: 0.8248357090950013\n",
      "9000\n",
      "Epoch 494, Loss: 0.7510039493110445\n",
      "Test Loss after Epoch 494: 0.8256996600627899\n",
      "9000\n",
      "Epoch 495, Loss: 0.7497374080949359\n",
      "Test Loss after Epoch 495: 0.8229488213062286\n",
      "9000\n",
      "Epoch 496, Loss: 0.7497214699188868\n",
      "Test Loss after Epoch 496: 0.8215892086029053\n",
      "9000\n",
      "Epoch 497, Loss: 0.7490168806844287\n",
      "Test Loss after Epoch 497: 0.8228273754119872\n",
      "9000\n",
      "Epoch 498, Loss: 0.7487756833897696\n",
      "Test Loss after Epoch 498: 0.8249870450496674\n",
      "9000\n",
      "Epoch 499, Loss: 0.7482341775099437\n",
      "Test Loss after Epoch 499: 0.8227137334346771\n",
      "9000\n",
      "Epoch 500, Loss: 0.7468980092340045\n",
      "Test Loss after Epoch 500: 0.8241618375778198\n",
      "9000\n",
      "Epoch 501, Loss: 0.7488920682403777\n",
      "Test Loss after Epoch 501: 0.8254109103679657\n",
      "9000\n",
      "Epoch 502, Loss: 0.7472053959369659\n",
      "Test Loss after Epoch 502: 0.8242845032215118\n",
      "9000\n",
      "Epoch 503, Loss: 0.7454062016407649\n",
      "Test Loss after Epoch 503: 0.8279013218879699\n",
      "9000\n",
      "Epoch 504, Loss: 0.7459758440123664\n",
      "Test Loss after Epoch 504: 0.8206976709365845\n",
      "9000\n",
      "Epoch 505, Loss: 0.745736523548762\n",
      "Test Loss after Epoch 505: 0.8236737337112426\n",
      "9000\n",
      "Epoch 506, Loss: 0.7480640816820993\n",
      "Test Loss after Epoch 506: 0.8280834140777588\n",
      "9000\n",
      "Epoch 507, Loss: 0.7466887518035041\n",
      "Test Loss after Epoch 507: 0.8237470080852508\n",
      "9000\n",
      "Epoch 508, Loss: 0.7471548464033338\n",
      "Test Loss after Epoch 508: 0.8280099787712097\n",
      "9000\n",
      "Epoch 509, Loss: 0.7463422701888615\n",
      "Test Loss after Epoch 509: 0.8253295726776123\n",
      "9000\n",
      "Epoch 510, Loss: 0.7479666492674086\n",
      "Test Loss after Epoch 510: 0.8283135743141175\n",
      "9000\n",
      "Epoch 511, Loss: 0.7467185733848148\n",
      "Test Loss after Epoch 511: 0.825189395904541\n",
      "9000\n",
      "Epoch 512, Loss: 0.7447633624076844\n",
      "Test Loss after Epoch 512: 0.8232929215431214\n",
      "9000\n",
      "Epoch 513, Loss: 0.7440009129709667\n",
      "Test Loss after Epoch 513: 0.8229809930324554\n",
      "9000\n",
      "Epoch 514, Loss: 0.7447919726901584\n",
      "Test Loss after Epoch 514: 0.8283358507156372\n",
      "9000\n",
      "Epoch 515, Loss: 0.7446367981168959\n",
      "Test Loss after Epoch 515: 0.8256287512779236\n",
      "Rolling back to best model from epoch 438\n",
      "Best test loss: 0.8154358453750611\n",
      "9000\n",
      "Epoch 516, Loss: 0.7608678819206026\n",
      "Test Loss after Epoch 516: 0.8178219976425171\n",
      "9000\n",
      "Epoch 517, Loss: 0.7578934168683158\n",
      "Test Loss after Epoch 517: 0.8235913045406341\n",
      "9000\n",
      "Epoch 518, Loss: 0.7571391183932622\n",
      "Test Loss after Epoch 518: 0.8277242095470428\n",
      "9000\n",
      "Epoch 519, Loss: 0.7572495590051015\n",
      "Test Loss after Epoch 519: 0.8205719435214996\n",
      "9000\n",
      "Epoch 520, Loss: 0.7583112245798111\n",
      "Test Loss after Epoch 520: 0.8272967205047608\n",
      "9000\n",
      "Epoch 521, Loss: 0.7598460589912203\n",
      "Test Loss after Epoch 521: 0.8248750717639923\n",
      "9000\n",
      "Epoch 522, Loss: 0.7584239957597521\n",
      "Test Loss after Epoch 522: 0.8257560801506042\n",
      "9000\n",
      "Epoch 523, Loss: 0.7587394209172991\n",
      "Test Loss after Epoch 523: 0.8201402342319488\n",
      "9000\n",
      "Epoch 524, Loss: 0.7584108160469267\n",
      "Test Loss after Epoch 524: 0.8249741311073303\n",
      "9000\n",
      "Epoch 525, Loss: 0.7582100303437975\n",
      "Test Loss after Epoch 525: 0.8208020887374878\n",
      "9000\n",
      "Epoch 526, Loss: 0.7602099629508124\n",
      "Test Loss after Epoch 526: 0.8264281044006347\n",
      "9000\n",
      "Epoch 527, Loss: 0.7587186083131366\n",
      "Test Loss after Epoch 527: 0.8250550417900085\n",
      "9000\n",
      "Epoch 528, Loss: 0.7585583842198054\n",
      "Test Loss after Epoch 528: 0.8294471070766449\n",
      "9000\n",
      "Epoch 529, Loss: 0.7597079076237149\n",
      "Test Loss after Epoch 529: 0.8260079536437989\n",
      "9000\n",
      "Epoch 530, Loss: 0.756752568324407\n",
      "Test Loss after Epoch 530: 0.8288501453399658\n",
      "9000\n",
      "Epoch 531, Loss: 0.7568195735613505\n",
      "Test Loss after Epoch 531: 0.8245219538211822\n",
      "9000\n",
      "Epoch 532, Loss: 0.7563204514185587\n",
      "Test Loss after Epoch 532: 0.8278073318004608\n",
      "9000\n",
      "Epoch 533, Loss: 0.7544098091390398\n",
      "Test Loss after Epoch 533: 0.8244208900928497\n",
      "9000\n",
      "Epoch 534, Loss: 0.7528181597391764\n",
      "Test Loss after Epoch 534: 0.8254945993423461\n",
      "9000\n",
      "Epoch 535, Loss: 0.7533812248044544\n",
      "Test Loss after Epoch 535: 0.8351451046466828\n",
      "9000\n",
      "Epoch 536, Loss: 0.7566061240302192\n",
      "Test Loss after Epoch 536: 0.8243008742332458\n",
      "9000\n",
      "Epoch 537, Loss: 0.7550654524034924\n",
      "Test Loss after Epoch 537: 0.8346491765975952\n",
      "9000\n",
      "Epoch 538, Loss: 0.7550401654375924\n",
      "Test Loss after Epoch 538: 0.8207987501621247\n",
      "9000\n",
      "Epoch 539, Loss: 0.7531536844041612\n",
      "Test Loss after Epoch 539: 0.8244037203788758\n",
      "9000\n",
      "Epoch 540, Loss: 0.7538093680275811\n",
      "Test Loss after Epoch 540: 0.8224265463352204\n",
      "9000\n",
      "Epoch 541, Loss: 0.7550516850550969\n",
      "Test Loss after Epoch 541: 0.8223740019798279\n",
      "9000\n",
      "Epoch 542, Loss: 0.753671685245302\n",
      "Test Loss after Epoch 542: 0.8233084921836853\n",
      "9000\n",
      "Epoch 543, Loss: 0.7537243875000212\n",
      "Test Loss after Epoch 543: 0.8260815672874451\n",
      "9000\n",
      "Epoch 544, Loss: 0.753249704029825\n",
      "Test Loss after Epoch 544: 0.8314669914245606\n",
      "9000\n",
      "Epoch 545, Loss: 0.7541225046714147\n",
      "Test Loss after Epoch 545: 0.8217366631031037\n",
      "9000\n",
      "Epoch 546, Loss: 0.7505119446913401\n",
      "Test Loss after Epoch 546: 0.8222346642017364\n",
      "9000\n",
      "Epoch 547, Loss: 0.7513623491525651\n",
      "Test Loss after Epoch 547: 0.8244484543800354\n",
      "9000\n",
      "Epoch 548, Loss: 0.7527164211405648\n",
      "Test Loss after Epoch 548: 0.8222173287868499\n",
      "9000\n",
      "Epoch 549, Loss: 0.749936103105545\n",
      "Test Loss after Epoch 549: 0.8291702449321747\n",
      "9000\n",
      "Epoch 550, Loss: 0.7497605892684724\n",
      "Test Loss after Epoch 550: 0.8285484144687653\n",
      "9000\n",
      "Epoch 551, Loss: 0.7517723057667415\n",
      "Test Loss after Epoch 551: 0.8256639113426208\n",
      "9000\n",
      "Epoch 552, Loss: 0.7506308205657535\n",
      "Test Loss after Epoch 552: 0.8263714003562928\n",
      "9000\n",
      "Epoch 553, Loss: 0.7499255788326263\n",
      "Test Loss after Epoch 553: 0.8256573731899262\n",
      "9000\n",
      "Epoch 554, Loss: 0.7505380653540293\n",
      "Test Loss after Epoch 554: 0.8268237595558167\n",
      "9000\n",
      "Epoch 555, Loss: 0.7491535014973746\n",
      "Test Loss after Epoch 555: 0.8280267679691314\n",
      "9000\n",
      "Epoch 556, Loss: 0.749966108891699\n",
      "Test Loss after Epoch 556: 0.8277049314975738\n",
      "9000\n",
      "Epoch 557, Loss: 0.7489681232902738\n",
      "Test Loss after Epoch 557: 0.8279979321956634\n",
      "9000\n",
      "Epoch 558, Loss: 0.7497775231599808\n",
      "Test Loss after Epoch 558: 0.8255716273784638\n",
      "9000\n",
      "Epoch 559, Loss: 0.7472528125709957\n",
      "Test Loss after Epoch 559: 0.8214605445861817\n",
      "9000\n",
      "Epoch 560, Loss: 0.7485765904585521\n",
      "Test Loss after Epoch 560: 0.8253684561252594\n",
      "9000\n",
      "Epoch 561, Loss: 0.7509750817086962\n",
      "Test Loss after Epoch 561: 0.8198494501113892\n",
      "9000\n",
      "Epoch 562, Loss: 0.7504346199962828\n",
      "Test Loss after Epoch 562: 0.8212112472057342\n",
      "9000\n",
      "Epoch 563, Loss: 0.75093413219187\n",
      "Test Loss after Epoch 563: 0.8220841710567475\n",
      "9000\n",
      "Epoch 564, Loss: 0.7497704640891817\n",
      "Test Loss after Epoch 564: 0.8207204704284667\n",
      "9000\n",
      "Epoch 565, Loss: 0.7500746869511075\n",
      "Test Loss after Epoch 565: 0.8196446435451508\n",
      "9000\n",
      "Epoch 566, Loss: 0.7487525378200743\n",
      "Test Loss after Epoch 566: 0.8192583775520325\n",
      "9000\n",
      "Epoch 567, Loss: 0.7482855483823352\n",
      "Test Loss after Epoch 567: 0.824320143699646\n",
      "9000\n",
      "Epoch 568, Loss: 0.7495325282547209\n",
      "Test Loss after Epoch 568: 0.8220063335895539\n",
      "9000\n",
      "Epoch 569, Loss: 0.7493236014048259\n",
      "Test Loss after Epoch 569: 0.820270852804184\n",
      "9000\n",
      "Epoch 570, Loss: 0.7457161244021522\n",
      "Test Loss after Epoch 570: 0.8245892791748047\n",
      "9000\n",
      "Epoch 571, Loss: 0.7475505309237375\n",
      "Test Loss after Epoch 571: 0.8213786275386811\n",
      "9000\n",
      "Epoch 572, Loss: 0.7468422556850646\n",
      "Test Loss after Epoch 572: 0.8250290541648865\n",
      "9000\n",
      "Epoch 573, Loss: 0.7471580904854669\n",
      "Test Loss after Epoch 573: 0.8245582969188691\n",
      "9000\n",
      "Epoch 574, Loss: 0.7461535121864743\n",
      "Test Loss after Epoch 574: 0.8234842598438263\n",
      "9000\n",
      "Epoch 575, Loss: 0.7465120686690012\n",
      "Test Loss after Epoch 575: 0.8224815313816071\n",
      "9000\n",
      "Epoch 576, Loss: 0.7463656386534373\n",
      "Test Loss after Epoch 576: 0.8208144664764404\n",
      "9000\n",
      "Epoch 577, Loss: 0.7454237001736959\n",
      "Test Loss after Epoch 577: 0.8224580891132355\n",
      "9000\n",
      "Epoch 578, Loss: 0.7450885135730108\n",
      "Test Loss after Epoch 578: 0.8233584384918213\n",
      "9000\n",
      "Epoch 579, Loss: 0.7442037679751714\n",
      "Test Loss after Epoch 579: 0.8254296371936798\n",
      "9000\n",
      "Epoch 580, Loss: 0.744594074010849\n",
      "Test Loss after Epoch 580: 0.822069582939148\n",
      "9000\n",
      "Epoch 581, Loss: 0.7460831352207395\n",
      "Test Loss after Epoch 581: 0.824601087808609\n",
      "9000\n",
      "Epoch 582, Loss: 0.7452874972422918\n",
      "Test Loss after Epoch 582: 0.8255951497554779\n",
      "9000\n",
      "Epoch 583, Loss: 0.7452383973466026\n",
      "Test Loss after Epoch 583: 0.8315225911140441\n",
      "9000\n",
      "Epoch 584, Loss: 0.7447319210635291\n",
      "Test Loss after Epoch 584: 0.8327546246051788\n",
      "9000\n",
      "Epoch 585, Loss: 0.7437132160663604\n",
      "Test Loss after Epoch 585: 0.8285729608535767\n",
      "9000\n",
      "Epoch 586, Loss: 0.7427711769209968\n",
      "Test Loss after Epoch 586: 0.8267482745647431\n",
      "9000\n",
      "Epoch 587, Loss: 0.7447623686525556\n",
      "Test Loss after Epoch 587: 0.8262963786125183\n",
      "9000\n",
      "Epoch 588, Loss: 0.7430749090247684\n",
      "Test Loss after Epoch 588: 0.8272289900779725\n",
      "9000\n",
      "Epoch 589, Loss: 0.7433412781688902\n",
      "Test Loss after Epoch 589: 0.8291575429439545\n",
      "9000\n",
      "Epoch 590, Loss: 0.7441469055016835\n",
      "Test Loss after Epoch 590: 0.8285182378292084\n",
      "9000\n",
      "Epoch 591, Loss: 0.7446161818239424\n",
      "Test Loss after Epoch 591: 0.8247693903446197\n",
      "9000\n",
      "Epoch 592, Loss: 0.7463895797861947\n",
      "Test Loss after Epoch 592: 0.8257684700489044\n",
      "9000\n",
      "Epoch 593, Loss: 0.7457198930316501\n",
      "Test Loss after Epoch 593: 0.8260225920677186\n",
      "9000\n",
      "Epoch 594, Loss: 0.7441496918333901\n",
      "Test Loss after Epoch 594: 0.8260773541927338\n",
      "9000\n",
      "Epoch 595, Loss: 0.7432842097149955\n",
      "Test Loss after Epoch 595: 0.8300456848144532\n",
      "9000\n",
      "Epoch 596, Loss: 0.7463607898818122\n",
      "Test Loss after Epoch 596: 0.8278926813602447\n",
      "9000\n",
      "Epoch 597, Loss: 0.7441365570757125\n",
      "Test Loss after Epoch 597: 0.8270780065059662\n",
      "9000\n",
      "Epoch 598, Loss: 0.7438674272563722\n",
      "Test Loss after Epoch 598: 0.8230016663074493\n",
      "9000\n",
      "Epoch 599, Loss: 0.7435345331430435\n",
      "Test Loss after Epoch 599: 0.8223951058387756\n",
      "9000\n",
      "Epoch 600, Loss: 0.742264998515447\n",
      "Test Loss after Epoch 600: 0.8275255360603333\n",
      "9000\n",
      "Epoch 601, Loss: 0.7433534187873204\n",
      "Test Loss after Epoch 601: 0.8284474990367889\n",
      "9000\n",
      "Epoch 602, Loss: 0.7424461485544841\n",
      "Test Loss after Epoch 602: 0.8346233656406402\n",
      "9000\n",
      "Epoch 603, Loss: 0.7412912081877391\n",
      "Test Loss after Epoch 603: 0.8305330669879913\n",
      "9000\n",
      "Epoch 604, Loss: 0.7421525902085834\n",
      "Test Loss after Epoch 604: 0.8328213164806366\n",
      "9000\n",
      "Epoch 605, Loss: 0.7415569211641947\n",
      "Test Loss after Epoch 605: 0.8254301323890686\n",
      "9000\n",
      "Epoch 606, Loss: 0.7431225800911585\n",
      "Test Loss after Epoch 606: 0.8304625482559204\n",
      "9000\n",
      "Epoch 607, Loss: 0.7430264042086071\n",
      "Test Loss after Epoch 607: 0.8274039521217346\n",
      "9000\n",
      "Epoch 608, Loss: 0.7429047391017278\n",
      "Test Loss after Epoch 608: 0.8315938460826874\n",
      "9000\n",
      "Epoch 609, Loss: 0.7416029784017139\n",
      "Test Loss after Epoch 609: 0.8290620763301849\n",
      "9000\n",
      "Epoch 610, Loss: 0.742680200431082\n",
      "Test Loss after Epoch 610: 0.8323209795951844\n",
      "9000\n",
      "Epoch 611, Loss: 0.7426713393794165\n",
      "Test Loss after Epoch 611: 0.8290706129074097\n",
      "9000\n",
      "Epoch 612, Loss: 0.7432386623488532\n",
      "Test Loss after Epoch 612: 0.828632185459137\n",
      "9000\n",
      "Epoch 613, Loss: 0.7419817331896887\n",
      "Test Loss after Epoch 613: 0.8243380072116852\n",
      "9000\n",
      "Epoch 614, Loss: 0.7424055460161633\n",
      "Test Loss after Epoch 614: 0.8288014459609986\n",
      "9000\n",
      "Epoch 615, Loss: 0.7425663873354594\n",
      "Test Loss after Epoch 615: 0.8321178936958313\n",
      "9000\n",
      "Epoch 616, Loss: 0.7412169491714902\n",
      "Test Loss after Epoch 616: 0.8246212115287781\n",
      "9000\n",
      "Epoch 617, Loss: 0.7429135926034716\n",
      "Test Loss after Epoch 617: 0.826952116727829\n",
      "9000\n",
      "Epoch 618, Loss: 0.7402503050963084\n",
      "Test Loss after Epoch 618: 0.827592805147171\n",
      "9000\n",
      "Epoch 619, Loss: 0.7410418948200014\n",
      "Test Loss after Epoch 619: 0.8324930963516235\n",
      "9000\n",
      "Epoch 620, Loss: 0.7402455003394021\n",
      "Test Loss after Epoch 620: 0.8351921784877777\n",
      "9000\n",
      "Epoch 621, Loss: 0.7420641163057751\n",
      "Test Loss after Epoch 621: 0.8306507060527801\n",
      "9000\n",
      "Epoch 622, Loss: 0.7414611035055584\n",
      "Test Loss after Epoch 622: 0.834994950056076\n",
      "9000\n",
      "Epoch 623, Loss: 0.7397699741257562\n",
      "Test Loss after Epoch 623: 0.832735586643219\n",
      "Rolling back to best model from epoch 438\n",
      "Best test loss: 0.8154358453750611\n",
      "9000\n",
      "Epoch 624, Loss: 0.7599127555025948\n",
      "Test Loss after Epoch 624: 0.8155744664669037\n",
      "9000\n",
      "Epoch 625, Loss: 0.7579701533185111\n",
      "Test Loss after Epoch 625: 0.8191792147159577\n",
      "9000\n",
      "Epoch 626, Loss: 0.7583826297124227\n",
      "Test Loss after Epoch 626: 0.8237378740310669\n",
      "9000\n",
      "Epoch 627, Loss: 0.757770088646147\n",
      "Test Loss after Epoch 627: 0.8207185499668121\n",
      "9000\n",
      "Epoch 628, Loss: 0.7590053548945321\n",
      "Test Loss after Epoch 628: 0.8204570138454437\n",
      "9000\n",
      "Epoch 629, Loss: 0.7587749494976468\n",
      "Test Loss after Epoch 629: 0.8194796993732453\n",
      "9000\n",
      "Epoch 630, Loss: 0.7578531295326021\n",
      "Test Loss after Epoch 630: 0.8210438580513001\n",
      "9000\n",
      "Epoch 631, Loss: 0.7575185917086071\n",
      "Test Loss after Epoch 631: 0.8252808210849762\n",
      "9000\n",
      "Epoch 632, Loss: 0.7563618303934733\n",
      "Test Loss after Epoch 632: 0.8218684999942779\n",
      "9000\n",
      "Epoch 633, Loss: 0.7571505766709645\n",
      "Test Loss after Epoch 633: 0.8259473373889923\n",
      "9000\n",
      "Epoch 634, Loss: 0.7569340395794975\n",
      "Test Loss after Epoch 634: 0.8206901226043701\n",
      "9000\n",
      "Epoch 635, Loss: 0.7550122978819741\n",
      "Test Loss after Epoch 635: 0.8214948506355285\n",
      "9000\n",
      "Epoch 636, Loss: 0.75736987929874\n",
      "Test Loss after Epoch 636: 0.8263503787517548\n",
      "9000\n",
      "Epoch 637, Loss: 0.7557186974022123\n",
      "Test Loss after Epoch 637: 0.8258234121799469\n",
      "9000\n",
      "Epoch 638, Loss: 0.7532161684963439\n",
      "Test Loss after Epoch 638: 0.8219121489524841\n",
      "9000\n",
      "Epoch 639, Loss: 0.7538682183557086\n",
      "Test Loss after Epoch 639: 0.8233414628505706\n",
      "9000\n",
      "Epoch 640, Loss: 0.7520880260732439\n",
      "Test Loss after Epoch 640: 0.82293758893013\n",
      "9000\n",
      "Epoch 641, Loss: 0.753915803194046\n",
      "Test Loss after Epoch 641: 0.820393152475357\n",
      "9000\n",
      "Epoch 642, Loss: 0.7520130227539275\n",
      "Test Loss after Epoch 642: 0.8235727407932282\n",
      "9000\n",
      "Epoch 643, Loss: 0.7524237266249126\n",
      "Test Loss after Epoch 643: 0.822222535610199\n",
      "9000\n",
      "Epoch 644, Loss: 0.7513939015732871\n",
      "Test Loss after Epoch 644: 0.8213738429546357\n",
      "9000\n",
      "Epoch 645, Loss: 0.7542867994440926\n",
      "Test Loss after Epoch 645: 0.8224814991950988\n",
      "9000\n",
      "Epoch 646, Loss: 0.7519288698699739\n",
      "Test Loss after Epoch 646: 0.8265097362995147\n",
      "9000\n",
      "Epoch 647, Loss: 0.751061723695861\n",
      "Test Loss after Epoch 647: 0.822991489648819\n",
      "9000\n",
      "Epoch 648, Loss: 0.7536845883528391\n",
      "Test Loss after Epoch 648: 0.8235396003723144\n",
      "9000\n",
      "Epoch 649, Loss: 0.7505819163719814\n",
      "Test Loss after Epoch 649: 0.8247820842266083\n",
      "9000\n",
      "Epoch 650, Loss: 0.7519722415341271\n",
      "Test Loss after Epoch 650: 0.8274369509220123\n",
      "9000\n",
      "Epoch 651, Loss: 0.7501234924793243\n",
      "Test Loss after Epoch 651: 0.8260083668231964\n",
      "9000\n",
      "Epoch 652, Loss: 0.752022798511717\n",
      "Test Loss after Epoch 652: 0.8225583600997924\n",
      "9000\n",
      "Epoch 653, Loss: 0.7504966582589679\n",
      "Test Loss after Epoch 653: 0.8259810061454773\n",
      "9000\n",
      "Epoch 654, Loss: 0.7495025175147586\n",
      "Test Loss after Epoch 654: 0.8185484199523926\n",
      "9000\n",
      "Epoch 655, Loss: 0.749817433476448\n",
      "Test Loss after Epoch 655: 0.8231056089401245\n",
      "9000\n",
      "Epoch 656, Loss: 0.7500528084304597\n",
      "Test Loss after Epoch 656: 0.8214070146083832\n",
      "9000\n",
      "Epoch 657, Loss: 0.7500236469109853\n",
      "Test Loss after Epoch 657: 0.8212094388008118\n",
      "9000\n",
      "Epoch 658, Loss: 0.7499857086605496\n",
      "Test Loss after Epoch 658: 0.8250441844463349\n",
      "9000\n",
      "Epoch 659, Loss: 0.7499426906373766\n",
      "Test Loss after Epoch 659: 0.8232129592895507\n",
      "9000\n",
      "Epoch 660, Loss: 0.7486806103653378\n",
      "Test Loss after Epoch 660: 0.8246107006072998\n",
      "9000\n",
      "Epoch 661, Loss: 0.7494385894536972\n",
      "Test Loss after Epoch 661: 0.8254808344841004\n",
      "9000\n",
      "Epoch 662, Loss: 0.7493122387992012\n",
      "Test Loss after Epoch 662: 0.8253062884807587\n",
      "9000\n",
      "Epoch 663, Loss: 0.749046135187149\n",
      "Test Loss after Epoch 663: 0.832189067363739\n",
      "9000\n",
      "Epoch 664, Loss: 0.7494442125029034\n",
      "Test Loss after Epoch 664: 0.8303254277706146\n",
      "9000\n",
      "Epoch 665, Loss: 0.7485600790447658\n",
      "Test Loss after Epoch 665: 0.8296182353496552\n",
      "9000\n",
      "Epoch 666, Loss: 0.7477861886024475\n",
      "Test Loss after Epoch 666: 0.8227532274723053\n",
      "9000\n",
      "Epoch 667, Loss: 0.7487500134441588\n",
      "Test Loss after Epoch 667: 0.8236782069206238\n",
      "9000\n",
      "Epoch 668, Loss: 0.7471456140544679\n",
      "Test Loss after Epoch 668: 0.8236206896305084\n",
      "9000\n",
      "Epoch 669, Loss: 0.746651846408844\n",
      "Test Loss after Epoch 669: 0.8232727799415588\n",
      "9000\n",
      "Epoch 670, Loss: 0.7469045282734765\n",
      "Test Loss after Epoch 670: 0.8236091048717499\n",
      "9000\n",
      "Epoch 671, Loss: 0.7489175496233834\n",
      "Test Loss after Epoch 671: 0.8288341219425202\n",
      "9000\n",
      "Epoch 672, Loss: 0.7478327755928039\n",
      "Test Loss after Epoch 672: 0.8226502296924592\n",
      "9000\n",
      "Epoch 673, Loss: 0.7482825367715623\n",
      "Test Loss after Epoch 673: 0.8211243650913238\n",
      "9000\n",
      "Epoch 674, Loss: 0.7460817631483078\n",
      "Test Loss after Epoch 674: 0.8290532069206238\n",
      "9000\n",
      "Epoch 675, Loss: 0.7445402260224024\n",
      "Test Loss after Epoch 675: 0.8254649195671081\n",
      "9000\n",
      "Epoch 676, Loss: 0.7470777318345175\n",
      "Test Loss after Epoch 676: 0.8286789817810059\n",
      "9000\n",
      "Epoch 677, Loss: 0.7447399898105197\n",
      "Test Loss after Epoch 677: 0.8258788130283355\n",
      "9000\n",
      "Epoch 678, Loss: 0.7434298735459646\n",
      "Test Loss after Epoch 678: 0.8252710025310517\n",
      "9000\n",
      "Epoch 679, Loss: 0.7446036036676831\n",
      "Test Loss after Epoch 679: 0.8232107026576996\n",
      "9000\n",
      "Epoch 680, Loss: 0.7446738201512231\n",
      "Test Loss after Epoch 680: 0.8235578246116638\n",
      "9000\n",
      "Epoch 681, Loss: 0.7445448928011789\n",
      "Test Loss after Epoch 681: 0.8243105912208557\n",
      "9000\n",
      "Epoch 682, Loss: 0.7439383728371726\n",
      "Test Loss after Epoch 682: 0.8239582245349883\n",
      "9000\n",
      "Epoch 683, Loss: 0.743125370701154\n",
      "Test Loss after Epoch 683: 0.8260412106513977\n",
      "9000\n",
      "Epoch 684, Loss: 0.7447759759955936\n",
      "Test Loss after Epoch 684: 0.8257024493217469\n",
      "9000\n",
      "Epoch 685, Loss: 0.7434430527687073\n",
      "Test Loss after Epoch 685: 0.8255473222732544\n",
      "9000\n",
      "Epoch 686, Loss: 0.7452981845140457\n",
      "Test Loss after Epoch 686: 0.8229103326797486\n",
      "9000\n",
      "Epoch 687, Loss: 0.7443525699906879\n",
      "Test Loss after Epoch 687: 0.8209881207942963\n",
      "9000\n",
      "Epoch 688, Loss: 0.7448651236428154\n",
      "Test Loss after Epoch 688: 0.8232375044822693\n",
      "9000\n",
      "Epoch 689, Loss: 0.7450222720305125\n",
      "Test Loss after Epoch 689: 0.8235641956329346\n",
      "9000\n",
      "Epoch 690, Loss: 0.7437124479346805\n",
      "Test Loss after Epoch 690: 0.8246105558872223\n",
      "9000\n",
      "Epoch 691, Loss: 0.7468878471718894\n",
      "Test Loss after Epoch 691: 0.8219593071937561\n",
      "9000\n",
      "Epoch 692, Loss: 0.744877896560563\n",
      "Test Loss after Epoch 692: 0.8223117568492889\n",
      "9000\n",
      "Epoch 693, Loss: 0.7445026695463393\n",
      "Test Loss after Epoch 693: 0.8183841991424561\n",
      "9000\n",
      "Epoch 694, Loss: 0.745144891222318\n",
      "Test Loss after Epoch 694: 0.8187501034736633\n",
      "9000\n",
      "Epoch 695, Loss: 0.744860029578209\n",
      "Test Loss after Epoch 695: 0.8260552659034729\n",
      "9000\n",
      "Epoch 696, Loss: 0.7443388943274816\n",
      "Test Loss after Epoch 696: 0.823682739496231\n",
      "9000\n",
      "Epoch 697, Loss: 0.7438460186852349\n",
      "Test Loss after Epoch 697: 0.8190237579345703\n",
      "9000\n",
      "Epoch 698, Loss: 0.7437377338409423\n",
      "Test Loss after Epoch 698: 0.8200207562446594\n",
      "9000\n",
      "Epoch 699, Loss: 0.7423631717761358\n",
      "Test Loss after Epoch 699: 0.8169837810993195\n",
      "9000\n",
      "Epoch 700, Loss: 0.7420334943797853\n",
      "Test Loss after Epoch 700: 0.8179342718124389\n",
      "9000\n",
      "Epoch 701, Loss: 0.7433396641545825\n",
      "Test Loss after Epoch 701: 0.8259516921043396\n",
      "9000\n",
      "Epoch 702, Loss: 0.7422299934360717\n",
      "Test Loss after Epoch 702: 0.8239588754177094\n",
      "9000\n",
      "Epoch 703, Loss: 0.743525764465332\n",
      "Test Loss after Epoch 703: 0.8250030703544616\n",
      "9000\n",
      "Epoch 704, Loss: 0.7410003079440859\n",
      "Test Loss after Epoch 704: 0.825470330953598\n",
      "9000\n",
      "Epoch 705, Loss: 0.7417476403183407\n",
      "Test Loss after Epoch 705: 0.8245682480335236\n",
      "9000\n",
      "Epoch 706, Loss: 0.7415041999022166\n",
      "Test Loss after Epoch 706: 0.8252594909667968\n",
      "9000\n",
      "Epoch 707, Loss: 0.7415238441626231\n",
      "Test Loss after Epoch 707: 0.8279287433624267\n",
      "9000\n",
      "Epoch 708, Loss: 0.742284346792433\n",
      "Test Loss after Epoch 708: 0.82310387134552\n",
      "9000\n",
      "Epoch 709, Loss: 0.7411131420797772\n",
      "Test Loss after Epoch 709: 0.8251051752567291\n",
      "9000\n",
      "Epoch 710, Loss: 0.7416905458635754\n",
      "Test Loss after Epoch 710: 0.8266031403541565\n",
      "9000\n",
      "Epoch 711, Loss: 0.741440982222557\n",
      "Test Loss after Epoch 711: 0.8272086446285247\n",
      "9000\n",
      "Epoch 712, Loss: 0.7407025805181927\n",
      "Test Loss after Epoch 712: 0.8302948596477508\n",
      "9000\n",
      "Epoch 713, Loss: 0.7410823674069511\n",
      "Test Loss after Epoch 713: 0.8277575674057007\n",
      "9000\n",
      "Epoch 714, Loss: 0.7408797027534909\n",
      "Test Loss after Epoch 714: 0.823031290769577\n",
      "9000\n",
      "Epoch 715, Loss: 0.7403651342259513\n",
      "Test Loss after Epoch 715: 0.8250108222961425\n",
      "9000\n",
      "Epoch 716, Loss: 0.7407367537021637\n",
      "Test Loss after Epoch 716: 0.8303516774177552\n",
      "9000\n",
      "Epoch 717, Loss: 0.7413543549776077\n",
      "Test Loss after Epoch 717: 0.8241158716678619\n",
      "9000\n",
      "Epoch 718, Loss: 0.7417126240068012\n",
      "Test Loss after Epoch 718: 0.825466365814209\n",
      "9000\n",
      "Epoch 719, Loss: 0.7407273690170711\n",
      "Test Loss after Epoch 719: 0.8260288834571838\n",
      "9000\n",
      "Epoch 720, Loss: 0.741533732758628\n",
      "Test Loss after Epoch 720: 0.8241339159011841\n",
      "9000\n",
      "Epoch 721, Loss: 0.7417280310259925\n",
      "Test Loss after Epoch 721: 0.8262669224739074\n",
      "9000\n",
      "Epoch 722, Loss: 0.7407501264810562\n",
      "Test Loss after Epoch 722: 0.8241603970527649\n",
      "9000\n",
      "Epoch 723, Loss: 0.7396668996678458\n",
      "Test Loss after Epoch 723: 0.825127509355545\n",
      "9000\n",
      "Epoch 724, Loss: 0.7409625951449076\n",
      "Test Loss after Epoch 724: 0.8260913600921631\n",
      "9000\n",
      "Epoch 725, Loss: 0.7397016737063726\n",
      "Test Loss after Epoch 725: 0.8274017786979675\n",
      "9000\n",
      "Epoch 726, Loss: 0.74046159377363\n",
      "Test Loss after Epoch 726: 0.8316560633182526\n",
      "9000\n",
      "Epoch 727, Loss: 0.740917346186108\n",
      "Test Loss after Epoch 727: 0.8249104135036468\n",
      "9000\n",
      "Epoch 728, Loss: 0.7413809321191576\n",
      "Test Loss after Epoch 728: 0.8241339383125306\n",
      "9000\n",
      "Epoch 729, Loss: 0.7390289136303796\n",
      "Test Loss after Epoch 729: 0.8266405310630799\n",
      "9000\n",
      "Epoch 730, Loss: 0.739976718876097\n",
      "Test Loss after Epoch 730: 0.8285339343547821\n",
      "9000\n",
      "Epoch 731, Loss: 0.7388257606824239\n",
      "Test Loss after Epoch 731: 0.8249225664138794\n",
      "9000\n",
      "Epoch 732, Loss: 0.7404509143299527\n",
      "Test Loss after Epoch 732: 0.8287334337234497\n",
      "9000\n",
      "Epoch 733, Loss: 0.7385845763948229\n",
      "Test Loss after Epoch 733: 0.8255147211551667\n",
      "9000\n",
      "Epoch 734, Loss: 0.7387896245850457\n",
      "Test Loss after Epoch 734: 0.8254127719402313\n",
      "9000\n",
      "Epoch 735, Loss: 0.738491251985232\n",
      "Test Loss after Epoch 735: 0.8240740971565247\n",
      "9000\n",
      "Epoch 736, Loss: 0.7396109759012858\n",
      "Test Loss after Epoch 736: 0.8280951013565063\n",
      "9000\n",
      "Epoch 737, Loss: 0.7396823806365331\n",
      "Test Loss after Epoch 737: 0.8316611514091492\n",
      "9000\n",
      "Epoch 738, Loss: 0.7379684042003419\n",
      "Test Loss after Epoch 738: 0.8317706670761108\n",
      "9000\n",
      "Epoch 739, Loss: 0.7380323808723026\n",
      "Test Loss after Epoch 739: 0.8269710140228271\n",
      "9000\n",
      "Epoch 740, Loss: 0.7381168066793018\n",
      "Test Loss after Epoch 740: 0.828105639219284\n",
      "9000\n",
      "Epoch 741, Loss: 0.7395665076971054\n",
      "Test Loss after Epoch 741: 0.827953182220459\n",
      "9000\n",
      "Epoch 742, Loss: 0.7382014207442602\n",
      "Test Loss after Epoch 742: 0.8278050117492676\n",
      "9000\n",
      "Epoch 743, Loss: 0.7390579439798991\n",
      "Test Loss after Epoch 743: 0.8306006889343261\n",
      "9000\n",
      "Epoch 744, Loss: 0.7389085772037506\n",
      "Test Loss after Epoch 744: 0.8330538699626923\n",
      "9000\n",
      "Epoch 745, Loss: 0.7383786045577792\n",
      "Test Loss after Epoch 745: 0.8295786895751953\n",
      "9000\n",
      "Epoch 746, Loss: 0.7392235589689679\n",
      "Test Loss after Epoch 746: 0.8304732995033264\n",
      "9000\n",
      "Epoch 747, Loss: 0.7386412197086546\n",
      "Test Loss after Epoch 747: 0.8269569051265716\n",
      "9000\n",
      "Epoch 748, Loss: 0.7372527781592475\n",
      "Test Loss after Epoch 748: 0.8302969410419464\n",
      "9000\n",
      "Epoch 749, Loss: 0.7378570684459475\n",
      "Test Loss after Epoch 749: 0.8307691435813904\n",
      "9000\n",
      "Epoch 750, Loss: 0.7378054450750351\n",
      "Test Loss after Epoch 750: 0.8278823447227478\n",
      "9000\n",
      "Epoch 751, Loss: 0.740313316239251\n",
      "Test Loss after Epoch 751: 0.8282122194766999\n",
      "9000\n",
      "Epoch 752, Loss: 0.7393998426331414\n",
      "Test Loss after Epoch 752: 0.8283736219406128\n",
      "9000\n",
      "Epoch 753, Loss: 0.7383049757745531\n",
      "Test Loss after Epoch 753: 0.8297935404777527\n",
      "9000\n",
      "Epoch 754, Loss: 0.739571339143647\n",
      "Test Loss after Epoch 754: 0.8302682120800018\n",
      "9000\n",
      "Epoch 755, Loss: 0.7382289797597461\n",
      "Test Loss after Epoch 755: 0.8293422060012817\n",
      "9000\n",
      "Epoch 756, Loss: 0.7389684614075555\n",
      "Test Loss after Epoch 756: 0.8282492907047272\n",
      "9000\n",
      "Epoch 757, Loss: 0.7384315627151066\n",
      "Test Loss after Epoch 757: 0.8286700251102448\n",
      "9000\n",
      "Epoch 758, Loss: 0.7371436029805077\n",
      "Test Loss after Epoch 758: 0.8286112749576569\n",
      "9000\n",
      "Epoch 759, Loss: 0.7379599892033472\n",
      "Test Loss after Epoch 759: 0.829640634059906\n",
      "9000\n",
      "Epoch 760, Loss: 0.7371065052482817\n",
      "Test Loss after Epoch 760: 0.8295568959712982\n",
      "9000\n",
      "Epoch 761, Loss: 0.7379275016387303\n",
      "Test Loss after Epoch 761: 0.831210111618042\n",
      "9000\n",
      "Epoch 762, Loss: 0.7370238285197152\n",
      "Test Loss after Epoch 762: 0.8317945275306702\n",
      "9000\n",
      "Epoch 763, Loss: 0.7373574449751112\n",
      "Test Loss after Epoch 763: 0.833018940448761\n",
      "9000\n",
      "Epoch 764, Loss: 0.7374128686851925\n",
      "Test Loss after Epoch 764: 0.8344932465553284\n",
      "9000\n",
      "Epoch 765, Loss: 0.7361000972986221\n",
      "Test Loss after Epoch 765: 0.8383936564922333\n",
      "9000\n",
      "Epoch 766, Loss: 0.7377233262856802\n",
      "Test Loss after Epoch 766: 0.8333475971221924\n",
      "9000\n",
      "Epoch 767, Loss: 0.7363109970092774\n",
      "Test Loss after Epoch 767: 0.8345877594947815\n",
      "9000\n",
      "Epoch 768, Loss: 0.7366703874932395\n",
      "Test Loss after Epoch 768: 0.831783753156662\n",
      "9000\n",
      "Epoch 769, Loss: 0.7357003549072477\n",
      "Test Loss after Epoch 769: 0.8352045989036561\n",
      "9000\n",
      "Epoch 770, Loss: 0.7359676733944152\n",
      "Test Loss after Epoch 770: 0.8359022133350372\n",
      "9000\n",
      "Epoch 771, Loss: 0.737945596628719\n",
      "Test Loss after Epoch 771: 0.8391651594638825\n",
      "9000\n",
      "Epoch 772, Loss: 0.7372332495186064\n",
      "Test Loss after Epoch 772: 0.836700049161911\n",
      "9000\n",
      "Epoch 773, Loss: 0.7377109408378602\n",
      "Test Loss after Epoch 773: 0.8359518027305604\n",
      "9000\n",
      "Epoch 774, Loss: 0.7377857123083539\n",
      "Test Loss after Epoch 774: 0.8344626302719116\n",
      "9000\n",
      "Epoch 775, Loss: 0.7363700207074483\n",
      "Test Loss after Epoch 775: 0.8324834008216858\n",
      "Rolling back to best model from epoch 438\n",
      "Best test loss: 0.8154358453750611\n",
      "9000\n",
      "Epoch 776, Loss: 0.7608197082678477\n",
      "Test Loss after Epoch 776: 0.8176123051643371\n",
      "9000\n",
      "Epoch 777, Loss: 0.7579173230992423\n",
      "Test Loss after Epoch 777: 0.820487630367279\n",
      "9000\n",
      "Epoch 778, Loss: 0.7548021748330858\n",
      "Test Loss after Epoch 778: 0.8202653942108155\n",
      "9000\n",
      "Epoch 779, Loss: 0.7533911498387654\n",
      "Test Loss after Epoch 779: 0.8236097822189331\n",
      "9000\n",
      "Epoch 780, Loss: 0.7547807990445031\n",
      "Test Loss after Epoch 780: 0.8216695325374603\n",
      "9000\n",
      "Epoch 781, Loss: 0.754803960469034\n",
      "Test Loss after Epoch 781: 0.8266667790412903\n",
      "9000\n",
      "Epoch 782, Loss: 0.7553889178964827\n",
      "Test Loss after Epoch 782: 0.8195867249965668\n",
      "9000\n",
      "Epoch 783, Loss: 0.7544361669619878\n",
      "Test Loss after Epoch 783: 0.8244042048454284\n",
      "9000\n",
      "Epoch 784, Loss: 0.755146755165524\n",
      "Test Loss after Epoch 784: 0.8186626727581024\n",
      "9000\n",
      "Epoch 785, Loss: 0.7538754325177934\n",
      "Test Loss after Epoch 785: 0.8224388101100921\n",
      "9000\n",
      "Epoch 786, Loss: 0.7553427683512369\n",
      "Test Loss after Epoch 786: 0.8212995266914368\n",
      "9000\n",
      "Epoch 787, Loss: 0.7538729001813465\n",
      "Test Loss after Epoch 787: 0.8238198690414429\n",
      "9000\n",
      "Epoch 788, Loss: 0.7549187627103594\n",
      "Test Loss after Epoch 788: 0.8211810777187347\n",
      "9000\n",
      "Epoch 789, Loss: 0.7532705495357513\n",
      "Test Loss after Epoch 789: 0.8199646801948547\n",
      "9000\n",
      "Epoch 790, Loss: 0.7534928830597136\n",
      "Test Loss after Epoch 790: 0.8258253865242005\n",
      "9000\n",
      "Epoch 791, Loss: 0.7520503082142936\n",
      "Test Loss after Epoch 791: 0.8251350038051605\n",
      "9000\n",
      "Epoch 792, Loss: 0.7506743592156304\n",
      "Test Loss after Epoch 792: 0.8243260724544526\n",
      "9000\n",
      "Epoch 793, Loss: 0.7513393097321193\n",
      "Test Loss after Epoch 793: 0.8250272445678711\n",
      "9000\n",
      "Epoch 794, Loss: 0.7518169382678138\n",
      "Test Loss after Epoch 794: 0.8250884795188904\n",
      "9000\n",
      "Epoch 795, Loss: 0.75096312851376\n",
      "Test Loss after Epoch 795: 0.8272787673473359\n",
      "9000\n",
      "Epoch 796, Loss: 0.7516165435976452\n",
      "Test Loss after Epoch 796: 0.8276073226928711\n",
      "9000\n",
      "Epoch 797, Loss: 0.7498268734349145\n",
      "Test Loss after Epoch 797: 0.8228766238689422\n",
      "9000\n",
      "Epoch 798, Loss: 0.7480229638682472\n",
      "Test Loss after Epoch 798: 0.823509806394577\n",
      "9000\n",
      "Epoch 799, Loss: 0.7486205953359604\n",
      "Test Loss after Epoch 799: 0.8242740564346314\n",
      "9000\n",
      "Epoch 800, Loss: 0.7478089907831615\n",
      "Test Loss after Epoch 800: 0.8250001780986785\n",
      "9000\n",
      "Epoch 801, Loss: 0.7503186922735638\n",
      "Test Loss after Epoch 801: 0.8237263088226319\n",
      "9000\n",
      "Epoch 802, Loss: 0.7483576213518779\n",
      "Test Loss after Epoch 802: 0.8186035072803497\n",
      "9000\n",
      "Epoch 803, Loss: 0.747925654384825\n",
      "Test Loss after Epoch 803: 0.8230949771404267\n",
      "9000\n",
      "Epoch 804, Loss: 0.7488510421514512\n",
      "Test Loss after Epoch 804: 0.8172335751056671\n",
      "9000\n",
      "Epoch 805, Loss: 0.7477502888705996\n",
      "Test Loss after Epoch 805: 0.8265447852611542\n",
      "9000\n",
      "Epoch 806, Loss: 0.7485566058821148\n",
      "Test Loss after Epoch 806: 0.8226830213069916\n",
      "9000\n",
      "Epoch 807, Loss: 0.747411461128129\n",
      "Test Loss after Epoch 807: 0.8235237998962402\n",
      "9000\n",
      "Epoch 808, Loss: 0.7473814038965437\n",
      "Test Loss after Epoch 808: 0.8257164287567139\n",
      "9000\n",
      "Epoch 809, Loss: 0.7486231175396177\n",
      "Test Loss after Epoch 809: 0.8210148494243622\n",
      "9000\n",
      "Epoch 810, Loss: 0.7470722951756583\n",
      "Test Loss after Epoch 810: 0.8217219722270965\n",
      "9000\n",
      "Epoch 811, Loss: 0.7477187481191423\n",
      "Test Loss after Epoch 811: 0.8185738949775696\n",
      "9000\n",
      "Epoch 812, Loss: 0.7473546566300922\n",
      "Test Loss after Epoch 812: 0.8237666440010071\n",
      "9000\n",
      "Epoch 813, Loss: 0.7458180105023914\n",
      "Test Loss after Epoch 813: 0.8308241972923279\n",
      "9000\n",
      "Epoch 814, Loss: 0.7463769849008984\n",
      "Test Loss after Epoch 814: 0.8305509514808654\n",
      "9000\n",
      "Epoch 815, Loss: 0.7458736139006085\n",
      "Test Loss after Epoch 815: 0.8302462065219879\n",
      "9000\n",
      "Epoch 816, Loss: 0.7470221670866013\n",
      "Test Loss after Epoch 816: 0.8258849680423737\n",
      "9000\n",
      "Epoch 817, Loss: 0.7467835095193651\n",
      "Test Loss after Epoch 817: 0.8253346881866455\n",
      "9000\n",
      "Epoch 818, Loss: 0.7469444315963322\n",
      "Test Loss after Epoch 818: 0.824180251121521\n",
      "9000\n",
      "Epoch 819, Loss: 0.7466004666090011\n",
      "Test Loss after Epoch 819: 0.8261096179485321\n",
      "9000\n",
      "Epoch 820, Loss: 0.7459910546541214\n",
      "Test Loss after Epoch 820: 0.8231642687320709\n",
      "9000\n",
      "Epoch 821, Loss: 0.7472288140058517\n",
      "Test Loss after Epoch 821: 0.8255148468017578\n",
      "9000\n",
      "Epoch 822, Loss: 0.7461919037898381\n",
      "Test Loss after Epoch 822: 0.8259918537139893\n",
      "9000\n",
      "Epoch 823, Loss: 0.7465722810824712\n",
      "Test Loss after Epoch 823: 0.8293944487571716\n",
      "9000\n",
      "Epoch 824, Loss: 0.7449534540176391\n",
      "Test Loss after Epoch 824: 0.8285164766311646\n",
      "9000\n",
      "Epoch 825, Loss: 0.7447635414732827\n",
      "Test Loss after Epoch 825: 0.8319859914779663\n",
      "9000\n",
      "Epoch 826, Loss: 0.7460487724542618\n",
      "Test Loss after Epoch 826: 0.8242058835029602\n",
      "9000\n",
      "Epoch 827, Loss: 0.7456752715905507\n",
      "Test Loss after Epoch 827: 0.8216958022117615\n",
      "9000\n",
      "Epoch 828, Loss: 0.7442693639331394\n",
      "Test Loss after Epoch 828: 0.8258205001354217\n",
      "9000\n",
      "Epoch 829, Loss: 0.7436439270443387\n",
      "Test Loss after Epoch 829: 0.8229279539585114\n",
      "9000\n",
      "Epoch 830, Loss: 0.7426635823249816\n",
      "Test Loss after Epoch 830: 0.8269551122188568\n",
      "9000\n",
      "Epoch 831, Loss: 0.7449980131255256\n",
      "Test Loss after Epoch 831: 0.8285312297344207\n",
      "9000\n",
      "Epoch 832, Loss: 0.745028147286839\n",
      "Test Loss after Epoch 832: 0.8270243780612946\n",
      "9000\n",
      "Epoch 833, Loss: 0.7426752346754074\n",
      "Test Loss after Epoch 833: 0.8285363762378692\n",
      "9000\n",
      "Epoch 834, Loss: 0.7447494588428073\n",
      "Test Loss after Epoch 834: 0.8314070823192596\n",
      "9000\n",
      "Epoch 835, Loss: 0.7433961017131805\n",
      "Test Loss after Epoch 835: 0.830267739534378\n",
      "9000\n",
      "Epoch 836, Loss: 0.7443255127668381\n",
      "Test Loss after Epoch 836: 0.8323113634586334\n",
      "9000\n",
      "Epoch 837, Loss: 0.7430943179263009\n",
      "Test Loss after Epoch 837: 0.8292390725612641\n",
      "9000\n",
      "Epoch 838, Loss: 0.7428450349701775\n",
      "Test Loss after Epoch 838: 0.8269014477729797\n",
      "9000\n",
      "Epoch 839, Loss: 0.742431381755405\n",
      "Test Loss after Epoch 839: 0.8280863478183746\n",
      "9000\n",
      "Epoch 840, Loss: 0.743447044412295\n",
      "Test Loss after Epoch 840: 0.8251592295169831\n",
      "9000\n",
      "Epoch 841, Loss: 0.744936480972502\n",
      "Test Loss after Epoch 841: 0.8218010222911835\n",
      "9000\n",
      "Epoch 842, Loss: 0.7438707874880897\n",
      "Test Loss after Epoch 842: 0.8266380846500396\n",
      "4500\n",
      "Epoch 843, Loss: 0.7345462427404191\n",
      "Test Loss after Epoch 843: 0.828692577123642\n",
      "4500\n",
      "Epoch 844, Loss: 0.7349379189014434\n",
      "Test Loss after Epoch 844: 0.828183078289032\n",
      "4500\n",
      "Epoch 845, Loss: 0.7364701031843821\n",
      "Test Loss after Epoch 845: 0.8266190876960754\n",
      "4500\n",
      "Epoch 846, Loss: 0.7358316328525544\n",
      "Test Loss after Epoch 846: 0.8267019271850586\n",
      "4500\n",
      "Epoch 847, Loss: 0.7342883046203189\n",
      "Test Loss after Epoch 847: 0.8265264766216278\n",
      "4500\n",
      "Epoch 848, Loss: 0.7344310676521725\n",
      "Test Loss after Epoch 848: 0.8250689105987549\n",
      "4500\n",
      "Epoch 849, Loss: 0.7335187993314531\n",
      "Test Loss after Epoch 849: 0.8231846227645874\n",
      "4500\n",
      "Epoch 850, Loss: 0.7344731464915806\n",
      "Test Loss after Epoch 850: 0.8213670444488526\n",
      "4500\n",
      "Epoch 851, Loss: 0.7343862859143151\n",
      "Test Loss after Epoch 851: 0.8264717652797698\n",
      "4500\n",
      "Epoch 852, Loss: 0.7352076924641927\n",
      "Test Loss after Epoch 852: 0.8304093453884125\n",
      "4500\n",
      "Epoch 853, Loss: 0.7348748516771528\n",
      "Test Loss after Epoch 853: 0.8291893393993378\n",
      "4500\n",
      "Epoch 854, Loss: 0.7338369365798102\n",
      "Test Loss after Epoch 854: 0.8248053624629974\n",
      "4500\n",
      "Epoch 855, Loss: 0.7333725519710117\n",
      "Test Loss after Epoch 855: 0.8241057021617889\n",
      "4500\n",
      "Epoch 856, Loss: 0.7339973213407729\n",
      "Test Loss after Epoch 856: 0.8249618451595306\n",
      "4500\n",
      "Epoch 857, Loss: 0.7347006350888147\n",
      "Test Loss after Epoch 857: 0.8269163119792938\n",
      "4500\n",
      "Epoch 858, Loss: 0.7358675492604574\n",
      "Test Loss after Epoch 858: 0.8283394536972046\n",
      "4500\n",
      "Epoch 859, Loss: 0.7341867230203417\n",
      "Test Loss after Epoch 859: 0.8346931827068329\n",
      "4500\n",
      "Epoch 860, Loss: 0.7330950130886502\n",
      "Test Loss after Epoch 860: 0.8366380288600922\n",
      "4500\n",
      "Epoch 861, Loss: 0.7337221908304427\n",
      "Test Loss after Epoch 861: 0.8332403855323791\n",
      "4500\n",
      "Epoch 862, Loss: 0.7339629731178283\n",
      "Test Loss after Epoch 862: 0.8326987779140472\n",
      "4500\n",
      "Epoch 863, Loss: 0.7334292395114899\n",
      "Test Loss after Epoch 863: 0.8325514161586761\n",
      "4500\n",
      "Epoch 864, Loss: 0.7345782074663374\n",
      "Test Loss after Epoch 864: 0.8366582639217377\n",
      "4500\n",
      "Epoch 865, Loss: 0.7329643831253052\n",
      "Test Loss after Epoch 865: 0.8353368105888367\n",
      "4500\n",
      "Epoch 866, Loss: 0.7343111414114635\n",
      "Test Loss after Epoch 866: 0.8362128069400787\n",
      "4500\n",
      "Epoch 867, Loss: 0.7333841942946117\n",
      "Test Loss after Epoch 867: 0.8365902502536774\n",
      "4500\n",
      "Epoch 868, Loss: 0.731664061917199\n",
      "Test Loss after Epoch 868: 0.835774718761444\n",
      "4500\n",
      "Epoch 869, Loss: 0.7341873582998911\n",
      "Test Loss after Epoch 869: 0.8341543049812317\n",
      "4500\n",
      "Epoch 870, Loss: 0.7324617176585727\n",
      "Test Loss after Epoch 870: 0.8338009724617005\n",
      "4500\n",
      "Epoch 871, Loss: 0.7321430676248338\n",
      "Test Loss after Epoch 871: 0.8354307818412781\n",
      "4500\n",
      "Epoch 872, Loss: 0.7322271999253167\n",
      "Test Loss after Epoch 872: 0.8343813264369965\n",
      "4500\n",
      "Epoch 873, Loss: 0.7337853672504425\n",
      "Test Loss after Epoch 873: 0.8311905214786529\n",
      "4500\n",
      "Epoch 874, Loss: 0.7328340218332079\n",
      "Test Loss after Epoch 874: 0.8301048455238342\n",
      "4500\n",
      "Epoch 875, Loss: 0.7324063194327884\n",
      "Test Loss after Epoch 875: 0.830978990316391\n",
      "4500\n",
      "Epoch 876, Loss: 0.7337902942233615\n",
      "Test Loss after Epoch 876: 0.8328624799251556\n",
      "4500\n",
      "Epoch 877, Loss: 0.7332376535733541\n",
      "Test Loss after Epoch 877: 0.8360508079528809\n",
      "4500\n",
      "Epoch 878, Loss: 0.7333139322863684\n",
      "Test Loss after Epoch 878: 0.8335103650093079\n",
      "4500\n",
      "Epoch 879, Loss: 0.7332049625184801\n",
      "Test Loss after Epoch 879: 0.8309012336730957\n",
      "4500\n",
      "Epoch 880, Loss: 0.7339445583290524\n",
      "Test Loss after Epoch 880: 0.8245063652992248\n",
      "4500\n",
      "Epoch 881, Loss: 0.7325298074351416\n",
      "Test Loss after Epoch 881: 0.8234460372924804\n",
      "4500\n",
      "Epoch 882, Loss: 0.7331195760303073\n",
      "Test Loss after Epoch 882: 0.8217220723628997\n",
      "4500\n",
      "Epoch 883, Loss: 0.7324670916133457\n",
      "Test Loss after Epoch 883: 0.8258397297859192\n",
      "4500\n",
      "Epoch 884, Loss: 0.7330178448888991\n",
      "Test Loss after Epoch 884: 0.8305858149528503\n",
      "4500\n",
      "Epoch 885, Loss: 0.7318923221694098\n",
      "Test Loss after Epoch 885: 0.8332620341777801\n",
      "4500\n",
      "Epoch 886, Loss: 0.7328949944972992\n",
      "Test Loss after Epoch 886: 0.8336678426265717\n",
      "4500\n",
      "Epoch 887, Loss: 0.7321054655445947\n",
      "Test Loss after Epoch 887: 0.8351474275588989\n",
      "4500\n",
      "Epoch 888, Loss: 0.7324834212197198\n",
      "Test Loss after Epoch 888: 0.836571230173111\n",
      "4500\n",
      "Epoch 889, Loss: 0.731672628349728\n",
      "Test Loss after Epoch 889: 0.8340080089569092\n",
      "4500\n",
      "Epoch 890, Loss: 0.7326292321152157\n",
      "Test Loss after Epoch 890: 0.831903386592865\n",
      "4500\n",
      "Epoch 891, Loss: 0.7335716792212592\n",
      "Test Loss after Epoch 891: 0.8307656927108764\n",
      "4500\n",
      "Epoch 892, Loss: 0.7328053718672858\n",
      "Test Loss after Epoch 892: 0.8311308917999267\n",
      "4500\n",
      "Epoch 893, Loss: 0.7334471532503763\n",
      "Test Loss after Epoch 893: 0.8287184789180756\n",
      "4500\n",
      "Epoch 894, Loss: 0.7326959621376461\n",
      "Test Loss after Epoch 894: 0.827474880695343\n",
      "4500\n",
      "Epoch 895, Loss: 0.7321453991466098\n",
      "Test Loss after Epoch 895: 0.8309622521400452\n",
      "4500\n",
      "Epoch 896, Loss: 0.733963809993532\n",
      "Test Loss after Epoch 896: 0.8336036109924316\n",
      "4500\n",
      "Epoch 897, Loss: 0.7331990026103126\n",
      "Test Loss after Epoch 897: 0.8324500012397766\n",
      "4500\n",
      "Epoch 898, Loss: 0.7320255076885224\n",
      "Test Loss after Epoch 898: 0.8290688171386719\n",
      "4500\n",
      "Epoch 899, Loss: 0.7314911378224691\n",
      "Test Loss after Epoch 899: 0.8283948335647583\n",
      "4500\n",
      "Epoch 900, Loss: 0.7333688241905636\n",
      "Test Loss after Epoch 900: 0.8291456072330475\n",
      "4500\n",
      "Epoch 901, Loss: 0.7329830430348714\n",
      "Test Loss after Epoch 901: 0.8290197730064393\n",
      "4500\n",
      "Epoch 902, Loss: 0.7305223811202579\n",
      "Test Loss after Epoch 902: 0.830867043018341\n",
      "4500\n",
      "Epoch 903, Loss: 0.7332058568265702\n",
      "Test Loss after Epoch 903: 0.8345441522598267\n",
      "4500\n",
      "Epoch 904, Loss: 0.7314453856415218\n",
      "Test Loss after Epoch 904: 0.8360012753009796\n",
      "4500\n",
      "Epoch 905, Loss: 0.7322325225406223\n",
      "Test Loss after Epoch 905: 0.8360435309410095\n",
      "4500\n",
      "Epoch 906, Loss: 0.732842682255639\n",
      "Test Loss after Epoch 906: 0.8353509964942932\n",
      "4500\n",
      "Epoch 907, Loss: 0.732893756336636\n",
      "Test Loss after Epoch 907: 0.8323360693454742\n",
      "4500\n",
      "Epoch 908, Loss: 0.7330531016455756\n",
      "Test Loss after Epoch 908: 0.8299656848907471\n",
      "4500\n",
      "Epoch 909, Loss: 0.7318947072029114\n",
      "Test Loss after Epoch 909: 0.8337648191452026\n",
      "4500\n",
      "Epoch 910, Loss: 0.7334791899522145\n",
      "Test Loss after Epoch 910: 0.8363518719673156\n",
      "4500\n",
      "Epoch 911, Loss: 0.7319497196939256\n",
      "Test Loss after Epoch 911: 0.8312340779304505\n",
      "4500\n",
      "Epoch 912, Loss: 0.7314469819068908\n",
      "Test Loss after Epoch 912: 0.8268495395183563\n",
      "4500\n",
      "Epoch 913, Loss: 0.7328109774059719\n",
      "Test Loss after Epoch 913: 0.8253892199993134\n",
      "4500\n",
      "Epoch 914, Loss: 0.7322845214472876\n",
      "Test Loss after Epoch 914: 0.8269169824123382\n",
      "4500\n",
      "Epoch 915, Loss: 0.7323184495502048\n",
      "Test Loss after Epoch 915: 0.8293017690181732\n",
      "4500\n",
      "Epoch 916, Loss: 0.7327173789342244\n",
      "Test Loss after Epoch 916: 0.8291660878658295\n",
      "4500\n",
      "Epoch 917, Loss: 0.7317488307423062\n",
      "Test Loss after Epoch 917: 0.830646785736084\n",
      "4500\n",
      "Epoch 918, Loss: 0.7318408234649234\n",
      "Test Loss after Epoch 918: 0.833835996389389\n",
      "4500\n",
      "Epoch 919, Loss: 0.7326349220275878\n",
      "Test Loss after Epoch 919: 0.834299054145813\n",
      "4500\n",
      "Epoch 920, Loss: 0.7321877123779721\n",
      "Test Loss after Epoch 920: 0.8338282141685486\n",
      "4500\n",
      "Epoch 921, Loss: 0.7328576383855607\n",
      "Test Loss after Epoch 921: 0.8353924705982209\n",
      "4500\n",
      "Epoch 922, Loss: 0.7315885722107357\n",
      "Test Loss after Epoch 922: 0.8402241327762604\n",
      "4500\n",
      "Epoch 923, Loss: 0.7315115196704864\n",
      "Test Loss after Epoch 923: 0.8381843988895417\n",
      "4500\n",
      "Epoch 924, Loss: 0.7320776075787014\n",
      "Test Loss after Epoch 924: 0.832470698595047\n",
      "4500\n",
      "Epoch 925, Loss: 0.7323334753248427\n",
      "Test Loss after Epoch 925: 0.8292356598377227\n",
      "4500\n",
      "Epoch 926, Loss: 0.7318719823360443\n",
      "Test Loss after Epoch 926: 0.8259775280952454\n",
      "4500\n",
      "Epoch 927, Loss: 0.7319688720173306\n",
      "Test Loss after Epoch 927: 0.8267846488952637\n",
      "4500\n",
      "Epoch 928, Loss: 0.7318480619854397\n",
      "Test Loss after Epoch 928: 0.8276146969795227\n",
      "4500\n",
      "Epoch 929, Loss: 0.731008208486769\n",
      "Test Loss after Epoch 929: 0.8280991094112397\n",
      "4500\n",
      "Epoch 930, Loss: 0.7315607844458686\n",
      "Test Loss after Epoch 930: 0.8288466656208038\n",
      "4500\n",
      "Epoch 931, Loss: 0.7311921622223324\n",
      "Test Loss after Epoch 931: 0.8289576296806336\n",
      "4500\n",
      "Epoch 932, Loss: 0.7309217618041568\n",
      "Test Loss after Epoch 932: 0.8300277287960053\n",
      "4500\n",
      "Epoch 933, Loss: 0.7313978351751963\n",
      "Test Loss after Epoch 933: 0.8332221190929413\n",
      "4500\n",
      "Epoch 934, Loss: 0.7326305837631225\n",
      "Test Loss after Epoch 934: 0.8339945306777954\n",
      "4500\n",
      "Epoch 935, Loss: 0.7313530690405103\n",
      "Test Loss after Epoch 935: 0.8329414582252502\n",
      "4500\n",
      "Epoch 936, Loss: 0.7307321492036184\n",
      "Test Loss after Epoch 936: 0.8319652771949768\n"
     ]
    }
   ],
   "source": [
    "for train_size in range(1000, 1001, 100):\n",
    "    for rat in [0.2, 0.5, 1]: #0.5, 1\n",
    "        cur_train_set = train_set[:int(train_size*0.9)]\n",
    "        cur_test_set = test_set[:int(train_size*0.1)]\n",
    "        train_examples = [ex for paradigm in cur_train_set for ex in generate_examples(paradigm)]\n",
    "        test_examples = [ex for paradigm in cur_test_set for ex in generate_examples(paradigm)]\n",
    "        print(len(cur_train_set), len(train_examples), len(test_examples))\n",
    "        model = CharTransformer(vocab_size, device=device, max_len=max_len)\n",
    "        train_model(model, train_examples, test_examples, hallucination_ratio=rat, hallucination_refresh_rate=5, stop_hallucinating_after=0.9, batch_size=398, epochs=int(20000*398/(train_size*(1+rat*0.7)*5)))\n",
    "\n",
    "        save_dir = \"/home/minhk/Assignments/CSCI 5801/project/model/\"\n",
    "        filename=f\"base_transformers_hallucinate_b_{rat}_{train_size}.pth\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        torch.save(model, save_dir+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PRS>monumentalize monumentalize\n",
      "monumentalize\n",
      "<3SG>monumentalize monumentalizes\n",
      "monumentalizes\n",
      "<PST>monumentalize monumentalized\n",
      "monumentalized\n",
      "<PRS.PTCP>monumentalize monumentalizing\n",
      "monumentalizing\n",
      "<PST.PTCP>monumentalize monumentalized\n",
      "monumentalized\n"
     ]
    }
   ],
   "source": [
    "# Select a random paradigm from validation set\n",
    "random_paradigm = random.choice(val_set)\n",
    "\n",
    "# Generate examples from the paradigm\n",
    "generated_examples = generate_examples(random_paradigm)\n",
    "\n",
    "for src, tgt in generated_examples: \n",
    "    print(list_to_word(src), list_to_word(tgt))\n",
    "    feature_mask = create_feature_mask(src).unsqueeze(0)\n",
    "    src_tokenized = torch.tensor(tokenize(src, char_to_idx), device=device).unsqueeze(0)\n",
    "    gen = model.generate(src_tokenized, feature_mask, beam_size=5).squeeze(0)\n",
    "    gen = list_to_word([idx_to_char[id.item()] for id in gen])\n",
    "    print(gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass + PRS = psss, psss\n",
      "pass + 3SG = pssses, pssses\n",
      "pass + PST = pssssasass, psssspasasspas\n",
      "pass + PRS.PTCP = psssing, psssing\n",
      "pass + PST.PTCP = psssspasasspas, psssspasasspas\n"
     ]
    }
   ],
   "source": [
    "word = 'pass'\n",
    "tags = [\"PRS\", \"3SG\", \"PST\", \"PRS.PTCP\", \"PST.PTCP\"]\n",
    "\n",
    "for tag in tags: \n",
    "    tokens = [\"<s>\", f\"<{tag}>\"] + list(word) + [\"</s>\"]\n",
    "    feature_mask = create_feature_mask(tokens).unsqueeze(0)\n",
    "    src_tokenized = torch.tensor(tokenize(tokens, char_to_idx), device=device).unsqueeze(0)\n",
    "    gen = model.generate(src_tokenized, feature_mask).squeeze(0)\n",
    "    gen = list_to_word([idx_to_char[id.item()] for id in gen])\n",
    "    gen2 = model.greedy_batch_decode(src_tokenized, feature_mask).squeeze(0)\n",
    "    gen2 = list_to_word([idx_to_char[id.item()] for id in gen2])\n",
    "    print(f\"{word} + {tag} = {gen}, {gen2}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1000 loaded\n",
      "1000 0.2 0.9135\n",
      "Model trained on 1000 paradigms; hallucination ratio 0.2; 109147 predictions correct out of 119480 total. Accuracy: 0.9135\n",
      "Model 1000 loaded\n",
      "1000 0.5 0.9319\n",
      "Model trained on 1000 paradigms; hallucination ratio 0.5; 111338 predictions correct out of 119480 total. Accuracy: 0.9319\n",
      "Model 1000 loaded\n",
      "1000 1 0.9293\n",
      "Model trained on 1000 paradigms; hallucination ratio 1; 111036 predictions correct out of 119480 total. Accuracy: 0.9293\n"
     ]
    }
   ],
   "source": [
    "# Perform greedy batched decoding\n",
    "val_set = data.items()\n",
    "\n",
    "for train_size in range(1000, 1001, 100):\n",
    "    for rat in [0.2, 0.5, 1]: # 0.5, 1\n",
    "        save_dir = f\"/home/minhk/Assignments/CSCI 5801/project/model/base_transformers_hallucinate_b_{rat}_{train_size}.pth\" # oops the file names are all wrong\n",
    "\n",
    "        # Load model\n",
    "        model = torch.load(save_dir, map_location=device, weights_only=False)\n",
    "        model.to(device)\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "\n",
    "        print(f\"Model {train_size} loaded\")\n",
    "\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        batch_size = 32\n",
    "\n",
    "        all_src = []\n",
    "        all_tgt = []\n",
    "\n",
    "        for paradigm in val_set:\n",
    "            pairs = generate_examples(paradigm)\n",
    "            for src, tgt in pairs:\n",
    "                all_src.append(src)\n",
    "                all_tgt.append(tgt)\n",
    "\n",
    "        # Determine max sequence length in the batch for padding\n",
    "        def pad_sequences(sequences, pad_token, max_len=None):\n",
    "            \"\"\"Pads sequences to the max length with the given pad token.\"\"\"\n",
    "            max_len = max(len(seq) for seq in sequences)\n",
    "            return [seq + [pad_token] * (max_len - len(seq)) for seq in sequences]\n",
    "\n",
    "        # Process in batches\n",
    "        for i in range(0, len(all_src), batch_size):\n",
    "            src_batch = all_src[i:i+batch_size]\n",
    "            tgt_batch = all_tgt[i:i+batch_size]\n",
    "\n",
    "            try:\n",
    "                # Pad inputs\n",
    "                src_batch_padded = pad_sequences(src_batch, \"<pad>\")\n",
    "                tgt_batch_padded = pad_sequences(tgt_batch, \"<pad>\")  # Only for consistent tensor creation\n",
    "                \n",
    "                feature_mask = torch.stack([create_feature_mask(src) for src in src_batch_padded]).to(device)\n",
    "                src_tokenized = torch.tensor([tokenize(seq, char_to_idx) for seq in src_batch_padded], device=device)\n",
    "                \n",
    "                gen_batch = model.greedy_batch_decode(src_tokenized, feature_mask)\n",
    "                \n",
    "                for gen, tgt in zip(gen_batch, tgt_batch):\n",
    "                    gen_str = \"\".join([idx_to_char[idx.item()] for idx in gen])\n",
    "                    \n",
    "                    # Trim at first stop token\n",
    "                    gen_str = gen_str.split(\"</s>\")[0][3:]\n",
    "                    \n",
    "                    correct_predictions += (gen_str == list_to_word(tgt))\n",
    "                    total_predictions += 1\n",
    "            except:\n",
    "                print('Error')\n",
    "                pass\n",
    "\n",
    "        # Print accuracy\n",
    "        print(f\"{train_size} {rat} {correct_predictions / total_predictions:.4f}\")\n",
    "        print(f\"Model trained on {train_size} paradigms; hallucination ratio {rat}; {correct_predictions} predictions correct out of {total_predictions} total. Accuracy: {correct_predictions / total_predictions:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncorrect_predictions = 0\\ntotal_predictions = 0\\n\\nfor paradigm in val_set:\\n    pairs = generate_examples(paradigm)\\n    for src, tgt in pairs: \\n        feature_mask = create_feature_mask(src).unsqueeze(0)\\n        src_tokenized = torch.tensor(tokenize(src, char_to_idx), device=device).unsqueeze(0)\\n        gen = model.generate(src_tokenized, feature_mask).squeeze(0)\\n        gen = [idx_to_char[id.item()] for id in gen]\\n        correct_predictions += (gen == tgt)\\n        total_predictions += 1\\n\\nprint(f\"{correct_predictions} predictions correct out of {total_predictions} total. Accuracy: {correct_predictions / total_predictions}\")\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "for paradigm in val_set:\n",
    "    pairs = generate_examples(paradigm)\n",
    "    for src, tgt in pairs: \n",
    "        feature_mask = create_feature_mask(src).unsqueeze(0)\n",
    "        src_tokenized = torch.tensor(tokenize(src, char_to_idx), device=device).unsqueeze(0)\n",
    "        gen = model.generate(src_tokenized, feature_mask).squeeze(0)\n",
    "        gen = [idx_to_char[id.item()] for id in gen]\n",
    "        correct_predictions += (gen == tgt)\n",
    "        total_predictions += 1\n",
    "\n",
    "print(f\"{correct_predictions} predictions correct out of {total_predictions} total. Accuracy: {correct_predictions / total_predictions}\")\n",
    "\"\"\" "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
