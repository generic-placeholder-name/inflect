{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LRScheduler\n",
    "from torch.nn import Transformer\n",
    "import json\n",
    "import random\n",
    "import copy\n",
    "import warnings\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer model class\n",
    "\n",
    "class CharTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, nhead=4, num_layers=4, d_ff=1024, max_len=32, \n",
    "                 dropout=0.3, device=\"cuda\", pad_token_id=0, start_token_id=1, end_token_id=2):\n",
    "        super(CharTransformer, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.start_token_id = start_token_id\n",
    "        self.end_token_id = end_token_id\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model).to(device)\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout after embedding\n",
    "        \n",
    "        self.learnable_positional_encoding = nn.Parameter(torch.zeros(max_len, d_model).to(device))\n",
    "        \n",
    "        self.transformer = Transformer(\n",
    "            d_model=d_model, nhead=nhead, num_encoder_layers=num_layers, \n",
    "            num_decoder_layers=num_layers, dim_feedforward=d_ff, activation=\"gelu\", batch_first=True, \n",
    "            dropout=dropout \n",
    "        ).to(device)\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size).to(device)\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.tril(torch.ones(sz, sz))\n",
    "        return torch.log(mask).to(self.device)\n",
    "        \n",
    "    def forward(self, src, tgt, feature_mask, tgt_is_causal=False, src_mask=None, tgt_mask=None, \n",
    "                src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        src, tgt, feature_mask = src.to(self.device), tgt.to(self.device), feature_mask.to(self.device)\n",
    "        \n",
    "        # Compute embeddings and apply dropout\n",
    "        src_emb = self.embedding(src)\n",
    "        tgt_emb = self.embedding(tgt)\n",
    "        \n",
    "        src_emb = self.dropout(src_emb)\n",
    "        tgt_emb = self.dropout(tgt_emb)\n",
    "\n",
    "        # Apply positional encoding only to non-feature tokens\n",
    "        src_emb += (1 - feature_mask[:, :src.shape[1], None]) * self.learnable_positional_encoding[:src.shape[1], :]\n",
    "\n",
    "        # Compute key padding masks if not provided\n",
    "        if src_key_padding_mask is None:\n",
    "            src_key_padding_mask = (src == self.pad_token_id)  \n",
    "        if tgt_key_padding_mask is None:\n",
    "            tgt_key_padding_mask = (tgt == self.pad_token_id)  \n",
    "\n",
    "        if tgt_is_causal: \n",
    "            if tgt_mask is None: \n",
    "                tgt_mask = self._generate_square_subsequent_mask(tgt.shape[1])\n",
    "            else:\n",
    "                tgt_mask += self._generate_square_subsequent_mask(tgt.shape[1])\n",
    "\n",
    "        # Transformer forward pass\n",
    "        transformer_output = self.transformer(\n",
    "            src_emb, tgt_emb, \n",
    "            src_mask=src_mask, \n",
    "            tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            tgt_is_causal=tgt_is_causal\n",
    "        )\n",
    "\n",
    "        output = self.fc_out(transformer_output)\n",
    "        return output\n",
    "\n",
    "    \n",
    "    def generate(self, src, feature_mask, max_len=32, beam_size=5):\n",
    "        self.eval()\n",
    "        src, feature_mask = src.to(self.device), feature_mask.to(self.device)\n",
    "\n",
    "        # Initialize beams: (sequence, log probability)\n",
    "        beams = torch.full((1, 1), self.start_token_id, device=self.device)  \n",
    "        beam_scores = torch.zeros(1, device=self.device)  # Log probabilities\n",
    "\n",
    "        completed_sequences = []  # Store completed sequences\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            # Expand `src` to match the number of beams\n",
    "            src_expanded = src.expand(beams.shape[0], -1)\n",
    "            feature_mask_expanded = feature_mask.expand(beams.shape[0], -1)\n",
    "\n",
    "            # Forward pass on all beams at once\n",
    "            out = self.forward(src_expanded, beams, feature_mask_expanded)  \n",
    "            logits = out[:, -1, :]  # Get last-step logits (shape: [beams, vocab_size])\n",
    "            log_probs = torch.log_softmax(logits, dim=-1)  # Convert logits to log-probabilities\n",
    "\n",
    "            # Get top-k candidates for each beam (shape: [beams, beam_size])\n",
    "            topk_log_probs, topk_ids = log_probs.topk(beam_size, dim=-1)\n",
    "\n",
    "            # Compute new scores by adding log probabilities (broadcasted)\n",
    "            expanded_scores = beam_scores.unsqueeze(1) + topk_log_probs  # Shape: [beams, beam_size]\n",
    "            expanded_scores = expanded_scores.view(-1)  # Flatten to [beams * beam_size]\n",
    "\n",
    "            # Get top-k overall candidates\n",
    "            topk_scores, topk_indices = expanded_scores.topk(beam_size)\n",
    "\n",
    "            # Convert flat indices to beam/token indices\n",
    "            beam_indices = topk_indices // beam_size  # Which original beam did this come from?\n",
    "            token_indices = topk_indices % beam_size  # Which token was selected?\n",
    "\n",
    "            # Append new tokens to sequences\n",
    "            new_beams = torch.cat([beams[beam_indices], topk_ids.view(-1, 1)[topk_indices]], dim=-1)\n",
    "\n",
    "            # Check for completed sequences\n",
    "            eos_mask = (new_beams[:, -1] == self.end_token_id)\n",
    "            if eos_mask.any():\n",
    "                for i in range(beam_size):\n",
    "                    if eos_mask[i]:\n",
    "                        completed_sequences.append((new_beams[i], topk_scores[i]))\n",
    "\n",
    "            # Keep only unfinished sequences\n",
    "            beams = new_beams[~eos_mask]\n",
    "            beam_scores = topk_scores[~eos_mask]\n",
    "\n",
    "            # If all sequences finished, stop early\n",
    "            if len(beams) == 0 or len(completed_sequences) >= beam_size:\n",
    "                break\n",
    "\n",
    "        # Choose the best sequence from completed ones\n",
    "        if completed_sequences:\n",
    "            best_sequence = max(completed_sequences, key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            best_sequence = beams[0]  # If no sequence completed, return best unfinished one\n",
    "\n",
    "        return best_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples: [(['<s>', '<PRS>', 'u', 'n', 's', 't', 'a', 'b', 'i', 'l', 'i', 'z', 'e', '</s>'], ['<s>', 'u', 'n', 's', 't', 'a', 'b', 'i', 'l', 'i', 'z', 'e', '</s>']), (['<s>', '<3SG>', 'u', 'n', 's', 't', 'a', 'b', 'i', 'l', 'i', 'z', 'e', '</s>'], ['<s>', 'u', 'n', 's', 't', 'a', 'b', 'i', 'l', 'i', 'z', 'e', 's', '</s>']), (['<s>', '<PST>', 'u', 'n', 's', 't', 'a', 'b', 'i', 'l', 'i', 'z', 'e', '</s>'], ['<s>', 'u', 'n', 's', 't', 'a', 'b', 'i', 'l', 'i', 'z', 'e', 'd', '</s>']), (['<s>', '<PRS.PTCP>', 'u', 'n', 's', 't', 'a', 'b', 'i', 'l', 'i', 'z', 'e', '</s>'], ['<s>', 'u', 'n', 's', 't', 'a', 'b', 'i', 'l', 'i', 'z', 'i', 'n', 'g', '</s>']), (['<s>', '<PST.PTCP>', 'u', 'n', 's', 't', 'a', 'b', 'i', 'l', 'i', 'z', 'e', '</s>'], ['<s>', 'u', 'n', 's', 't', 'a', 'b', 'i', 'l', 'i', 'z', 'e', 'd', '</s>'])]\n",
      "Test examples: [(['<s>', '<PRS>', 's', 'i', 'd', 'e', 's', 't', 'e', 'p', '</s>'], ['<s>', 's', 'i', 'd', 'e', 's', 't', 'e', 'p', '</s>']), (['<s>', '<3SG>', 's', 'i', 'd', 'e', 's', 't', 'e', 'p', '</s>'], ['<s>', 's', 'i', 'd', 'e', 's', 't', 'e', 'p', 's', '</s>']), (['<s>', '<PST>', 's', 'i', 'd', 'e', 's', 't', 'e', 'p', '</s>'], ['<s>', 's', 'i', 'd', 'e', 's', 't', 'e', 'p', 'p', 'e', 'd', '</s>']), (['<s>', '<PRS.PTCP>', 's', 'i', 'd', 'e', 's', 't', 'e', 'p', '</s>'], ['<s>', 's', 'i', 'd', 'e', 's', 't', 'e', 'p', 'p', 'i', 'n', 'g', '</s>']), (['<s>', '<PST.PTCP>', 's', 'i', 'd', 'e', 's', 't', 'e', 'p', '</s>'], ['<s>', 's', 'i', 'd', 'e', 's', 't', 'e', 'p', 'p', 'e', 'd', '</s>'])]\n",
      "Validation examples: [(['<s>', '<PRS>', 'a', 's', 's', 'a', 'i', 'l', 'e', '</s>'], ['<s>', 'a', 's', 's', 'a', 'i', 'l', 'e', '</s>']), (['<s>', '<3SG>', 'a', 's', 's', 'a', 'i', 'l', 'e', '</s>'], ['<s>', 'a', 's', 's', 'a', 'i', 'l', 'e', 's', '</s>']), (['<s>', '<PST>', 'a', 's', 's', 'a', 'i', 'l', 'e', '</s>'], ['<s>', 'a', 's', 's', 'a', 'i', 'l', 'e', 'd', '</s>']), (['<s>', '<PRS.PTCP>', 'a', 's', 's', 'a', 'i', 'l', 'e', '</s>'], ['<s>', 'a', 's', 's', 'a', 'i', 'l', 'i', 'n', 'g', '</s>']), (['<s>', '<PST.PTCP>', 'a', 's', 's', 'a', 'i', 'l', 'e', '</s>'], ['<s>', 'a', 's', 's', 'a', 'i', 'l', 'e', 'd', '</s>'])]\n"
     ]
    }
   ],
   "source": [
    "# Data preparation\n",
    "\n",
    "def prepare_data(data, val_size=1000, train_size=2000, test_ratio=0.4):\n",
    "    paradigms = list(data.items())\n",
    "    random.shuffle(paradigms)\n",
    "    \n",
    "    val_set = paradigms[:val_size]\n",
    "    train_test_set = paradigms[val_size:val_size + train_size]\n",
    "    \n",
    "    train_size = int((1 - test_ratio) * len(train_test_set))\n",
    "    train_set = train_test_set[:train_size]\n",
    "    test_set = train_test_set[train_size:]\n",
    "    \n",
    "    return train_set, test_set, val_set\n",
    "\n",
    "def generate_examples(paradigm):\n",
    "    lemma = list(paradigm[0])  # Convert lemma to list of characters\n",
    "    forms = paradigm[1]\n",
    "    examples = []\n",
    "    \n",
    "    for tag, form in forms.items():\n",
    "        src = ['<s>', f'<{tag}>'] + lemma + ['</s>']\n",
    "        tgt = ['<s>'] + list(form) + ['</s>']\n",
    "        examples.append((src, tgt))\n",
    "    \n",
    "    return examples\n",
    "\n",
    "# Load dataset\n",
    "with open(\"/home/minhk/Assignments/CSCI 5801/project/data/processed/eng_v.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "train_set, test_set, val_set = prepare_data(data, val_size=1000, train_size=1000, test_ratio=0.1)\n",
    "\n",
    "train_examples = [ex for paradigm in train_set for ex in generate_examples(paradigm)]\n",
    "test_examples = [ex for paradigm in test_set for ex in generate_examples(paradigm)]\n",
    "val_examples = [ex for paradigm in val_set for ex in generate_examples(paradigm)]\n",
    "\n",
    "print(\"Train examples:\", train_examples[:5])  # Show first 5 examples\n",
    "print(\"Test examples:\", test_examples[:5])\n",
    "print(\"Validation examples:\", val_examples[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InverseSquareLRWithWarmup(LRScheduler):\n",
    "    \"\"\"\n",
    "    Implements an inverse square learning rate scheduler with warmup steps.\n",
    "    \n",
    "    During warmup, the learning rate increases linearly from init_lr to max_lr.\n",
    "    After warmup, the learning rate decreases according to the inverse square of the step number:\n",
    "    lr = max_lr * (warmup_steps / step)^2 for step > warmup_steps\n",
    "    \n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        init_lr (float): Initial learning rate during warmup phase. Default: 0.0\n",
    "        max_lr (float): Maximum learning rate after warmup phase. Default: 0.1\n",
    "        warmup_steps (int): Number of warmup steps. Default: 1000\n",
    "        last_epoch (int): The index of the last epoch. Default: -1\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer, init_lr=0.0, max_lr=0.001, warmup_steps=1000, last_epoch=-1):\n",
    "        self.init_lr = init_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        super(InverseSquareLRWithWarmup, self).__init__(optimizer, last_epoch)\n",
    "        \n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\")\n",
    "        \n",
    "        if self.last_epoch < self.warmup_steps:\n",
    "            # Linear warmup phase\n",
    "            alpha = self.last_epoch / self.warmup_steps\n",
    "            return [self.init_lr + alpha * (self.max_lr - self.init_lr) for _ in self.base_lrs]\n",
    "        else:\n",
    "            # Inverse square decay phase\n",
    "            decay_factor = math.sqrt(self.warmup_steps / self.last_epoch)\n",
    "            return [self.max_lr * decay_factor for _ in self.base_lrs]\n",
    "            \n",
    "    def _get_closed_form_lr(self):\n",
    "        if self.last_epoch < self.warmup_steps:\n",
    "            # Linear warmup phase\n",
    "            alpha = self.last_epoch / self.warmup_steps\n",
    "            return [self.init_lr + alpha * (self.max_lr - self.init_lr) for _ in self.base_lrs]\n",
    "        else:\n",
    "            # Inverse square decay phase\n",
    "            decay_factor = (self.warmup_steps / self.last_epoch) ** 2\n",
    "            return [self.max_lr * decay_factor for _ in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampler for scheduled learning, gradually replaces ground truth (teacher forcing) with model input\n",
    "\n",
    "class ScheduledSampler():\n",
    "    def __init__(self, base_rate=0.5, warmup_steps=1000):\n",
    "        self.base_rate = base_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.step_count = 0        \n",
    "        self.sampling_rate = 1\n",
    "    \n",
    "    def step(self):\n",
    "        self.step_count += 1\n",
    "        if self.step_count > self.warmup_steps:\n",
    "            self.sampling_rate = self.base_rate + (1 - self.base_rate) * math.sqrt(self.warmup_steps / self.step_count)\n",
    "\n",
    "    def sample(self, logits, truth_ids):\n",
    "        \"\"\"\n",
    "        Selects truth_ids with probability `sampling_rate`, \n",
    "        otherwise samples using Gumbel noise.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, vocab_size = logits.shape\n",
    "        \n",
    "        # Decide per-token whether to take ground truth (1) or Gumbel sample (0)\n",
    "        mask = torch.bernoulli(torch.full((batch_size, seq_len), self.sampling_rate, device=logits.device, dtype=float)).bool()\n",
    "        \n",
    "        # Gumbel-sampled predictions\n",
    "        gumbel_preds = self._gumbel_sample(logits)\n",
    "        \n",
    "        # Use ground truth where mask == True, else use gumbel_preds\n",
    "        return torch.where(mask, truth_ids, gumbel_preds)\n",
    "    \n",
    "    def _gumbel_sample(self, logits):\n",
    "        gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits)))  # Generate Gumbel noise\n",
    "        return (logits + gumbel_noise).argmax(dim=-1)  # Apply Gumbel noise and take the argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "def tokenize(sequence, char_to_idx):\n",
    "    return [char_to_idx[char] for char in sequence]\n",
    "\n",
    "# Build character vocabulary\n",
    "all_chars = set()\n",
    "for ex in train_examples + test_examples + val_examples:\n",
    "    all_chars.update(ex[0])\n",
    "    all_chars.update(ex[1])\n",
    "all_chars.remove('<s>')\n",
    "all_chars.remove('</s>')\n",
    "char_to_idx = {char: i for i, char in enumerate(sorted(all_chars), start=3)}  # Reserve 0, 1, 2 for special tokens\n",
    "char_to_idx['<pad>'] = 0\n",
    "char_to_idx['<s>'] = 1\n",
    "char_to_idx['</s>'] = 2\n",
    "idx_to_char = {\n",
    "    i: char for char, i in char_to_idx.items()\n",
    "}\n",
    "vocab_size = len(char_to_idx)\n",
    "max_len = 32\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CharTransformer(vocab_size, device=device, max_len=max_len)\n",
    "\n",
    "def pad_sequence(sequence, max_len, pad_token='<pad>'):\n",
    "    return sequence + [pad_token] * (max_len - len(sequence))\n",
    "\n",
    "def create_feature_mask(sequence):\n",
    "    \"\"\"Create a feature mask where tags (enclosed in < >) are marked as 1, else 0.\"\"\"\n",
    "    return torch.tensor([1 if char.startswith('<') and char.endswith('>') else 0 for char in sequence], device=device)\n",
    "\n",
    "def create_padding_mask(sequence, pad_token='<pad>'):\n",
    "    \"\"\"Create a padding mask where padding tokens are marked as True (to be ignored).\"\"\"\n",
    "    return (sequence == pad_token)\n",
    "\n",
    "def train_model(model, train_examples, test_examples, epochs=1000, batch_size=256, patience=20):\n",
    "    optimizer = optim.AdamW(model.parameters(), betas=(0.99, 0.98))\n",
    "    scheduler = InverseSquareLRWithWarmup(optimizer, init_lr=1e-5, max_lr=1e-3, warmup_steps=4000)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    sampler = ScheduledSampler(base_rate=0.5, warmup_steps=4000)\n",
    "\n",
    "    pad_token = char_to_idx['<pad>']\n",
    "    best_test_loss = float('inf')  # Initialize the best test loss to a very large value\n",
    "    best_model_state = copy.deepcopy(model.state_dict())  # Store best model parameters\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        random.shuffle(train_examples)\n",
    "        \n",
    "        for i in range(0, len(train_examples), batch_size):\n",
    "            batch = train_examples[i:i+batch_size]\n",
    "            src_batch, tgt_batch = zip(*batch)\n",
    "\n",
    "            max_batch_len = max(max(len(s) for s in src_batch), max(len(t) for t in tgt_batch))\n",
    "            \n",
    "            # Pad sequences to the maximum length (max_len) in the batch\n",
    "            src_padded = [pad_sequence(seq, max_batch_len) for seq in src_batch]\n",
    "            tgt_padded = [pad_sequence(seq, max_batch_len) for seq in tgt_batch]\n",
    "            \n",
    "            # Convert padded sequences to tensors\n",
    "            src_tensor = torch.tensor([tokenize(seq, char_to_idx) for seq in src_padded], device=device)\n",
    "            tgt_tensor = torch.tensor([tokenize(seq, char_to_idx) for seq in tgt_padded], device=device)\n",
    "            \n",
    "            # Create the feature mask\n",
    "            feature_mask_src = torch.stack([create_feature_mask(seq) for seq in src_padded], dim=0)\n",
    "\n",
    "            # Shift target tensor for teacher forcing (the model will predict next token)\n",
    "            tgt_input = tgt_tensor[:, :-1]  # Remove the last token (it's not used as input)\n",
    "            tgt_expected = tgt_tensor[:, 1:]  # The target sequence for the loss is shifted by 1\n",
    "\n",
    "            # First round of predictions (using teacher forcing)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src_tensor, tgt_input, feature_mask_src, tgt_is_causal=True)\n",
    "\n",
    "            # Sample from model output and true target based on current sampling rate\n",
    "            sampled_tgt_input = sampler.sample(output, tgt_expected)\n",
    "\n",
    "            # Ensure proper alignment for next round of input:\n",
    "            # Take the first element of each item in the target sequence (start token)\n",
    "            # Concatenate with the sampled output (excluding the last token)\n",
    "            sampled_tgt_input = torch.cat(\n",
    "                [tgt_input[:, :1], sampled_tgt_input[:, :-1]], dim=1\n",
    "            )\n",
    "\n",
    "            # Second round of predictions using the sampled input\n",
    "            output = model(src_tensor, sampled_tgt_input, feature_mask_src, tgt_is_causal=True)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(output.reshape(-1, vocab_size), tgt_expected.reshape(-1))\n",
    "\n",
    "            # Apply padding mask to loss (ignores padded tokens)\n",
    "            tgt_mask = (tgt_input != pad_token).float().view(-1)\n",
    "            loss = loss * tgt_mask  # Element-wise multiply with the mask to ignore padding tokens\n",
    "            loss = loss.sum() / tgt_mask.sum()  # Normalize the loss (average over non-padding tokens)\n",
    "            \n",
    "            # Apply loss\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            sampler.step()  # Update the sampler (teacher forcing rate)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss * batch_size / len(train_examples)}\")\n",
    "        \n",
    "        # Evaluate on the test set\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(test_examples), batch_size):\n",
    "                batch = test_examples[i:i+batch_size]\n",
    "                src_batch, tgt_batch = zip(*batch)\n",
    "\n",
    "                max_batch_len = max(max(len(s) for s in src_batch), max(len(t) for t in tgt_batch))\n",
    "\n",
    "                # Pad sequences to the maximum length (max_len) in the batch\n",
    "                src_padded = [pad_sequence(seq, max_batch_len) for seq in src_batch]\n",
    "                tgt_padded = [pad_sequence(seq, max_batch_len) for seq in tgt_batch]\n",
    "\n",
    "                # Convert padded sequences to tensors\n",
    "                src_tensor = torch.tensor([tokenize(seq, char_to_idx) for seq in src_padded], device=device)\n",
    "                tgt_tensor = torch.tensor([tokenize(seq, char_to_idx) for seq in tgt_padded], device=device)\n",
    "\n",
    "                # Create the feature mask\n",
    "                feature_mask_src = torch.stack([create_feature_mask(seq) for seq in src_padded], dim=0)\n",
    "\n",
    "                # Shift target tensor\n",
    "                tgt_input = tgt_tensor[:, :-1]\n",
    "                tgt_expected = tgt_tensor[:, 1:]\n",
    "\n",
    "                # Forward pass\n",
    "                output = model(src_tensor, tgt_input, feature_mask_src, tgt_is_causal=True)\n",
    "\n",
    "                # Compute the loss\n",
    "                loss = criterion(output.reshape(-1, vocab_size), tgt_expected.reshape(-1))\n",
    "\n",
    "                # Apply padding mask to loss\n",
    "                tgt_mask = (tgt_expected != pad_token).float().view(-1)\n",
    "                loss = loss * tgt_mask  # Element-wise multiply with the mask to ignore padding tokens\n",
    "                loss = loss.sum() / tgt_mask.sum()  # Normalize the loss (average over non-padding tokens)\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "\n",
    "        test_loss = test_loss * batch_size / len(test_examples)\n",
    "        print(f\"Test Loss after Epoch {epoch+1}: {test_loss}\")\n",
    "\n",
    "        # Early stopping based on test set loss\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            best_epoch = epoch\n",
    "            patience_count = 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())  # Save best model state\n",
    "        else:\n",
    "            patience_count += 1\n",
    "            if patience_count == patience:\n",
    "                patience_count = 0\n",
    "                patience = int(patience * math.sqrt(2))\n",
    "                # Rollback to best model state (undo last epoch update)\n",
    "                model.load_state_dict(best_model_state)\n",
    "                print(f\"Rolling back to best model from epoch {best_epoch + 1}\")\n",
    "                print(f\"Best test loss: {best_test_loss}\")\n",
    "        \n",
    "    model.load_state_dict(best_model_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhk/Assignments/CSCI 5801/project/venv/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3.297080620659722\n",
      "Test Loss after Epoch 1: 2.911204833984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhk/Assignments/CSCI 5801/project/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 2.6395516493055555\n",
      "Test Loss after Epoch 2: 2.750455322265625\n",
      "Epoch 3, Loss: 2.504649386935764\n",
      "Test Loss after Epoch 3: 2.3752802734375\n",
      "Epoch 4, Loss: 2.325970458984375\n",
      "Test Loss after Epoch 4: 2.2539462890625\n",
      "Epoch 5, Loss: 2.163264655219184\n",
      "Test Loss after Epoch 5: 2.1372369384765624\n",
      "Epoch 6, Loss: 2.049650614420573\n",
      "Test Loss after Epoch 6: 2.0540411987304688\n",
      "Epoch 7, Loss: 1.9592076348198784\n",
      "Test Loss after Epoch 7: 2.0184488525390627\n",
      "Epoch 8, Loss: 1.9218873833550347\n",
      "Test Loss after Epoch 8: 1.9830564575195313\n",
      "Epoch 9, Loss: 1.8567845052083334\n",
      "Test Loss after Epoch 9: 1.8948652954101564\n",
      "Epoch 10, Loss: 1.7878001641167536\n",
      "Test Loss after Epoch 10: 1.86536474609375\n",
      "Epoch 11, Loss: 1.7494991590711806\n",
      "Test Loss after Epoch 11: 1.8282542724609374\n",
      "Epoch 12, Loss: 1.7031078016493055\n",
      "Test Loss after Epoch 12: 1.8113834838867187\n",
      "Epoch 13, Loss: 1.6500536159939236\n",
      "Test Loss after Epoch 13: 1.7835491943359374\n",
      "Epoch 14, Loss: 1.621664781358507\n",
      "Test Loss after Epoch 14: 1.763561279296875\n",
      "Epoch 15, Loss: 1.609550042046441\n",
      "Test Loss after Epoch 15: 1.7218252563476562\n",
      "Epoch 16, Loss: 1.581505140516493\n",
      "Test Loss after Epoch 16: 1.7101854248046875\n",
      "Epoch 17, Loss: 1.5413590087890625\n",
      "Test Loss after Epoch 17: 1.6775382690429688\n",
      "Epoch 18, Loss: 1.5364774373372396\n",
      "Test Loss after Epoch 18: 1.6479608764648437\n",
      "Epoch 19, Loss: 1.4995471869574653\n",
      "Test Loss after Epoch 19: 1.6269960327148438\n",
      "Epoch 20, Loss: 1.4900384928385417\n",
      "Test Loss after Epoch 20: 1.6012803955078125\n",
      "Epoch 21, Loss: 1.4735910034179687\n",
      "Test Loss after Epoch 21: 1.5736439208984374\n",
      "Epoch 22, Loss: 1.4338826836480034\n",
      "Test Loss after Epoch 22: 1.5487249145507813\n",
      "Epoch 23, Loss: 1.4172283053927952\n",
      "Test Loss after Epoch 23: 1.535648681640625\n",
      "Epoch 24, Loss: 1.3940407307942708\n",
      "Test Loss after Epoch 24: 1.5279591064453124\n",
      "Epoch 25, Loss: 1.389578633626302\n",
      "Test Loss after Epoch 25: 1.5097611694335937\n",
      "Epoch 26, Loss: 1.3769417928059895\n",
      "Test Loss after Epoch 26: 1.4927792358398437\n",
      "Epoch 27, Loss: 1.3514254353841146\n",
      "Test Loss after Epoch 27: 1.4747857055664062\n",
      "Epoch 28, Loss: 1.329638414171007\n",
      "Test Loss after Epoch 28: 1.4705323486328126\n",
      "Epoch 29, Loss: 1.3068010457356771\n",
      "Test Loss after Epoch 29: 1.4478018798828125\n",
      "Epoch 30, Loss: 1.2980950046115451\n",
      "Test Loss after Epoch 30: 1.4322748413085937\n",
      "Epoch 31, Loss: 1.2954453531901042\n",
      "Test Loss after Epoch 31: 1.4333185424804689\n",
      "Epoch 32, Loss: 1.268634738498264\n",
      "Test Loss after Epoch 32: 1.429609375\n",
      "Epoch 33, Loss: 1.2581032104492187\n",
      "Test Loss after Epoch 33: 1.4132869873046876\n",
      "Epoch 34, Loss: 1.2368498331705728\n",
      "Test Loss after Epoch 34: 1.4126991577148438\n",
      "Epoch 35, Loss: 1.2377209065755208\n",
      "Test Loss after Epoch 35: 1.404961181640625\n",
      "Epoch 36, Loss: 1.2271346164279513\n",
      "Test Loss after Epoch 36: 1.4024364013671875\n",
      "Epoch 37, Loss: 1.2117862616644965\n",
      "Test Loss after Epoch 37: 1.3983016967773438\n",
      "Epoch 38, Loss: 1.2025079142252604\n",
      "Test Loss after Epoch 38: 1.3839628295898438\n",
      "Epoch 39, Loss: 1.1849788275824653\n",
      "Test Loss after Epoch 39: 1.38106396484375\n",
      "Epoch 40, Loss: 1.1809158935546875\n",
      "Test Loss after Epoch 40: 1.3704597778320313\n",
      "Epoch 41, Loss: 1.1647382134331596\n",
      "Test Loss after Epoch 41: 1.37527587890625\n",
      "Epoch 42, Loss: 1.1598121948242188\n",
      "Test Loss after Epoch 42: 1.3625701293945311\n",
      "Epoch 43, Loss: 1.1450206909179688\n",
      "Test Loss after Epoch 43: 1.3508719482421876\n",
      "Epoch 44, Loss: 1.1372288750542534\n",
      "Test Loss after Epoch 44: 1.3455416870117187\n",
      "Epoch 45, Loss: 1.1368584798177084\n",
      "Test Loss after Epoch 45: 1.34620068359375\n",
      "Epoch 46, Loss: 1.1297931382921007\n",
      "Test Loss after Epoch 46: 1.3484104614257812\n",
      "Epoch 47, Loss: 1.114670627170139\n",
      "Test Loss after Epoch 47: 1.3378377685546874\n",
      "Epoch 48, Loss: 1.110068820529514\n",
      "Test Loss after Epoch 48: 1.3341276245117188\n",
      "Epoch 49, Loss: 1.1052966918945313\n",
      "Test Loss after Epoch 49: 1.33141943359375\n",
      "Epoch 50, Loss: 1.0975651516384548\n",
      "Test Loss after Epoch 50: 1.3413070678710937\n",
      "Epoch 51, Loss: 1.0969265916612414\n",
      "Test Loss after Epoch 51: 1.3226657104492188\n",
      "Epoch 52, Loss: 1.0760102030436198\n",
      "Test Loss after Epoch 52: 1.3307982788085937\n",
      "Epoch 53, Loss: 1.0680235188802083\n",
      "Test Loss after Epoch 53: 1.3211082763671875\n",
      "Epoch 54, Loss: 1.0614149881998698\n",
      "Test Loss after Epoch 54: 1.326736083984375\n",
      "Epoch 55, Loss: 1.0619596557617188\n",
      "Test Loss after Epoch 55: 1.311603271484375\n",
      "Epoch 56, Loss: 1.0479544949001736\n",
      "Test Loss after Epoch 56: 1.3136978759765625\n",
      "Epoch 57, Loss: 1.0420997890896266\n",
      "Test Loss after Epoch 57: 1.31293115234375\n",
      "Epoch 58, Loss: 1.0374219190809462\n",
      "Test Loss after Epoch 58: 1.3113746337890626\n",
      "Epoch 59, Loss: 1.0282364942762587\n",
      "Test Loss after Epoch 59: 1.3071688232421874\n",
      "Epoch 60, Loss: 1.0272129550509983\n",
      "Test Loss after Epoch 60: 1.3059208374023437\n",
      "Epoch 61, Loss: 1.0137622985839845\n",
      "Test Loss after Epoch 61: 1.31096630859375\n",
      "Epoch 62, Loss: 1.0101106906467014\n",
      "Test Loss after Epoch 62: 1.2974017944335938\n",
      "Epoch 63, Loss: 1.0072865702311198\n",
      "Test Loss after Epoch 63: 1.2921173706054687\n",
      "Epoch 64, Loss: 0.9941336263020834\n",
      "Test Loss after Epoch 64: 1.2823953247070312\n",
      "Epoch 65, Loss: 0.989282975938585\n",
      "Test Loss after Epoch 65: 1.2844451904296874\n",
      "Epoch 66, Loss: 0.9822116868760851\n",
      "Test Loss after Epoch 66: 1.2659752197265626\n",
      "Epoch 67, Loss: 0.9671080220540365\n",
      "Test Loss after Epoch 67: 1.2437195434570312\n",
      "Epoch 68, Loss: 0.9709705030653212\n",
      "Test Loss after Epoch 68: 1.2367052612304688\n",
      "Epoch 69, Loss: 0.9621448838975695\n",
      "Test Loss after Epoch 69: 1.2485442504882813\n",
      "Epoch 70, Loss: 0.9549224209255642\n",
      "Test Loss after Epoch 70: 1.2147437133789063\n",
      "Epoch 71, Loss: 0.9461096700032552\n",
      "Test Loss after Epoch 71: 1.2027374267578126\n",
      "Epoch 72, Loss: 0.9361142272949219\n",
      "Test Loss after Epoch 72: 1.1926000366210938\n",
      "Epoch 73, Loss: 0.9323927171495225\n",
      "Test Loss after Epoch 73: 1.1733474731445312\n",
      "Epoch 74, Loss: 0.9215733642578126\n",
      "Test Loss after Epoch 74: 1.1807906494140625\n",
      "Epoch 75, Loss: 0.9148219706217448\n",
      "Test Loss after Epoch 75: 1.1730326538085938\n",
      "Epoch 76, Loss: 0.9062311367458767\n",
      "Test Loss after Epoch 76: 1.1498759765625\n",
      "Epoch 77, Loss: 0.9007708062065972\n",
      "Test Loss after Epoch 77: 1.154693359375\n",
      "Epoch 78, Loss: 0.8946054450141059\n",
      "Test Loss after Epoch 78: 1.13025732421875\n",
      "Epoch 79, Loss: 0.8899446343315972\n",
      "Test Loss after Epoch 79: 1.126027099609375\n",
      "Epoch 80, Loss: 0.8868143513997396\n",
      "Test Loss after Epoch 80: 1.1152116088867188\n",
      "Epoch 81, Loss: 0.8729317593044705\n",
      "Test Loss after Epoch 81: 1.120240234375\n",
      "Epoch 82, Loss: 0.8663844163682726\n",
      "Test Loss after Epoch 82: 1.103049072265625\n",
      "Epoch 83, Loss: 0.8617435234917534\n",
      "Test Loss after Epoch 83: 1.0877604370117187\n",
      "Epoch 84, Loss: 0.8559400024414062\n",
      "Test Loss after Epoch 84: 1.0809049682617187\n",
      "Epoch 85, Loss: 0.8519464043511285\n",
      "Test Loss after Epoch 85: 1.0959066162109374\n",
      "Epoch 86, Loss: 0.8441371154785157\n",
      "Test Loss after Epoch 86: 1.0900762329101563\n",
      "Epoch 87, Loss: 0.8383209737141927\n",
      "Test Loss after Epoch 87: 1.1051585693359376\n",
      "Epoch 88, Loss: 0.8321962788899739\n",
      "Test Loss after Epoch 88: 1.0710510864257812\n",
      "Epoch 89, Loss: 0.8282377624511719\n",
      "Test Loss after Epoch 89: 1.0945188598632813\n",
      "Epoch 90, Loss: 0.8217439134385851\n",
      "Test Loss after Epoch 90: 1.0698134155273438\n",
      "Epoch 91, Loss: 0.8195445522732205\n",
      "Test Loss after Epoch 91: 1.0570052490234374\n",
      "Epoch 92, Loss: 0.8160470987955729\n",
      "Test Loss after Epoch 92: 1.06293115234375\n",
      "Epoch 93, Loss: 0.8118461371527778\n",
      "Test Loss after Epoch 93: 1.0467068481445312\n",
      "Epoch 94, Loss: 0.8075184156629774\n",
      "Test Loss after Epoch 94: 1.0557885131835938\n",
      "Epoch 95, Loss: 0.8035356309678819\n",
      "Test Loss after Epoch 95: 1.0437554931640625\n",
      "Epoch 96, Loss: 0.7972848409016927\n",
      "Test Loss after Epoch 96: 1.03868310546875\n",
      "Epoch 97, Loss: 0.7963811747233073\n",
      "Test Loss after Epoch 97: 1.0398572387695313\n",
      "Epoch 98, Loss: 0.7900900132921007\n",
      "Test Loss after Epoch 98: 1.0320076904296875\n",
      "Epoch 99, Loss: 0.7893519727918836\n",
      "Test Loss after Epoch 99: 1.021374237060547\n",
      "Epoch 100, Loss: 0.7848593614366319\n",
      "Test Loss after Epoch 100: 1.0151703186035157\n",
      "Epoch 101, Loss: 0.781134779188368\n",
      "Test Loss after Epoch 101: 1.0344134521484376\n",
      "Epoch 102, Loss: 0.7804875149197048\n",
      "Test Loss after Epoch 102: 1.028931640625\n",
      "Epoch 103, Loss: 0.7766435072157118\n",
      "Test Loss after Epoch 103: 1.0213925170898437\n",
      "Epoch 104, Loss: 0.7733626708984375\n",
      "Test Loss after Epoch 104: 1.0064886474609376\n",
      "Epoch 105, Loss: 0.7704585808648003\n",
      "Test Loss after Epoch 105: 1.014038055419922\n",
      "Epoch 106, Loss: 0.7684062025282118\n",
      "Test Loss after Epoch 106: 1.0175151977539063\n",
      "Epoch 107, Loss: 0.7675950995551215\n",
      "Test Loss after Epoch 107: 1.0099431762695312\n",
      "Epoch 108, Loss: 0.764941667344835\n",
      "Test Loss after Epoch 108: 1.0048212890625\n",
      "Epoch 109, Loss: 0.7667992960611979\n",
      "Test Loss after Epoch 109: 0.9920253295898438\n",
      "Epoch 110, Loss: 0.7641257900661892\n",
      "Test Loss after Epoch 110: 0.9943853454589844\n",
      "Epoch 111, Loss: 0.7599345567491319\n",
      "Test Loss after Epoch 111: 0.992078369140625\n",
      "Epoch 112, Loss: 0.7591387532552083\n",
      "Test Loss after Epoch 112: 0.9955901489257812\n",
      "Epoch 113, Loss: 0.75707566663954\n",
      "Test Loss after Epoch 113: 0.9710194702148438\n",
      "Epoch 114, Loss: 0.753213375515408\n",
      "Test Loss after Epoch 114: 0.9943677368164062\n",
      "Epoch 115, Loss: 0.7537747328016493\n",
      "Test Loss after Epoch 115: 0.976050048828125\n",
      "Epoch 116, Loss: 0.7541890733506944\n",
      "Test Loss after Epoch 116: 0.98529638671875\n",
      "Epoch 117, Loss: 0.7514264763726128\n",
      "Test Loss after Epoch 117: 0.9773713073730469\n",
      "Epoch 118, Loss: 0.7470959269205729\n",
      "Test Loss after Epoch 118: 0.9730012817382813\n",
      "Epoch 119, Loss: 0.7474309895833333\n",
      "Test Loss after Epoch 119: 0.96876513671875\n",
      "Epoch 120, Loss: 0.744582027859158\n",
      "Test Loss after Epoch 120: 0.9566809387207031\n",
      "Epoch 121, Loss: 0.7455231730143229\n",
      "Test Loss after Epoch 121: 0.9651968688964844\n",
      "Epoch 122, Loss: 0.7440608791775174\n",
      "Test Loss after Epoch 122: 0.9523028564453125\n",
      "Epoch 123, Loss: 0.7419800855848524\n",
      "Test Loss after Epoch 123: 0.951791015625\n",
      "Epoch 124, Loss: 0.7437708401150174\n",
      "Test Loss after Epoch 124: 0.9589898071289062\n",
      "Epoch 125, Loss: 0.7408841688368055\n",
      "Test Loss after Epoch 125: 0.9517473449707031\n",
      "Epoch 126, Loss: 0.739306884765625\n",
      "Test Loss after Epoch 126: 0.9462822265625\n",
      "Epoch 127, Loss: 0.7402703179253473\n",
      "Test Loss after Epoch 127: 0.9405126342773438\n",
      "Epoch 128, Loss: 0.7384864230685764\n",
      "Test Loss after Epoch 128: 0.9715371398925782\n",
      "Epoch 129, Loss: 0.740392333984375\n",
      "Test Loss after Epoch 129: 0.933833740234375\n",
      "Epoch 130, Loss: 0.7385096232096354\n",
      "Test Loss after Epoch 130: 0.9460379943847657\n",
      "Epoch 131, Loss: 0.7386898023817274\n",
      "Test Loss after Epoch 131: 0.9240453796386718\n",
      "Epoch 132, Loss: 0.7379880133734809\n",
      "Test Loss after Epoch 132: 0.91122119140625\n",
      "Epoch 133, Loss: 0.7377046881781684\n",
      "Test Loss after Epoch 133: 0.916975341796875\n",
      "Epoch 134, Loss: 0.7342125684950087\n",
      "Test Loss after Epoch 134: 0.9152283325195313\n",
      "Epoch 135, Loss: 0.7333424173990886\n",
      "Test Loss after Epoch 135: 0.9446977844238281\n",
      "Epoch 136, Loss: 0.7322837388780382\n",
      "Test Loss after Epoch 136: 0.9320283508300782\n",
      "Epoch 137, Loss: 0.7332150573730469\n",
      "Test Loss after Epoch 137: 0.9397994384765626\n",
      "Epoch 138, Loss: 0.732541524251302\n",
      "Test Loss after Epoch 138: 0.9350790710449218\n",
      "Epoch 139, Loss: 0.7304327426486545\n",
      "Test Loss after Epoch 139: 0.9429913330078125\n",
      "Epoch 140, Loss: 0.7305440809461805\n",
      "Test Loss after Epoch 140: 0.9149402160644531\n",
      "Epoch 141, Loss: 0.7315582105848524\n",
      "Test Loss after Epoch 141: 0.9060293273925781\n",
      "Epoch 142, Loss: 0.7293750949435764\n",
      "Test Loss after Epoch 142: 0.914998046875\n",
      "Epoch 143, Loss: 0.7285729743109809\n",
      "Test Loss after Epoch 143: 0.8982423706054687\n",
      "Epoch 144, Loss: 0.7287405463324653\n",
      "Test Loss after Epoch 144: 0.9260012512207031\n",
      "Epoch 145, Loss: 0.7277366807725695\n",
      "Test Loss after Epoch 145: 0.9359363708496093\n",
      "Epoch 146, Loss: 0.7253477206759983\n",
      "Test Loss after Epoch 146: 0.9150685119628906\n",
      "Epoch 147, Loss: 0.7240523206922743\n",
      "Test Loss after Epoch 147: 0.9123860778808593\n",
      "Epoch 148, Loss: 0.7240572577582465\n",
      "Test Loss after Epoch 148: 0.9084170532226562\n",
      "Epoch 149, Loss: 0.7265359530978732\n",
      "Test Loss after Epoch 149: 0.9056455383300781\n",
      "Epoch 150, Loss: 0.7235856323242188\n",
      "Test Loss after Epoch 150: 0.9197692565917969\n",
      "Epoch 151, Loss: 0.7234005194769966\n",
      "Test Loss after Epoch 151: 0.9339486083984375\n",
      "Epoch 152, Loss: 0.7255385877821181\n",
      "Test Loss after Epoch 152: 0.9297421264648438\n",
      "Epoch 153, Loss: 0.7245983072916666\n",
      "Test Loss after Epoch 153: 0.9191141967773437\n",
      "Epoch 154, Loss: 0.7244922383626302\n",
      "Test Loss after Epoch 154: 0.9195373229980469\n",
      "Epoch 155, Loss: 0.7224007602267795\n",
      "Test Loss after Epoch 155: 0.9097216186523438\n",
      "Epoch 156, Loss: 0.7230939229329427\n",
      "Test Loss after Epoch 156: 0.8995413513183593\n",
      "Epoch 157, Loss: 0.724490960015191\n",
      "Test Loss after Epoch 157: 0.9071797485351563\n",
      "Epoch 158, Loss: 0.7214401584201389\n",
      "Test Loss after Epoch 158: 0.90109375\n",
      "Epoch 159, Loss: 0.7232837897406684\n",
      "Test Loss after Epoch 159: 0.9082333374023438\n",
      "Epoch 160, Loss: 0.7228789232042101\n",
      "Test Loss after Epoch 160: 0.8967423400878907\n",
      "Epoch 161, Loss: 0.7212622680664063\n",
      "Test Loss after Epoch 161: 0.8992449340820312\n",
      "Epoch 162, Loss: 0.722771002875434\n",
      "Test Loss after Epoch 162: 0.8940706787109375\n",
      "Epoch 163, Loss: 0.7216105109320746\n",
      "Test Loss after Epoch 163: 0.9016724853515625\n",
      "Epoch 164, Loss: 0.7193759358723958\n",
      "Test Loss after Epoch 164: 0.88398193359375\n",
      "Epoch 165, Loss: 0.7199241095648872\n",
      "Test Loss after Epoch 165: 0.9269797058105469\n",
      "Epoch 166, Loss: 0.7195802137586805\n",
      "Test Loss after Epoch 166: 0.9107680358886718\n",
      "Epoch 167, Loss: 0.7198960876464844\n",
      "Test Loss after Epoch 167: 0.9057956848144532\n",
      "Epoch 168, Loss: 0.7194573771158854\n",
      "Test Loss after Epoch 168: 0.9431437072753907\n",
      "Epoch 169, Loss: 0.720322984483507\n",
      "Test Loss after Epoch 169: 0.9227826843261718\n",
      "Epoch 170, Loss: 0.717958028157552\n",
      "Test Loss after Epoch 170: 0.937520263671875\n",
      "Epoch 171, Loss: 0.7176133490668403\n",
      "Test Loss after Epoch 171: 0.9158042602539063\n",
      "Epoch 172, Loss: 0.7183506876627604\n",
      "Test Loss after Epoch 172: 0.8903935241699219\n",
      "Epoch 173, Loss: 0.7176041666666667\n",
      "Test Loss after Epoch 173: 0.8916717224121093\n",
      "Epoch 174, Loss: 0.7176960076226129\n",
      "Test Loss after Epoch 174: 0.8859790954589843\n",
      "Epoch 175, Loss: 0.7176248135036892\n",
      "Test Loss after Epoch 175: 0.8869059753417968\n",
      "Epoch 176, Loss: 0.7172854580349393\n",
      "Test Loss after Epoch 176: 0.9148204956054687\n",
      "Epoch 177, Loss: 0.7167230292426215\n",
      "Test Loss after Epoch 177: 0.9076684265136719\n",
      "Epoch 178, Loss: 0.716620832655165\n",
      "Test Loss after Epoch 178: 0.9142439880371094\n",
      "Epoch 179, Loss: 0.7160571628146701\n",
      "Test Loss after Epoch 179: 0.9094287109375\n",
      "Epoch 180, Loss: 0.7166846618652344\n",
      "Test Loss after Epoch 180: 0.9315575256347656\n",
      "Epoch 181, Loss: 0.7189055413140191\n",
      "Test Loss after Epoch 181: 0.9096088562011718\n",
      "Epoch 182, Loss: 0.7180371873643663\n",
      "Test Loss after Epoch 182: 0.9029840698242187\n",
      "Epoch 183, Loss: 0.716168690999349\n",
      "Test Loss after Epoch 183: 0.9165739440917968\n",
      "Epoch 184, Loss: 0.7178622775607639\n",
      "Test Loss after Epoch 184: 0.9185296630859375\n",
      "Rolling back to best model from epoch 164\n",
      "Best test loss: 0.88398193359375\n",
      "Epoch 185, Loss: 0.7183059726291232\n",
      "Test Loss after Epoch 185: 0.9134697570800782\n",
      "Epoch 186, Loss: 0.7171495496961806\n",
      "Test Loss after Epoch 186: 0.9028218383789063\n",
      "Epoch 187, Loss: 0.7184167717827691\n",
      "Test Loss after Epoch 187: 0.8852909851074219\n",
      "Epoch 188, Loss: 0.7184569566514757\n",
      "Test Loss after Epoch 188: 0.9382949829101562\n",
      "Epoch 189, Loss: 0.7175708889431424\n",
      "Test Loss after Epoch 189: 0.8906788330078125\n",
      "Epoch 190, Loss: 0.7203660210503472\n",
      "Test Loss after Epoch 190: 0.9085372619628906\n",
      "Epoch 191, Loss: 0.7179715779622395\n",
      "Test Loss after Epoch 191: 0.8958442993164063\n",
      "Epoch 192, Loss: 0.7167736104329427\n",
      "Test Loss after Epoch 192: 0.8815674133300782\n",
      "Epoch 193, Loss: 0.7162518683539496\n",
      "Test Loss after Epoch 193: 0.8947938537597656\n",
      "Epoch 194, Loss: 0.7169808180067274\n",
      "Test Loss after Epoch 194: 0.9070741882324219\n",
      "Epoch 195, Loss: 0.7188460591634115\n",
      "Test Loss after Epoch 195: 0.9204888916015626\n",
      "Epoch 196, Loss: 0.7203301154242622\n",
      "Test Loss after Epoch 196: 0.9045414123535156\n",
      "Epoch 197, Loss: 0.7188513997395833\n",
      "Test Loss after Epoch 197: 0.9123926696777344\n",
      "Epoch 198, Loss: 0.719057125515408\n",
      "Test Loss after Epoch 198: 0.89600439453125\n",
      "Epoch 199, Loss: 0.7160340372721354\n",
      "Test Loss after Epoch 199: 0.8643417663574219\n",
      "Epoch 200, Loss: 0.7169977620442708\n",
      "Test Loss after Epoch 200: 0.8882990112304687\n",
      "Epoch 201, Loss: 0.7160464274088542\n",
      "Test Loss after Epoch 201: 0.8673102111816406\n",
      "Epoch 202, Loss: 0.7192138061523438\n",
      "Test Loss after Epoch 202: 0.87952587890625\n",
      "Epoch 203, Loss: 0.7169534742567274\n",
      "Test Loss after Epoch 203: 0.9178738708496094\n",
      "Epoch 204, Loss: 0.7179661424424914\n",
      "Test Loss after Epoch 204: 0.8548878173828125\n",
      "Epoch 205, Loss: 0.7180059271918403\n",
      "Test Loss after Epoch 205: 0.8758993835449219\n",
      "Epoch 206, Loss: 0.7184104885525173\n",
      "Test Loss after Epoch 206: 0.8913807067871093\n",
      "Epoch 207, Loss: 0.7172730882432725\n",
      "Test Loss after Epoch 207: 0.8725115356445312\n",
      "Epoch 208, Loss: 0.718163343641493\n",
      "Test Loss after Epoch 208: 0.8719254150390625\n",
      "Epoch 209, Loss: 0.7166881849500868\n",
      "Test Loss after Epoch 209: 0.8614174499511719\n",
      "Epoch 210, Loss: 0.7159026590983073\n",
      "Test Loss after Epoch 210: 0.8807751770019532\n",
      "Epoch 211, Loss: 0.716201917860243\n",
      "Test Loss after Epoch 211: 0.8962320861816406\n",
      "Epoch 212, Loss: 0.713888176812066\n",
      "Test Loss after Epoch 212: 0.8709766540527344\n",
      "Epoch 213, Loss: 0.7162321675618489\n",
      "Test Loss after Epoch 213: 0.8708771667480468\n",
      "Epoch 214, Loss: 0.7150141059027778\n",
      "Test Loss after Epoch 214: 0.9023402709960937\n",
      "Epoch 215, Loss: 0.7147419297960069\n",
      "Test Loss after Epoch 215: 0.8789160461425781\n",
      "Epoch 216, Loss: 0.7156195746527778\n",
      "Test Loss after Epoch 216: 0.8622880249023438\n",
      "Epoch 217, Loss: 0.7149146694607205\n",
      "Test Loss after Epoch 217: 0.8688464965820313\n",
      "Epoch 218, Loss: 0.7145338473849826\n",
      "Test Loss after Epoch 218: 0.8812366638183594\n",
      "Epoch 219, Loss: 0.7146919250488282\n",
      "Test Loss after Epoch 219: 0.8905140075683594\n",
      "Epoch 220, Loss: 0.7136435546875\n",
      "Test Loss after Epoch 220: 0.8924240112304688\n",
      "Epoch 221, Loss: 0.7159926249186198\n",
      "Test Loss after Epoch 221: 0.8603398742675781\n",
      "Epoch 222, Loss: 0.7160861477322049\n",
      "Test Loss after Epoch 222: 0.8689681091308594\n",
      "Epoch 223, Loss: 0.7152814432779948\n",
      "Test Loss after Epoch 223: 0.8571312255859375\n",
      "Epoch 224, Loss: 0.7152284884982639\n",
      "Test Loss after Epoch 224: 0.8757217102050782\n",
      "Epoch 225, Loss: 0.7147014600965712\n",
      "Test Loss after Epoch 225: 0.846074951171875\n",
      "Epoch 226, Loss: 0.714487060546875\n",
      "Test Loss after Epoch 226: 0.8638527526855468\n",
      "Epoch 227, Loss: 0.713891594780816\n",
      "Test Loss after Epoch 227: 0.8607998046875\n",
      "Epoch 228, Loss: 0.7152136298285591\n",
      "Test Loss after Epoch 228: 0.8506942443847656\n",
      "Epoch 229, Loss: 0.7145525478786893\n",
      "Test Loss after Epoch 229: 0.8594095764160157\n",
      "Epoch 230, Loss: 0.7151968010796441\n",
      "Test Loss after Epoch 230: 0.8657988891601562\n",
      "Epoch 231, Loss: 0.7137329576280382\n",
      "Test Loss after Epoch 231: 0.8645382995605468\n",
      "Epoch 232, Loss: 0.7166652459038628\n",
      "Test Loss after Epoch 232: 0.8370758361816406\n",
      "Epoch 233, Loss: 0.7176241048177083\n",
      "Test Loss after Epoch 233: 0.853425048828125\n",
      "Epoch 234, Loss: 0.717628675672743\n",
      "Test Loss after Epoch 234: 0.8539502563476562\n",
      "Epoch 235, Loss: 0.7150538804796007\n",
      "Test Loss after Epoch 235: 0.8534440002441406\n",
      "Epoch 236, Loss: 0.7164805331759982\n",
      "Test Loss after Epoch 236: 0.8845569458007813\n",
      "Epoch 237, Loss: 0.7162560865614149\n",
      "Test Loss after Epoch 237: 0.8514901733398438\n",
      "Epoch 238, Loss: 0.7148475748697917\n",
      "Test Loss after Epoch 238: 0.8693657531738281\n",
      "Epoch 239, Loss: 0.7142964240180122\n",
      "Test Loss after Epoch 239: 0.8741893615722657\n",
      "Epoch 240, Loss: 0.7144566684299045\n",
      "Test Loss after Epoch 240: 0.8713317260742187\n",
      "Epoch 241, Loss: 0.7127684800889756\n",
      "Test Loss after Epoch 241: 0.9093368225097657\n",
      "Epoch 242, Loss: 0.7141826002332899\n",
      "Test Loss after Epoch 242: 0.8749444580078125\n",
      "Epoch 243, Loss: 0.7147025146484375\n",
      "Test Loss after Epoch 243: 0.8689171752929687\n",
      "Epoch 244, Loss: 0.7131037326388889\n",
      "Test Loss after Epoch 244: 0.8793825073242187\n",
      "Epoch 245, Loss: 0.7142683546278212\n",
      "Test Loss after Epoch 245: 0.8559846496582031\n",
      "Epoch 246, Loss: 0.7129642808702257\n",
      "Test Loss after Epoch 246: 0.8932252502441407\n",
      "Epoch 247, Loss: 0.7135691562228733\n",
      "Test Loss after Epoch 247: 0.8659721069335937\n",
      "Epoch 248, Loss: 0.7150217725965712\n",
      "Test Loss after Epoch 248: 0.8412214050292969\n",
      "Epoch 249, Loss: 0.7128447875976562\n",
      "Test Loss after Epoch 249: 0.8559214172363281\n",
      "Epoch 250, Loss: 0.7121148715549045\n",
      "Test Loss after Epoch 250: 0.8498190612792969\n",
      "Epoch 251, Loss: 0.7138411458333334\n",
      "Test Loss after Epoch 251: 0.8723674621582032\n",
      "Epoch 252, Loss: 0.7134950697157119\n",
      "Test Loss after Epoch 252: 0.8754038696289063\n",
      "Epoch 253, Loss: 0.7145348917643229\n",
      "Test Loss after Epoch 253: 0.8751573181152343\n",
      "Epoch 254, Loss: 0.7140280931260851\n",
      "Test Loss after Epoch 254: 0.90610205078125\n",
      "Epoch 255, Loss: 0.7134240112304687\n",
      "Test Loss after Epoch 255: 0.8812969360351562\n",
      "Epoch 256, Loss: 0.7122171020507813\n",
      "Test Loss after Epoch 256: 0.8913474731445312\n",
      "Epoch 257, Loss: 0.7144510396321615\n",
      "Test Loss after Epoch 257: 0.8666805114746093\n",
      "Epoch 258, Loss: 0.7123151550292969\n",
      "Test Loss after Epoch 258: 0.88022314453125\n",
      "Epoch 259, Loss: 0.7116718512641059\n",
      "Test Loss after Epoch 259: 0.870798828125\n",
      "Epoch 260, Loss: 0.7121506483289931\n",
      "Test Loss after Epoch 260: 0.8762745056152343\n",
      "Rolling back to best model from epoch 232\n",
      "Best test loss: 0.8370758361816406\n",
      "Epoch 261, Loss: 0.72136376953125\n",
      "Test Loss after Epoch 261: 0.8604968566894531\n",
      "Epoch 262, Loss: 0.7192399291992188\n",
      "Test Loss after Epoch 262: 0.8750261840820313\n",
      "Epoch 263, Loss: 0.7189566345214844\n",
      "Test Loss after Epoch 263: 0.8279050598144532\n",
      "Epoch 264, Loss: 0.716848385281033\n",
      "Test Loss after Epoch 264: 0.8566155395507813\n",
      "Epoch 265, Loss: 0.7200823805067275\n",
      "Test Loss after Epoch 265: 0.8469629211425781\n",
      "Epoch 266, Loss: 0.7177606981065539\n",
      "Test Loss after Epoch 266: 0.8703081665039063\n",
      "Epoch 267, Loss: 0.7176048177083333\n",
      "Test Loss after Epoch 267: 0.852736083984375\n",
      "Epoch 268, Loss: 0.7167244601779514\n",
      "Test Loss after Epoch 268: 0.8692498474121094\n",
      "Epoch 269, Loss: 0.7165769755045573\n",
      "Test Loss after Epoch 269: 0.8573008422851562\n",
      "Epoch 270, Loss: 0.7161362779405382\n",
      "Test Loss after Epoch 270: 0.8431705322265625\n",
      "Epoch 271, Loss: 0.7153369818793403\n",
      "Test Loss after Epoch 271: 0.8516539916992187\n",
      "Epoch 272, Loss: 0.7157774251302084\n",
      "Test Loss after Epoch 272: 0.8313426818847657\n",
      "Epoch 273, Loss: 0.7137985636393229\n",
      "Test Loss after Epoch 273: 0.8467189636230469\n",
      "Epoch 274, Loss: 0.7146749233669705\n",
      "Test Loss after Epoch 274: 0.850771728515625\n",
      "Epoch 275, Loss: 0.7143507012261284\n",
      "Test Loss after Epoch 275: 0.8831170043945312\n",
      "Epoch 276, Loss: 0.7157542555067274\n",
      "Test Loss after Epoch 276: 0.8505419616699219\n",
      "Epoch 277, Loss: 0.714015126546224\n",
      "Test Loss after Epoch 277: 0.8462781677246094\n",
      "Epoch 278, Loss: 0.7136270989312066\n",
      "Test Loss after Epoch 278: 0.8805533447265625\n",
      "Epoch 279, Loss: 0.7155650092230903\n",
      "Test Loss after Epoch 279: 0.8434577026367187\n",
      "Epoch 280, Loss: 0.7168256259494358\n",
      "Test Loss after Epoch 280: 0.8522465515136719\n",
      "Epoch 281, Loss: 0.7135948757595486\n",
      "Test Loss after Epoch 281: 0.8698509216308594\n",
      "Epoch 282, Loss: 0.7134508870442708\n",
      "Test Loss after Epoch 282: 0.8664322814941406\n",
      "Epoch 283, Loss: 0.7127330661349827\n",
      "Test Loss after Epoch 283: 0.8745350341796875\n",
      "Epoch 284, Loss: 0.7126763271755643\n",
      "Test Loss after Epoch 284: 0.8611818237304687\n",
      "Epoch 285, Loss: 0.712729017469618\n",
      "Test Loss after Epoch 285: 0.8756946411132812\n",
      "Epoch 286, Loss: 0.7128651089138455\n",
      "Test Loss after Epoch 286: 0.8531576843261719\n",
      "Epoch 287, Loss: 0.7117400478786893\n",
      "Test Loss after Epoch 287: 0.8606611938476563\n",
      "Epoch 288, Loss: 0.7135458814832899\n",
      "Test Loss after Epoch 288: 0.8441513671875\n",
      "Epoch 289, Loss: 0.7113528205023871\n",
      "Test Loss after Epoch 289: 0.8502340087890625\n",
      "Epoch 290, Loss: 0.7102679341634115\n",
      "Test Loss after Epoch 290: 0.8624305114746094\n",
      "Epoch 291, Loss: 0.7122037489149305\n",
      "Test Loss after Epoch 291: 0.8597020263671875\n",
      "Epoch 292, Loss: 0.7129766404893663\n",
      "Test Loss after Epoch 292: 0.8490682983398438\n",
      "Epoch 293, Loss: 0.7117108425564236\n",
      "Test Loss after Epoch 293: 0.8541092224121094\n",
      "Epoch 294, Loss: 0.7108202548556858\n",
      "Test Loss after Epoch 294: 0.8737424621582032\n",
      "Epoch 295, Loss: 0.710608645968967\n",
      "Test Loss after Epoch 295: 0.8631707458496094\n",
      "Epoch 296, Loss: 0.7114950798882379\n",
      "Test Loss after Epoch 296: 0.8938573913574219\n",
      "Epoch 297, Loss: 0.7104419453938802\n",
      "Test Loss after Epoch 297: 0.8684814147949219\n",
      "Epoch 298, Loss: 0.7105250074598525\n",
      "Test Loss after Epoch 298: 0.8743282470703125\n",
      "Epoch 299, Loss: 0.7117442423502605\n",
      "Test Loss after Epoch 299: 0.8728715209960938\n",
      "Epoch 300, Loss: 0.7101980251736111\n",
      "Test Loss after Epoch 300: 0.8629472351074219\n",
      "Epoch 301, Loss: 0.709857174343533\n",
      "Test Loss after Epoch 301: 0.8681265563964844\n",
      "Epoch 302, Loss: 0.7101944240993924\n",
      "Test Loss after Epoch 302: 0.8956644897460937\n",
      "Rolling back to best model from epoch 263\n",
      "Best test loss: 0.8279050598144532\n",
      "Epoch 303, Loss: 0.719158915201823\n",
      "Test Loss after Epoch 303: 0.8643583679199218\n",
      "Epoch 304, Loss: 0.7192173360188802\n",
      "Test Loss after Epoch 304: 0.8287406616210937\n",
      "Epoch 305, Loss: 0.7199674377441406\n",
      "Test Loss after Epoch 305: 0.853980712890625\n",
      "Epoch 306, Loss: 0.7200244513617622\n",
      "Test Loss after Epoch 306: 0.860313720703125\n",
      "Epoch 307, Loss: 0.7206655036078559\n",
      "Test Loss after Epoch 307: 0.8626068115234375\n",
      "Epoch 308, Loss: 0.7198504740397136\n",
      "Test Loss after Epoch 308: 0.8790422058105469\n",
      "Epoch 309, Loss: 0.7212912055121528\n",
      "Test Loss after Epoch 309: 0.8490239562988281\n",
      "Epoch 310, Loss: 0.7188530714246962\n",
      "Test Loss after Epoch 310: 0.8757673034667969\n",
      "Epoch 311, Loss: 0.718060523139106\n",
      "Test Loss after Epoch 311: 0.8488863220214844\n",
      "Epoch 312, Loss: 0.7193491448296441\n",
      "Test Loss after Epoch 312: 0.8536583862304687\n",
      "Epoch 313, Loss: 0.7180956013997396\n",
      "Test Loss after Epoch 313: 0.87580517578125\n",
      "Epoch 314, Loss: 0.7164039238823785\n",
      "Test Loss after Epoch 314: 0.8598565673828125\n",
      "Epoch 315, Loss: 0.7170674777560764\n",
      "Test Loss after Epoch 315: 0.8532734680175781\n",
      "Epoch 316, Loss: 0.7151557413736979\n",
      "Test Loss after Epoch 316: 0.8659477844238281\n",
      "Epoch 317, Loss: 0.7147391662597656\n",
      "Test Loss after Epoch 317: 0.8568511047363281\n",
      "Epoch 318, Loss: 0.7157669779459636\n",
      "Test Loss after Epoch 318: 0.8850129089355468\n",
      "Epoch 319, Loss: 0.7156185472276476\n",
      "Test Loss after Epoch 319: 0.8905755920410157\n",
      "Epoch 320, Loss: 0.7142886386447482\n",
      "Test Loss after Epoch 320: 0.8840706176757812\n",
      "Epoch 321, Loss: 0.7140020785861545\n",
      "Test Loss after Epoch 321: 0.8839422607421875\n",
      "Epoch 322, Loss: 0.7124114176432291\n",
      "Test Loss after Epoch 322: 0.8633642272949219\n",
      "Epoch 323, Loss: 0.7149862060546875\n",
      "Test Loss after Epoch 323: 0.8822080993652344\n",
      "Epoch 324, Loss: 0.7131630520290798\n",
      "Test Loss after Epoch 324: 0.8621024780273437\n",
      "Epoch 325, Loss: 0.7121859605577257\n",
      "Test Loss after Epoch 325: 0.8536971435546875\n",
      "Epoch 326, Loss: 0.7119987996419271\n",
      "Test Loss after Epoch 326: 0.87121484375\n",
      "Epoch 327, Loss: 0.7136578877766927\n",
      "Test Loss after Epoch 327: 0.8569107055664062\n",
      "Epoch 328, Loss: 0.7133691270616319\n",
      "Test Loss after Epoch 328: 0.8461062316894531\n",
      "Epoch 329, Loss: 0.7130040520562066\n",
      "Test Loss after Epoch 329: 0.8504773254394531\n",
      "Epoch 330, Loss: 0.712495859781901\n",
      "Test Loss after Epoch 330: 0.8675716247558594\n",
      "Epoch 331, Loss: 0.7113248460557726\n",
      "Test Loss after Epoch 331: 0.8456635437011719\n",
      "Epoch 332, Loss: 0.7117260437011719\n",
      "Test Loss after Epoch 332: 0.863780517578125\n",
      "Epoch 333, Loss: 0.7110772264268663\n",
      "Test Loss after Epoch 333: 0.8845864562988281\n",
      "Epoch 334, Loss: 0.7134822523328993\n",
      "Test Loss after Epoch 334: 0.8771313781738281\n",
      "Epoch 335, Loss: 0.712958977593316\n",
      "Test Loss after Epoch 335: 0.9165694885253907\n",
      "Epoch 336, Loss: 0.7111458808051215\n",
      "Test Loss after Epoch 336: 0.8447767944335938\n",
      "Epoch 337, Loss: 0.7119062940809462\n",
      "Test Loss after Epoch 337: 0.86925146484375\n",
      "Epoch 338, Loss: 0.7121791619194878\n",
      "Test Loss after Epoch 338: 0.872229248046875\n",
      "Epoch 339, Loss: 0.7105046895345052\n",
      "Test Loss after Epoch 339: 0.8680730590820313\n",
      "Epoch 340, Loss: 0.7109226142035591\n",
      "Test Loss after Epoch 340: 0.8586447143554687\n",
      "Epoch 341, Loss: 0.7109078301323785\n",
      "Test Loss after Epoch 341: 0.852075439453125\n",
      "Epoch 342, Loss: 0.7099761522081163\n",
      "Test Loss after Epoch 342: 0.8753251342773437\n",
      "Epoch 343, Loss: 0.7103455301920573\n",
      "Test Loss after Epoch 343: 0.8838102111816406\n",
      "Epoch 344, Loss: 0.7104728020562066\n",
      "Test Loss after Epoch 344: 0.8888031005859375\n",
      "Epoch 345, Loss: 0.7091919216579861\n",
      "Test Loss after Epoch 345: 0.8818060607910156\n",
      "Epoch 346, Loss: 0.708609130859375\n",
      "Test Loss after Epoch 346: 0.8570550842285156\n",
      "Epoch 347, Loss: 0.708973398844401\n",
      "Test Loss after Epoch 347: 0.8585650939941406\n",
      "Epoch 348, Loss: 0.7104523179796007\n",
      "Test Loss after Epoch 348: 0.8633088989257812\n",
      "Epoch 349, Loss: 0.7083625149197048\n",
      "Test Loss after Epoch 349: 0.8714072570800782\n",
      "Epoch 350, Loss: 0.707640628390842\n",
      "Test Loss after Epoch 350: 0.8751917114257812\n",
      "Epoch 351, Loss: 0.707844736735026\n",
      "Test Loss after Epoch 351: 0.8563863525390625\n",
      "Epoch 352, Loss: 0.7081883443196615\n",
      "Test Loss after Epoch 352: 0.8602785034179687\n",
      "Epoch 353, Loss: 0.7082928738064236\n",
      "Test Loss after Epoch 353: 0.8597386169433594\n",
      "Epoch 354, Loss: 0.7099712592230902\n",
      "Test Loss after Epoch 354: 0.8672736511230469\n",
      "Epoch 355, Loss: 0.7079752536349826\n",
      "Test Loss after Epoch 355: 0.8684061889648438\n",
      "Epoch 356, Loss: 0.7064264662000868\n",
      "Test Loss after Epoch 356: 0.8971497802734375\n",
      "Epoch 357, Loss: 0.7073167114257812\n",
      "Test Loss after Epoch 357: 0.8839567565917968\n",
      "Rolling back to best model from epoch 263\n",
      "Best test loss: 0.8279050598144532\n",
      "Epoch 358, Loss: 0.7255524088541667\n",
      "Test Loss after Epoch 358: 0.8555506286621094\n",
      "Epoch 359, Loss: 0.7260191548665365\n",
      "Test Loss after Epoch 359: 0.8341694030761718\n",
      "Epoch 360, Loss: 0.7218830057779948\n",
      "Test Loss after Epoch 360: 0.8548643188476562\n",
      "Epoch 361, Loss: 0.724749026828342\n",
      "Test Loss after Epoch 361: 0.8707830810546875\n",
      "Epoch 362, Loss: 0.7233491482204861\n",
      "Test Loss after Epoch 362: 0.8490602722167969\n",
      "Epoch 363, Loss: 0.7226517401801216\n",
      "Test Loss after Epoch 363: 0.844200439453125\n",
      "Epoch 364, Loss: 0.7189397583007813\n",
      "Test Loss after Epoch 364: 0.838127197265625\n",
      "Epoch 365, Loss: 0.7202431064181858\n",
      "Test Loss after Epoch 365: 0.8446319885253907\n",
      "Epoch 366, Loss: 0.7187880113389757\n",
      "Test Loss after Epoch 366: 0.8589206237792969\n",
      "Epoch 367, Loss: 0.7176151123046876\n",
      "Test Loss after Epoch 367: 0.8453485107421875\n",
      "Epoch 368, Loss: 0.7162936028374566\n",
      "Test Loss after Epoch 368: 0.8550916748046875\n",
      "Epoch 369, Loss: 0.7201312561035156\n",
      "Test Loss after Epoch 369: 0.8379827880859375\n",
      "Epoch 370, Loss: 0.7174111124674479\n",
      "Test Loss after Epoch 370: 0.8163599548339844\n",
      "Epoch 371, Loss: 0.715986562093099\n",
      "Test Loss after Epoch 371: 0.83191162109375\n",
      "Epoch 372, Loss: 0.7159711235894097\n",
      "Test Loss after Epoch 372: 0.8475404968261718\n",
      "Epoch 373, Loss: 0.7177435065375434\n",
      "Test Loss after Epoch 373: 0.837433349609375\n",
      "Epoch 374, Loss: 0.718114508734809\n",
      "Test Loss after Epoch 374: 0.8593672790527344\n",
      "Epoch 375, Loss: 0.7170374077690972\n",
      "Test Loss after Epoch 375: 0.8608609619140625\n",
      "Epoch 376, Loss: 0.7158299696180556\n",
      "Test Loss after Epoch 376: 0.853886474609375\n",
      "Epoch 377, Loss: 0.7149567532009549\n",
      "Test Loss after Epoch 377: 0.8529749450683594\n",
      "Epoch 378, Loss: 0.715209726969401\n",
      "Test Loss after Epoch 378: 0.8576403503417969\n",
      "Epoch 379, Loss: 0.7135748155381945\n",
      "Test Loss after Epoch 379: 0.8439768676757813\n",
      "Epoch 380, Loss: 0.714182620578342\n",
      "Test Loss after Epoch 380: 0.8288364562988281\n",
      "Epoch 381, Loss: 0.7145578850640191\n",
      "Test Loss after Epoch 381: 0.8505557250976562\n",
      "Epoch 382, Loss: 0.713594970703125\n",
      "Test Loss after Epoch 382: 0.8653755187988281\n",
      "Epoch 383, Loss: 0.7150358615451389\n",
      "Test Loss after Epoch 383: 0.8435876770019531\n",
      "Epoch 384, Loss: 0.7141164720323351\n",
      "Test Loss after Epoch 384: 0.8262757263183593\n",
      "Epoch 385, Loss: 0.7137364434136285\n",
      "Test Loss after Epoch 385: 0.8336663818359376\n",
      "Epoch 386, Loss: 0.7128012152777777\n",
      "Test Loss after Epoch 386: 0.8615770874023437\n",
      "Epoch 387, Loss: 0.7121665717230903\n",
      "Test Loss after Epoch 387: 0.8687684936523438\n",
      "Epoch 388, Loss: 0.7127954644097222\n",
      "Test Loss after Epoch 388: 0.84862109375\n",
      "Epoch 389, Loss: 0.7132606608072917\n",
      "Test Loss after Epoch 389: 0.8546513977050781\n",
      "Epoch 390, Loss: 0.7098941752115886\n",
      "Test Loss after Epoch 390: 0.8464278869628906\n",
      "Epoch 391, Loss: 0.7108115030924479\n",
      "Test Loss after Epoch 391: 0.844965576171875\n",
      "Epoch 392, Loss: 0.7107616000705295\n",
      "Test Loss after Epoch 392: 0.8628512573242187\n",
      "Epoch 393, Loss: 0.7116998697916667\n",
      "Test Loss after Epoch 393: 0.84330908203125\n",
      "Epoch 394, Loss: 0.7100998060438368\n",
      "Test Loss after Epoch 394: 0.8572817993164062\n",
      "Epoch 395, Loss: 0.7100249633789063\n",
      "Test Loss after Epoch 395: 0.8300318908691406\n",
      "Epoch 396, Loss: 0.7100059712727864\n",
      "Test Loss after Epoch 396: 0.8307478332519531\n",
      "Epoch 397, Loss: 0.7096449957953559\n",
      "Test Loss after Epoch 397: 0.83569677734375\n",
      "Epoch 398, Loss: 0.7102784423828125\n",
      "Test Loss after Epoch 398: 0.8546305236816406\n",
      "Epoch 399, Loss: 0.7093758884006076\n",
      "Test Loss after Epoch 399: 0.877611572265625\n",
      "Epoch 400, Loss: 0.7090820414225261\n",
      "Test Loss after Epoch 400: 0.867782958984375\n",
      "Epoch 401, Loss: 0.7089336886935764\n",
      "Test Loss after Epoch 401: 0.8606502380371094\n",
      "Epoch 402, Loss: 0.7085533209906684\n",
      "Test Loss after Epoch 402: 0.8319915466308594\n",
      "Epoch 403, Loss: 0.7089813469780816\n",
      "Test Loss after Epoch 403: 0.8274244384765626\n",
      "Epoch 404, Loss: 0.7079160427517361\n",
      "Test Loss after Epoch 404: 0.8516629333496094\n",
      "Epoch 405, Loss: 0.7067039252387153\n",
      "Test Loss after Epoch 405: 0.845090087890625\n",
      "Epoch 406, Loss: 0.7089700724283854\n",
      "Test Loss after Epoch 406: 0.8396220703125\n",
      "Epoch 407, Loss: 0.7078798489040798\n",
      "Test Loss after Epoch 407: 0.8411234436035157\n",
      "Epoch 408, Loss: 0.7095657179090712\n",
      "Test Loss after Epoch 408: 0.8630977783203125\n",
      "Epoch 409, Loss: 0.7092251959906684\n",
      "Test Loss after Epoch 409: 0.8357556762695313\n",
      "Epoch 410, Loss: 0.7089237569173177\n",
      "Test Loss after Epoch 410: 0.8576555786132812\n",
      "Epoch 411, Loss: 0.7080865749782986\n",
      "Test Loss after Epoch 411: 0.8250389404296875\n",
      "Epoch 412, Loss: 0.7084979146321615\n",
      "Test Loss after Epoch 412: 0.8510765991210938\n",
      "Epoch 413, Loss: 0.7077935146755643\n",
      "Test Loss after Epoch 413: 0.8508823547363281\n",
      "Epoch 414, Loss: 0.7083126797146267\n",
      "Test Loss after Epoch 414: 0.8557239379882813\n",
      "Epoch 415, Loss: 0.7089259101019966\n",
      "Test Loss after Epoch 415: 0.8395418701171875\n",
      "Epoch 416, Loss: 0.7075173102484809\n",
      "Test Loss after Epoch 416: 0.8682180786132813\n",
      "Epoch 417, Loss: 0.7074740058051215\n",
      "Test Loss after Epoch 417: 0.8682376403808594\n",
      "Epoch 418, Loss: 0.7085837300618489\n",
      "Test Loss after Epoch 418: 0.8604280090332032\n",
      "Epoch 419, Loss: 0.7072396850585938\n",
      "Test Loss after Epoch 419: 0.8306512756347656\n",
      "Epoch 420, Loss: 0.7067433369954427\n",
      "Test Loss after Epoch 420: 0.8349059448242188\n",
      "Epoch 421, Loss: 0.7086757371690539\n",
      "Test Loss after Epoch 421: 0.8532086791992187\n",
      "Epoch 422, Loss: 0.7052872212727864\n",
      "Test Loss after Epoch 422: 0.8761997985839843\n",
      "Epoch 423, Loss: 0.7051177300347222\n",
      "Test Loss after Epoch 423: 0.8480633239746094\n",
      "Epoch 424, Loss: 0.7060552164713542\n",
      "Test Loss after Epoch 424: 0.8605863952636719\n",
      "Epoch 425, Loss: 0.7072431979709202\n",
      "Test Loss after Epoch 425: 0.8512054138183593\n",
      "Epoch 426, Loss: 0.7057504238552518\n",
      "Test Loss after Epoch 426: 0.86754296875\n",
      "Epoch 427, Loss: 0.7048892890082465\n",
      "Test Loss after Epoch 427: 0.8480224304199219\n",
      "Epoch 428, Loss: 0.7055337185329861\n",
      "Test Loss after Epoch 428: 0.8618435974121094\n",
      "Epoch 429, Loss: 0.7061038411458334\n",
      "Test Loss after Epoch 429: 0.843686767578125\n",
      "Epoch 430, Loss: 0.7058926459418403\n",
      "Test Loss after Epoch 430: 0.8564631042480468\n",
      "Epoch 431, Loss: 0.7056770155164931\n",
      "Test Loss after Epoch 431: 0.8282911987304687\n",
      "Epoch 432, Loss: 0.7059087490505642\n",
      "Test Loss after Epoch 432: 0.8377701110839844\n",
      "Epoch 433, Loss: 0.7050233832465278\n",
      "Test Loss after Epoch 433: 0.8311793823242187\n",
      "Epoch 434, Loss: 0.7040849168565538\n",
      "Test Loss after Epoch 434: 0.8320790100097656\n",
      "Epoch 435, Loss: 0.704380357530382\n",
      "Test Loss after Epoch 435: 0.8290615234375\n",
      "Epoch 436, Loss: 0.705494642469618\n",
      "Test Loss after Epoch 436: 0.8197026977539063\n",
      "Epoch 437, Loss: 0.7045681254069011\n",
      "Test Loss after Epoch 437: 0.836014404296875\n",
      "Epoch 438, Loss: 0.7039719000922309\n",
      "Test Loss after Epoch 438: 0.8360260620117187\n",
      "Epoch 439, Loss: 0.704284657796224\n",
      "Test Loss after Epoch 439: 0.8486012268066406\n",
      "Epoch 440, Loss: 0.7045895216200087\n",
      "Test Loss after Epoch 440: 0.830125244140625\n",
      "Epoch 441, Loss: 0.7045508083767361\n",
      "Test Loss after Epoch 441: 0.8400525207519531\n",
      "Epoch 442, Loss: 0.7033667161729601\n",
      "Test Loss after Epoch 442: 0.8265460205078125\n",
      "Epoch 443, Loss: 0.7052674357096355\n",
      "Test Loss after Epoch 443: 0.8332501220703125\n",
      "Epoch 444, Loss: 0.7047324659559462\n",
      "Test Loss after Epoch 444: 0.8313526000976562\n",
      "Epoch 445, Loss: 0.7045709364149305\n",
      "Test Loss after Epoch 445: 0.8462101745605469\n",
      "Epoch 446, Loss: 0.7054023844401042\n",
      "Test Loss after Epoch 446: 0.8441746215820313\n",
      "Epoch 447, Loss: 0.7032852274576823\n",
      "Test Loss after Epoch 447: 0.8535007019042968\n",
      "Rolling back to best model from epoch 370\n",
      "Best test loss: 0.8163599548339844\n",
      "Epoch 448, Loss: 0.7182473958333333\n",
      "Test Loss after Epoch 448: 0.8240540771484375\n",
      "Epoch 449, Loss: 0.717079566107856\n",
      "Test Loss after Epoch 449: 0.8225677490234375\n",
      "Epoch 450, Loss: 0.7173521898057725\n",
      "Test Loss after Epoch 450: 0.8180227966308594\n",
      "Epoch 451, Loss: 0.7153690999348958\n",
      "Test Loss after Epoch 451: 0.8407585144042968\n",
      "Epoch 452, Loss: 0.7161298590766059\n",
      "Test Loss after Epoch 452: 0.8267716674804687\n",
      "Epoch 453, Loss: 0.7173542955186631\n",
      "Test Loss after Epoch 453: 0.8391811218261719\n",
      "Epoch 454, Loss: 0.7177425333658854\n",
      "Test Loss after Epoch 454: 0.8248154907226563\n",
      "Epoch 455, Loss: 0.7177750108506944\n",
      "Test Loss after Epoch 455: 0.840310302734375\n",
      "Epoch 456, Loss: 0.7175699395073785\n",
      "Test Loss after Epoch 456: 0.8479665222167969\n",
      "Epoch 457, Loss: 0.7163569641113281\n",
      "Test Loss after Epoch 457: 0.8199885864257812\n",
      "Epoch 458, Loss: 0.7156834581163194\n",
      "Test Loss after Epoch 458: 0.8434336547851562\n",
      "Epoch 459, Loss: 0.715517340766059\n",
      "Test Loss after Epoch 459: 0.8441624755859375\n",
      "Epoch 460, Loss: 0.7159608425564236\n",
      "Test Loss after Epoch 460: 0.8428189392089844\n",
      "Epoch 461, Loss: 0.7133883904351128\n",
      "Test Loss after Epoch 461: 0.8573331298828125\n",
      "Epoch 462, Loss: 0.7135943976508247\n",
      "Test Loss after Epoch 462: 0.8437427673339843\n",
      "Epoch 463, Loss: 0.7118192816840277\n",
      "Test Loss after Epoch 463: 0.8636636047363281\n",
      "Epoch 464, Loss: 0.7128526814778646\n",
      "Test Loss after Epoch 464: 0.8333655090332032\n",
      "Epoch 465, Loss: 0.7154971279568142\n",
      "Test Loss after Epoch 465: 0.843806396484375\n",
      "Epoch 466, Loss: 0.7118585374620225\n",
      "Test Loss after Epoch 466: 0.8368395690917969\n",
      "Epoch 467, Loss: 0.7105068427191841\n",
      "Test Loss after Epoch 467: 0.83442333984375\n",
      "Epoch 468, Loss: 0.7118833685980903\n",
      "Test Loss after Epoch 468: 0.8416835021972656\n",
      "Epoch 469, Loss: 0.7113671095106336\n",
      "Test Loss after Epoch 469: 0.8520232849121093\n",
      "Epoch 470, Loss: 0.7100034891764323\n",
      "Test Loss after Epoch 470: 0.8451426086425782\n",
      "Epoch 471, Loss: 0.7106821119520399\n",
      "Test Loss after Epoch 471: 0.8419557800292968\n",
      "Epoch 472, Loss: 0.7105930786132812\n",
      "Test Loss after Epoch 472: 0.8548110656738281\n",
      "Epoch 473, Loss: 0.7121403401692709\n",
      "Test Loss after Epoch 473: 0.8561255493164063\n",
      "Epoch 474, Loss: 0.7102597384982638\n",
      "Test Loss after Epoch 474: 0.8428629760742188\n",
      "Epoch 475, Loss: 0.7109260864257813\n",
      "Test Loss after Epoch 475: 0.833576416015625\n",
      "Epoch 476, Loss: 0.7095763108995226\n",
      "Test Loss after Epoch 476: 0.8563640441894531\n",
      "Epoch 477, Loss: 0.7089811333550348\n",
      "Test Loss after Epoch 477: 0.8566182556152344\n",
      "Epoch 478, Loss: 0.7085196431477865\n",
      "Test Loss after Epoch 478: 0.8758074951171875\n",
      "Epoch 479, Loss: 0.7085422058105468\n",
      "Test Loss after Epoch 479: 0.8596717224121094\n",
      "Epoch 480, Loss: 0.7096446567111545\n",
      "Test Loss after Epoch 480: 0.8311175842285157\n",
      "Epoch 481, Loss: 0.7096255594889322\n",
      "Test Loss after Epoch 481: 0.8439170227050782\n",
      "Epoch 482, Loss: 0.7069660407172309\n",
      "Test Loss after Epoch 482: 0.8491380615234375\n",
      "Epoch 483, Loss: 0.7070328335232204\n",
      "Test Loss after Epoch 483: 0.8497613525390625\n",
      "Epoch 484, Loss: 0.7085231526692708\n",
      "Test Loss after Epoch 484: 0.8346315002441407\n",
      "Epoch 485, Loss: 0.7069676140679253\n",
      "Test Loss after Epoch 485: 0.8480614624023437\n",
      "Epoch 486, Loss: 0.7087229275173611\n",
      "Test Loss after Epoch 486: 0.858114990234375\n",
      "Epoch 487, Loss: 0.7082677951388889\n",
      "Test Loss after Epoch 487: 0.8576244506835937\n",
      "Epoch 488, Loss: 0.7084187384711371\n",
      "Test Loss after Epoch 488: 0.8772933654785157\n",
      "Epoch 489, Loss: 0.7074879828559028\n",
      "Test Loss after Epoch 489: 0.8699074401855469\n",
      "Epoch 490, Loss: 0.7080992906358506\n",
      "Test Loss after Epoch 490: 0.8627311096191407\n",
      "Epoch 491, Loss: 0.7078407253689236\n",
      "Test Loss after Epoch 491: 0.8598397827148437\n",
      "Epoch 492, Loss: 0.7070242852105034\n",
      "Test Loss after Epoch 492: 0.8486750793457031\n",
      "Epoch 493, Loss: 0.7073122151692708\n",
      "Test Loss after Epoch 493: 0.8689169006347657\n",
      "Epoch 494, Loss: 0.7082587449815538\n",
      "Test Loss after Epoch 494: 0.8557585754394531\n",
      "Epoch 495, Loss: 0.7059875047471789\n",
      "Test Loss after Epoch 495: 0.8417701110839844\n",
      "Epoch 496, Loss: 0.7063420240614149\n",
      "Test Loss after Epoch 496: 0.8466385192871094\n",
      "Epoch 497, Loss: 0.7055963575575087\n",
      "Test Loss after Epoch 497: 0.8547486267089843\n",
      "Epoch 498, Loss: 0.7060121866861979\n",
      "Test Loss after Epoch 498: 0.8489026489257813\n",
      "Epoch 499, Loss: 0.7066182217068142\n",
      "Test Loss after Epoch 499: 0.8441405334472656\n",
      "Epoch 500, Loss: 0.7058354627821181\n",
      "Test Loss after Epoch 500: 0.8371798095703125\n",
      "Epoch 501, Loss: 0.7058631795247395\n",
      "Test Loss after Epoch 501: 0.8614770202636719\n",
      "Epoch 502, Loss: 0.7077664015028212\n",
      "Test Loss after Epoch 502: 0.8668284301757813\n",
      "Epoch 503, Loss: 0.7055187377929687\n",
      "Test Loss after Epoch 503: 0.837698974609375\n",
      "Epoch 504, Loss: 0.7060528394911024\n",
      "Test Loss after Epoch 504: 0.85314013671875\n",
      "Epoch 505, Loss: 0.705218014187283\n",
      "Test Loss after Epoch 505: 0.8800906066894532\n",
      "Epoch 506, Loss: 0.704228261311849\n",
      "Test Loss after Epoch 506: 0.8629983215332031\n",
      "Epoch 507, Loss: 0.7044794379340278\n",
      "Test Loss after Epoch 507: 0.8731926574707031\n",
      "Epoch 508, Loss: 0.7045420498318142\n",
      "Test Loss after Epoch 508: 0.865573486328125\n",
      "Epoch 509, Loss: 0.7057649332682292\n",
      "Test Loss after Epoch 509: 0.8621812744140624\n",
      "Epoch 510, Loss: 0.7048721279568142\n",
      "Test Loss after Epoch 510: 0.8919516906738282\n",
      "Epoch 511, Loss: 0.7040682033962674\n",
      "Test Loss after Epoch 511: 0.8654142150878906\n",
      "Epoch 512, Loss: 0.7050675286187066\n",
      "Test Loss after Epoch 512: 0.85172412109375\n",
      "Epoch 513, Loss: 0.704320315890842\n",
      "Test Loss after Epoch 513: 0.8538422546386719\n",
      "Epoch 514, Loss: 0.7039835510253907\n",
      "Test Loss after Epoch 514: 0.8333079833984375\n",
      "Epoch 515, Loss: 0.704629862467448\n",
      "Test Loss after Epoch 515: 0.8537400512695312\n",
      "Epoch 516, Loss: 0.7038894483778212\n",
      "Test Loss after Epoch 516: 0.8410758666992187\n",
      "Epoch 517, Loss: 0.7027771809895833\n",
      "Test Loss after Epoch 517: 0.8315598754882813\n",
      "Epoch 518, Loss: 0.7024869045681423\n",
      "Test Loss after Epoch 518: 0.844823974609375\n",
      "Epoch 519, Loss: 0.7043775126139323\n",
      "Test Loss after Epoch 519: 0.8520469970703125\n",
      "Epoch 520, Loss: 0.7034532097710503\n",
      "Test Loss after Epoch 520: 0.843399658203125\n",
      "Epoch 521, Loss: 0.7037364468044705\n",
      "Test Loss after Epoch 521: 0.8474043579101562\n",
      "Epoch 522, Loss: 0.7038094245062934\n",
      "Test Loss after Epoch 522: 0.8386184692382812\n",
      "Epoch 523, Loss: 0.7031497497558594\n",
      "Test Loss after Epoch 523: 0.8245534362792969\n",
      "Epoch 524, Loss: 0.7026439107259115\n",
      "Test Loss after Epoch 524: 0.8284308166503906\n",
      "Epoch 525, Loss: 0.7022099982367621\n",
      "Test Loss after Epoch 525: 0.8370910034179687\n",
      "Epoch 526, Loss: 0.7018900044759114\n",
      "Test Loss after Epoch 526: 0.8300219421386719\n",
      "Epoch 527, Loss: 0.7038174065483941\n",
      "Test Loss after Epoch 527: 0.8147863159179688\n",
      "Epoch 528, Loss: 0.7041483222113716\n",
      "Test Loss after Epoch 528: 0.8340509338378906\n",
      "Epoch 529, Loss: 0.7035599161783854\n",
      "Test Loss after Epoch 529: 0.8458721008300781\n",
      "Epoch 530, Loss: 0.7044605034722222\n",
      "Test Loss after Epoch 530: 0.8433692016601563\n",
      "Epoch 531, Loss: 0.702649403889974\n",
      "Test Loss after Epoch 531: 0.8326106262207031\n",
      "Epoch 532, Loss: 0.7024273444281685\n",
      "Test Loss after Epoch 532: 0.8332811889648437\n",
      "Epoch 533, Loss: 0.7031338399251302\n",
      "Test Loss after Epoch 533: 0.8283466186523437\n",
      "Epoch 534, Loss: 0.7023211059570312\n",
      "Test Loss after Epoch 534: 0.8421041259765625\n",
      "Epoch 535, Loss: 0.7011934305826822\n",
      "Test Loss after Epoch 535: 0.8630389404296875\n",
      "Epoch 536, Loss: 0.7027332458496094\n",
      "Test Loss after Epoch 536: 0.84047216796875\n",
      "Epoch 537, Loss: 0.7019123840332031\n",
      "Test Loss after Epoch 537: 0.841385986328125\n",
      "Epoch 538, Loss: 0.7015455830891927\n",
      "Test Loss after Epoch 538: 0.8544287109375\n",
      "Epoch 539, Loss: 0.7018908420138888\n",
      "Test Loss after Epoch 539: 0.8424120483398437\n",
      "Epoch 540, Loss: 0.7031737094455295\n",
      "Test Loss after Epoch 540: 0.8386955261230469\n",
      "Epoch 541, Loss: 0.7018535563151042\n",
      "Test Loss after Epoch 541: 0.8354002990722657\n",
      "Epoch 542, Loss: 0.7012619154188368\n",
      "Test Loss after Epoch 542: 0.8316354675292968\n",
      "Epoch 543, Loss: 0.7003206414116754\n",
      "Test Loss after Epoch 543: 0.8393898620605469\n",
      "Epoch 544, Loss: 0.7027648722330729\n",
      "Test Loss after Epoch 544: 0.8566839599609375\n",
      "Epoch 545, Loss: 0.7038145005967882\n",
      "Test Loss after Epoch 545: 0.85129150390625\n",
      "Epoch 546, Loss: 0.7016239725748697\n",
      "Test Loss after Epoch 546: 0.8297117919921875\n",
      "Epoch 547, Loss: 0.7027139587402343\n",
      "Test Loss after Epoch 547: 0.8192585144042969\n",
      "Epoch 548, Loss: 0.7001882697211371\n",
      "Test Loss after Epoch 548: 0.8294778137207032\n",
      "Epoch 549, Loss: 0.702551534016927\n",
      "Test Loss after Epoch 549: 0.8394989929199219\n",
      "Epoch 550, Loss: 0.7015600823296441\n",
      "Test Loss after Epoch 550: 0.8270825805664063\n",
      "Epoch 551, Loss: 0.7011092563205296\n",
      "Test Loss after Epoch 551: 0.8299610595703125\n",
      "Epoch 552, Loss: 0.7011450534396702\n",
      "Test Loss after Epoch 552: 0.8308784484863281\n",
      "Epoch 553, Loss: 0.7013729248046875\n",
      "Test Loss after Epoch 553: 0.833403076171875\n",
      "Epoch 554, Loss: 0.7021047159830729\n",
      "Test Loss after Epoch 554: 0.8416912231445313\n",
      "Epoch 555, Loss: 0.7009908684624566\n",
      "Test Loss after Epoch 555: 0.8394002075195313\n",
      "Epoch 556, Loss: 0.7002569139268663\n",
      "Test Loss after Epoch 556: 0.8554134826660156\n",
      "Epoch 557, Loss: 0.7017093302408854\n",
      "Test Loss after Epoch 557: 0.8436440124511718\n",
      "Epoch 558, Loss: 0.700955318874783\n",
      "Test Loss after Epoch 558: 0.8348699951171875\n",
      "Epoch 559, Loss: 0.700010738796658\n",
      "Test Loss after Epoch 559: 0.8534466247558594\n",
      "Epoch 560, Loss: 0.7011058349609375\n",
      "Test Loss after Epoch 560: 0.8320921936035156\n",
      "Epoch 561, Loss: 0.7029156053331164\n",
      "Test Loss after Epoch 561: 0.8345752258300781\n",
      "Epoch 562, Loss: 0.701220482720269\n",
      "Test Loss after Epoch 562: 0.8457650146484375\n",
      "Epoch 563, Loss: 0.7000499403211805\n",
      "Test Loss after Epoch 563: 0.84996142578125\n",
      "Epoch 564, Loss: 0.70088525390625\n",
      "Test Loss after Epoch 564: 0.8734537353515625\n",
      "Epoch 565, Loss: 0.7015389370388455\n",
      "Test Loss after Epoch 565: 0.8704087219238281\n",
      "Epoch 566, Loss: 0.7007114122178819\n",
      "Test Loss after Epoch 566: 0.8635713500976563\n",
      "Epoch 567, Loss: 0.7029922451443142\n",
      "Test Loss after Epoch 567: 0.8798380126953125\n",
      "Epoch 568, Loss: 0.7004717407226563\n",
      "Test Loss after Epoch 568: 0.8684180297851563\n",
      "Epoch 569, Loss: 0.7002372673882379\n",
      "Test Loss after Epoch 569: 0.8546658325195312\n",
      "Epoch 570, Loss: 0.6996094529893663\n",
      "Test Loss after Epoch 570: 0.8493023681640625\n",
      "Epoch 571, Loss: 0.7020823940700955\n",
      "Test Loss after Epoch 571: 0.863259521484375\n",
      "Epoch 572, Loss: 0.7006475931803385\n",
      "Test Loss after Epoch 572: 0.8645774536132812\n",
      "Epoch 573, Loss: 0.7015066663953993\n",
      "Test Loss after Epoch 573: 0.8728330993652343\n",
      "Epoch 574, Loss: 0.6991323581271701\n",
      "Test Loss after Epoch 574: 0.8609783325195313\n",
      "Epoch 575, Loss: 0.699236100938585\n",
      "Test Loss after Epoch 575: 0.859549560546875\n",
      "Epoch 576, Loss: 0.7000047030978732\n",
      "Test Loss after Epoch 576: 0.8657218627929687\n",
      "Epoch 577, Loss: 0.7011093241373698\n",
      "Test Loss after Epoch 577: 0.8667982788085937\n",
      "Epoch 578, Loss: 0.7003969387478298\n",
      "Test Loss after Epoch 578: 0.8746769409179688\n",
      "Epoch 579, Loss: 0.6996886664496528\n",
      "Test Loss after Epoch 579: 0.8561687316894532\n",
      "Epoch 580, Loss: 0.7001378648546007\n",
      "Test Loss after Epoch 580: 0.8384904479980468\n",
      "Epoch 581, Loss: 0.7007887742784288\n",
      "Test Loss after Epoch 581: 0.8554415283203125\n",
      "Epoch 582, Loss: 0.6995372619628907\n",
      "Test Loss after Epoch 582: 0.8835611877441406\n",
      "Epoch 583, Loss: 0.7001646423339843\n",
      "Test Loss after Epoch 583: 0.8814163818359375\n",
      "Epoch 584, Loss: 0.6999627380371094\n",
      "Test Loss after Epoch 584: 0.8663531494140625\n",
      "Epoch 585, Loss: 0.6992738613552517\n",
      "Test Loss after Epoch 585: 0.8551657409667969\n",
      "Epoch 586, Loss: 0.7001365017361111\n",
      "Test Loss after Epoch 586: 0.842726806640625\n",
      "Epoch 587, Loss: 0.6999295281304253\n",
      "Test Loss after Epoch 587: 0.8482159118652344\n",
      "Epoch 588, Loss: 0.6995359768337673\n",
      "Test Loss after Epoch 588: 0.8621019592285156\n",
      "Epoch 589, Loss: 0.7003339945475261\n",
      "Test Loss after Epoch 589: 0.8668470458984375\n",
      "Epoch 590, Loss: 0.6992970241970486\n",
      "Test Loss after Epoch 590: 0.8517694702148437\n",
      "Epoch 591, Loss: 0.6995226643880208\n",
      "Test Loss after Epoch 591: 0.8362492980957031\n",
      "Epoch 592, Loss: 0.6995539076063368\n",
      "Test Loss after Epoch 592: 0.8404417724609375\n",
      "Epoch 593, Loss: 0.7007807176378038\n",
      "Test Loss after Epoch 593: 0.8248516845703125\n",
      "Epoch 594, Loss: 0.7011697353786892\n",
      "Test Loss after Epoch 594: 0.8325895080566407\n",
      "Epoch 595, Loss: 0.6998707377115886\n",
      "Test Loss after Epoch 595: 0.8824571533203125\n",
      "Epoch 596, Loss: 0.7008735622829861\n",
      "Test Loss after Epoch 596: 0.8589680480957032\n",
      "Epoch 597, Loss: 0.6988491041395399\n",
      "Test Loss after Epoch 597: 0.8228436584472656\n",
      "Epoch 598, Loss: 0.6994644775390625\n",
      "Test Loss after Epoch 598: 0.8262772216796875\n",
      "Epoch 599, Loss: 0.6995790337456598\n",
      "Test Loss after Epoch 599: 0.8578982849121094\n",
      "Epoch 600, Loss: 0.7004615241156684\n",
      "Test Loss after Epoch 600: 0.8638320617675781\n",
      "Epoch 601, Loss: 0.698804951985677\n",
      "Test Loss after Epoch 601: 0.8358786010742187\n",
      "Epoch 602, Loss: 0.7009986334906684\n",
      "Test Loss after Epoch 602: 0.8286959533691406\n",
      "Epoch 603, Loss: 0.6987278578016493\n",
      "Test Loss after Epoch 603: 0.8470569152832031\n",
      "Epoch 604, Loss: 0.6999600490993924\n",
      "Test Loss after Epoch 604: 0.8633543701171875\n",
      "Epoch 605, Loss: 0.7003428751627604\n",
      "Test Loss after Epoch 605: 0.8597142639160156\n",
      "Epoch 606, Loss: 0.7004472249348959\n",
      "Test Loss after Epoch 606: 0.8498763732910156\n",
      "Epoch 607, Loss: 0.6993843180338541\n",
      "Test Loss after Epoch 607: 0.8643101806640625\n",
      "Epoch 608, Loss: 0.6998197021484375\n",
      "Test Loss after Epoch 608: 0.8929351806640625\n",
      "Epoch 609, Loss: 0.6996849433051215\n",
      "Test Loss after Epoch 609: 0.8955948486328125\n",
      "Epoch 610, Loss: 0.6999322340223524\n",
      "Test Loss after Epoch 610: 0.8827834167480468\n",
      "Epoch 611, Loss: 0.6988542853461371\n",
      "Test Loss after Epoch 611: 0.8651908874511719\n",
      "Epoch 612, Loss: 0.698667487250434\n",
      "Test Loss after Epoch 612: 0.8586738586425782\n",
      "Epoch 613, Loss: 0.6988867085774739\n",
      "Test Loss after Epoch 613: 0.8493226318359375\n",
      "Epoch 614, Loss: 0.6992620815700955\n",
      "Test Loss after Epoch 614: 0.8358626403808593\n",
      "Epoch 615, Loss: 0.6988628200954861\n",
      "Test Loss after Epoch 615: 0.8366658325195313\n",
      "Epoch 616, Loss: 0.6992081061469184\n",
      "Test Loss after Epoch 616: 0.8553948059082032\n",
      "Epoch 617, Loss: 0.6992556423611112\n",
      "Test Loss after Epoch 617: 0.8619757080078125\n",
      "Epoch 618, Loss: 0.6993020189073351\n",
      "Test Loss after Epoch 618: 0.8328305053710937\n",
      "Epoch 619, Loss: 0.699760498046875\n",
      "Test Loss after Epoch 619: 0.8469674682617188\n",
      "Epoch 620, Loss: 0.6999356214735243\n",
      "Test Loss after Epoch 620: 0.8574142150878906\n",
      "Epoch 621, Loss: 0.6987401936848958\n",
      "Test Loss after Epoch 621: 0.8346175231933594\n",
      "Epoch 622, Loss: 0.6987752244737413\n",
      "Test Loss after Epoch 622: 0.8231148986816407\n",
      "Epoch 623, Loss: 0.6988798929850261\n",
      "Test Loss after Epoch 623: 0.8360528259277343\n",
      "Epoch 624, Loss: 0.6981471896701389\n",
      "Test Loss after Epoch 624: 0.8397678833007812\n",
      "Epoch 625, Loss: 0.6980086364746094\n",
      "Test Loss after Epoch 625: 0.831460693359375\n",
      "Epoch 626, Loss: 0.6984794514973959\n",
      "Test Loss after Epoch 626: 0.8379078979492187\n",
      "Epoch 627, Loss: 0.6995963880750868\n",
      "Test Loss after Epoch 627: 0.8483646240234375\n",
      "Epoch 628, Loss: 0.698082753499349\n",
      "Test Loss after Epoch 628: 0.852433349609375\n",
      "Epoch 629, Loss: 0.6991956380208333\n",
      "Test Loss after Epoch 629: 0.8580830383300782\n",
      "Epoch 630, Loss: 0.6981272311740452\n",
      "Test Loss after Epoch 630: 0.8831489562988282\n",
      "Epoch 631, Loss: 0.6985951809353299\n",
      "Test Loss after Epoch 631: 0.8807503356933594\n",
      "Epoch 632, Loss: 0.6982829149034289\n",
      "Test Loss after Epoch 632: 0.8618800964355469\n",
      "Epoch 633, Loss: 0.6970076904296875\n",
      "Test Loss after Epoch 633: 0.8668050842285157\n",
      "Epoch 634, Loss: 0.699171630859375\n",
      "Test Loss after Epoch 634: 0.8818150634765625\n",
      "Epoch 635, Loss: 0.6984494798448351\n",
      "Test Loss after Epoch 635: 0.8458205261230469\n",
      "Rolling back to best model from epoch 527\n",
      "Best test loss: 0.8147863159179688\n",
      "Epoch 636, Loss: 0.703585947672526\n",
      "Test Loss after Epoch 636: 0.8211906127929688\n",
      "Epoch 637, Loss: 0.7022462158203125\n",
      "Test Loss after Epoch 637: 0.8413768310546875\n",
      "Epoch 638, Loss: 0.7007543470594618\n",
      "Test Loss after Epoch 638: 0.8424722900390625\n",
      "Epoch 639, Loss: 0.7016178792317709\n",
      "Test Loss after Epoch 639: 0.8212218933105468\n",
      "Epoch 640, Loss: 0.7023581678602431\n",
      "Test Loss after Epoch 640: 0.8300565185546875\n",
      "Epoch 641, Loss: 0.7023748372395834\n",
      "Test Loss after Epoch 641: 0.8884881286621094\n",
      "Epoch 642, Loss: 0.7040104132758247\n",
      "Test Loss after Epoch 642: 0.862234130859375\n",
      "Epoch 643, Loss: 0.7021986456976996\n",
      "Test Loss after Epoch 643: 0.8498858947753907\n",
      "Epoch 644, Loss: 0.7033366529676649\n",
      "Test Loss after Epoch 644: 0.8468702697753906\n",
      "Epoch 645, Loss: 0.7027649671766493\n",
      "Test Loss after Epoch 645: 0.8646067810058594\n",
      "Epoch 646, Loss: 0.7029543796115452\n",
      "Test Loss after Epoch 646: 0.8678128356933594\n",
      "Epoch 647, Loss: 0.702315416124132\n",
      "Test Loss after Epoch 647: 0.837594970703125\n",
      "Epoch 648, Loss: 0.7032706875271267\n",
      "Test Loss after Epoch 648: 0.8308712768554688\n",
      "Epoch 649, Loss: 0.70301611328125\n",
      "Test Loss after Epoch 649: 0.8569458923339843\n",
      "Epoch 650, Loss: 0.7023267279730903\n",
      "Test Loss after Epoch 650: 0.8499842834472656\n",
      "Epoch 651, Loss: 0.701603264702691\n",
      "Test Loss after Epoch 651: 0.8333407592773437\n",
      "Epoch 652, Loss: 0.7019848870171441\n",
      "Test Loss after Epoch 652: 0.8275016784667969\n",
      "Epoch 653, Loss: 0.7021668260362414\n",
      "Test Loss after Epoch 653: 0.8386992492675781\n",
      "Epoch 654, Loss: 0.7013854743109809\n",
      "Test Loss after Epoch 654: 0.8451454467773437\n",
      "Epoch 655, Loss: 0.7021893581814236\n",
      "Test Loss after Epoch 655: 0.8421018981933593\n",
      "Epoch 656, Loss: 0.7008070441351997\n",
      "Test Loss after Epoch 656: 0.8312835388183594\n",
      "Epoch 657, Loss: 0.7017877977159288\n",
      "Test Loss after Epoch 657: 0.8376320190429688\n",
      "Epoch 658, Loss: 0.7009187113444011\n",
      "Test Loss after Epoch 658: 0.8444524230957031\n",
      "Epoch 659, Loss: 0.7011579318576389\n",
      "Test Loss after Epoch 659: 0.8225506286621094\n",
      "Epoch 660, Loss: 0.7008539835611979\n",
      "Test Loss after Epoch 660: 0.8362294006347656\n",
      "Epoch 661, Loss: 0.7010106574164496\n",
      "Test Loss after Epoch 661: 0.8621534118652344\n",
      "Epoch 662, Loss: 0.701454576280382\n",
      "Test Loss after Epoch 662: 0.8409342041015625\n",
      "Epoch 663, Loss: 0.7011657206217448\n",
      "Test Loss after Epoch 663: 0.8269332580566406\n",
      "Epoch 664, Loss: 0.7008481004503038\n",
      "Test Loss after Epoch 664: 0.8315690307617187\n",
      "Epoch 665, Loss: 0.7020775722927517\n",
      "Test Loss after Epoch 665: 0.8396380004882813\n",
      "Epoch 666, Loss: 0.7016515943739149\n",
      "Test Loss after Epoch 666: 0.840130859375\n",
      "Epoch 667, Loss: 0.7014538201226128\n",
      "Test Loss after Epoch 667: 0.8316336059570313\n",
      "Epoch 668, Loss: 0.7000198872884115\n",
      "Test Loss after Epoch 668: 0.8299716186523437\n",
      "Epoch 669, Loss: 0.7002311469184028\n",
      "Test Loss after Epoch 669: 0.8400753784179688\n",
      "Epoch 670, Loss: 0.7022367892795139\n",
      "Test Loss after Epoch 670: 0.8463125\n",
      "Epoch 671, Loss: 0.7011930474175347\n",
      "Test Loss after Epoch 671: 0.84712548828125\n",
      "Epoch 672, Loss: 0.7009832695855035\n",
      "Test Loss after Epoch 672: 0.8334759826660156\n",
      "Epoch 673, Loss: 0.6997355109320746\n",
      "Test Loss after Epoch 673: 0.8504457702636719\n",
      "Epoch 674, Loss: 0.7006781548394098\n",
      "Test Loss after Epoch 674: 0.8537708129882813\n",
      "Epoch 675, Loss: 0.7005458001030817\n",
      "Test Loss after Epoch 675: 0.8287676391601563\n",
      "Epoch 676, Loss: 0.6992689073350694\n",
      "Test Loss after Epoch 676: 0.8228472290039063\n",
      "Epoch 677, Loss: 0.6997513224283854\n",
      "Test Loss after Epoch 677: 0.8372432250976563\n",
      "Epoch 678, Loss: 0.698699215359158\n",
      "Test Loss after Epoch 678: 0.8270108947753906\n",
      "Epoch 679, Loss: 0.7005266655815973\n",
      "Test Loss after Epoch 679: 0.8128997497558593\n",
      "Epoch 680, Loss: 0.6994699062771267\n",
      "Test Loss after Epoch 680: 0.8111338500976563\n",
      "Epoch 681, Loss: 0.7001087748209636\n",
      "Test Loss after Epoch 681: 0.8204728393554688\n",
      "Epoch 682, Loss: 0.7002316012912326\n",
      "Test Loss after Epoch 682: 0.8388467102050782\n",
      "Epoch 683, Loss: 0.7003606770833334\n",
      "Test Loss after Epoch 683: 0.8580976867675781\n",
      "Epoch 684, Loss: 0.6997193162706163\n",
      "Test Loss after Epoch 684: 0.8636708374023437\n",
      "Epoch 685, Loss: 0.699516116672092\n",
      "Test Loss after Epoch 685: 0.8639476013183593\n",
      "Epoch 686, Loss: 0.7006975741916233\n",
      "Test Loss after Epoch 686: 0.8604183349609374\n",
      "Epoch 687, Loss: 0.6992953728569878\n",
      "Test Loss after Epoch 687: 0.8483782043457031\n",
      "Epoch 688, Loss: 0.7003687506781684\n",
      "Test Loss after Epoch 688: 0.83992236328125\n",
      "Epoch 689, Loss: 0.7009312811957465\n",
      "Test Loss after Epoch 689: 0.84135693359375\n",
      "Epoch 690, Loss: 0.70003466796875\n",
      "Test Loss after Epoch 690: 0.8232072143554687\n",
      "Epoch 691, Loss: 0.699192637125651\n",
      "Test Loss after Epoch 691: 0.8142639770507812\n",
      "Epoch 692, Loss: 0.7001135219997829\n",
      "Test Loss after Epoch 692: 0.8251094360351563\n",
      "Epoch 693, Loss: 0.7000185139973958\n",
      "Test Loss after Epoch 693: 0.8483099365234374\n",
      "Epoch 694, Loss: 0.7000564303927952\n",
      "Test Loss after Epoch 694: 0.8710316162109375\n",
      "Epoch 695, Loss: 0.6992169494628906\n",
      "Test Loss after Epoch 695: 0.8527483520507813\n",
      "Epoch 696, Loss: 0.6986897786458334\n",
      "Test Loss after Epoch 696: 0.8417485656738282\n",
      "Epoch 697, Loss: 0.6993251241048177\n",
      "Test Loss after Epoch 697: 0.8462615661621093\n",
      "Epoch 698, Loss: 0.6993497043185763\n",
      "Test Loss after Epoch 698: 0.8300104370117187\n",
      "Epoch 699, Loss: 0.7005373229980468\n",
      "Test Loss after Epoch 699: 0.8260483093261719\n",
      "Epoch 700, Loss: 0.6993409254286024\n",
      "Test Loss after Epoch 700: 0.829041748046875\n",
      "Epoch 701, Loss: 0.6986501261393229\n",
      "Test Loss after Epoch 701: 0.83089501953125\n",
      "Epoch 702, Loss: 0.6985604858398438\n",
      "Test Loss after Epoch 702: 0.8274878234863281\n",
      "Epoch 703, Loss: 0.6976554463704427\n",
      "Test Loss after Epoch 703: 0.8299825439453125\n",
      "Epoch 704, Loss: 0.6996233723958334\n",
      "Test Loss after Epoch 704: 0.8246626586914062\n",
      "Epoch 705, Loss: 0.6994767252604167\n",
      "Test Loss after Epoch 705: 0.8121167907714844\n",
      "Epoch 706, Loss: 0.6992962544759115\n",
      "Test Loss after Epoch 706: 0.8157068176269531\n",
      "Epoch 707, Loss: 0.6988184441460503\n",
      "Test Loss after Epoch 707: 0.8324646911621094\n",
      "Epoch 708, Loss: 0.6990907931857638\n",
      "Test Loss after Epoch 708: 0.8478251037597656\n",
      "Epoch 709, Loss: 0.6985837775336372\n",
      "Test Loss after Epoch 709: 0.862568603515625\n",
      "Epoch 710, Loss: 0.69937646484375\n",
      "Test Loss after Epoch 710: 0.8423349914550782\n",
      "Epoch 711, Loss: 0.6982933247884114\n",
      "Test Loss after Epoch 711: 0.8296170043945312\n",
      "Epoch 712, Loss: 0.6991193610297309\n",
      "Test Loss after Epoch 712: 0.8348407592773438\n",
      "Epoch 713, Loss: 0.6978731486002604\n",
      "Test Loss after Epoch 713: 0.8339752502441407\n",
      "Epoch 714, Loss: 0.6980943942599827\n",
      "Test Loss after Epoch 714: 0.8255836181640624\n",
      "Epoch 715, Loss: 0.6986815728081597\n",
      "Test Loss after Epoch 715: 0.8253530883789062\n",
      "Epoch 716, Loss: 0.6986104532877604\n",
      "Test Loss after Epoch 716: 0.8278179626464843\n",
      "Epoch 717, Loss: 0.6982179260253907\n",
      "Test Loss after Epoch 717: 0.8263785095214844\n",
      "Epoch 718, Loss: 0.697821292453342\n",
      "Test Loss after Epoch 718: 0.82314794921875\n",
      "Epoch 719, Loss: 0.6979733784993489\n",
      "Test Loss after Epoch 719: 0.8287894287109375\n",
      "Epoch 720, Loss: 0.6991025661892362\n",
      "Test Loss after Epoch 720: 0.8375965270996094\n",
      "Epoch 721, Loss: 0.6987969190809462\n",
      "Test Loss after Epoch 721: 0.8382677001953125\n",
      "Epoch 722, Loss: 0.6992557237413194\n",
      "Test Loss after Epoch 722: 0.827589111328125\n",
      "Epoch 723, Loss: 0.6983542887369791\n",
      "Test Loss after Epoch 723: 0.8259442443847657\n",
      "Epoch 724, Loss: 0.6996588609483507\n",
      "Test Loss after Epoch 724: 0.8290301513671875\n",
      "Epoch 725, Loss: 0.698436272515191\n",
      "Test Loss after Epoch 725: 0.8420321044921875\n",
      "Epoch 726, Loss: 0.6986703423394097\n",
      "Test Loss after Epoch 726: 0.8445740356445313\n",
      "Epoch 727, Loss: 0.6988783230251736\n",
      "Test Loss after Epoch 727: 0.8386914978027343\n",
      "Epoch 728, Loss: 0.6980105658637152\n",
      "Test Loss after Epoch 728: 0.8436947326660156\n",
      "Epoch 729, Loss: 0.6988403150770399\n",
      "Test Loss after Epoch 729: 0.8377674560546875\n",
      "Epoch 730, Loss: 0.6993017578125\n",
      "Test Loss after Epoch 730: 0.8399226684570312\n",
      "Epoch 731, Loss: 0.6987083943684896\n",
      "Test Loss after Epoch 731: 0.8562813110351563\n",
      "Epoch 732, Loss: 0.6986120266384549\n",
      "Test Loss after Epoch 732: 0.8457498474121093\n",
      "Epoch 733, Loss: 0.6968216518825955\n",
      "Test Loss after Epoch 733: 0.8313603820800781\n",
      "Epoch 734, Loss: 0.6984851243760851\n",
      "Test Loss after Epoch 734: 0.8294497375488281\n",
      "Epoch 735, Loss: 0.6982236599392361\n",
      "Test Loss after Epoch 735: 0.8402693176269531\n",
      "Epoch 736, Loss: 0.6989031914605035\n",
      "Test Loss after Epoch 736: 0.8485029907226562\n",
      "Epoch 737, Loss: 0.6979333123101128\n",
      "Test Loss after Epoch 737: 0.8308200988769531\n",
      "Epoch 738, Loss: 0.6991896870930989\n",
      "Test Loss after Epoch 738: 0.8171358032226562\n",
      "Epoch 739, Loss: 0.6980057847764757\n",
      "Test Loss after Epoch 739: 0.8306655578613281\n",
      "Epoch 740, Loss: 0.6985292595757379\n",
      "Test Loss after Epoch 740: 0.8660186767578125\n",
      "Epoch 741, Loss: 0.6995692477756077\n",
      "Test Loss after Epoch 741: 0.843233154296875\n",
      "Epoch 742, Loss: 0.6994108547634549\n",
      "Test Loss after Epoch 742: 0.8285330810546875\n",
      "Epoch 743, Loss: 0.6986579081217448\n",
      "Test Loss after Epoch 743: 0.8261268615722657\n",
      "Epoch 744, Loss: 0.6976682773166233\n",
      "Test Loss after Epoch 744: 0.8436799621582032\n",
      "Epoch 745, Loss: 0.697301008436415\n",
      "Test Loss after Epoch 745: 0.8396686401367187\n",
      "Epoch 746, Loss: 0.6977164950900607\n",
      "Test Loss after Epoch 746: 0.8411871337890625\n",
      "Epoch 747, Loss: 0.6979325018988716\n",
      "Test Loss after Epoch 747: 0.8583717956542969\n",
      "Epoch 748, Loss: 0.6983422580295139\n",
      "Test Loss after Epoch 748: 0.8550693969726563\n",
      "Epoch 749, Loss: 0.69797509765625\n",
      "Test Loss after Epoch 749: 0.8226318359375\n",
      "Epoch 750, Loss: 0.6976432732476129\n",
      "Test Loss after Epoch 750: 0.8355092468261719\n",
      "Epoch 751, Loss: 0.6980807461208768\n",
      "Test Loss after Epoch 751: 0.8492396850585937\n",
      "Epoch 752, Loss: 0.6983654649522569\n",
      "Test Loss after Epoch 752: 0.8422535095214844\n",
      "Epoch 753, Loss: 0.6971300354003906\n",
      "Test Loss after Epoch 753: 0.8266575622558594\n",
      "Epoch 754, Loss: 0.6982653198242188\n",
      "Test Loss after Epoch 754: 0.8209538879394531\n",
      "Epoch 755, Loss: 0.6985460917154948\n",
      "Test Loss after Epoch 755: 0.823376708984375\n",
      "Epoch 756, Loss: 0.6977275458441841\n",
      "Test Loss after Epoch 756: 0.8520333251953125\n",
      "Epoch 757, Loss: 0.697671620686849\n",
      "Test Loss after Epoch 757: 0.8791455688476563\n",
      "Epoch 758, Loss: 0.6977118259006077\n",
      "Test Loss after Epoch 758: 0.8533287658691406\n",
      "Epoch 759, Loss: 0.6976016947428385\n",
      "Test Loss after Epoch 759: 0.8423978271484375\n",
      "Epoch 760, Loss: 0.698013668484158\n",
      "Test Loss after Epoch 760: 0.8671294860839843\n",
      "Epoch 761, Loss: 0.6976901346842448\n",
      "Test Loss after Epoch 761: 0.8699811096191407\n",
      "Epoch 762, Loss: 0.6966831732855903\n",
      "Test Loss after Epoch 762: 0.8432651977539063\n",
      "Epoch 763, Loss: 0.6974589335123698\n",
      "Test Loss after Epoch 763: 0.8355927429199219\n",
      "Epoch 764, Loss: 0.697110843234592\n",
      "Test Loss after Epoch 764: 0.8348477783203125\n",
      "Epoch 765, Loss: 0.6980760972764757\n",
      "Test Loss after Epoch 765: 0.8344346313476563\n",
      "Epoch 766, Loss: 0.6976602783203125\n",
      "Test Loss after Epoch 766: 0.8387144470214843\n",
      "Epoch 767, Loss: 0.69802783203125\n",
      "Test Loss after Epoch 767: 0.8328727416992188\n",
      "Epoch 768, Loss: 0.6975972052680122\n",
      "Test Loss after Epoch 768: 0.8537911071777343\n",
      "Epoch 769, Loss: 0.6972210862901476\n",
      "Test Loss after Epoch 769: 0.8609214782714844\n",
      "Epoch 770, Loss: 0.6975695326063368\n",
      "Test Loss after Epoch 770: 0.8379071044921875\n",
      "Epoch 771, Loss: 0.6978234219021268\n",
      "Test Loss after Epoch 771: 0.8275529174804688\n",
      "Epoch 772, Loss: 0.6980982835557725\n",
      "Test Loss after Epoch 772: 0.8514388427734375\n",
      "Epoch 773, Loss: 0.697054433186849\n",
      "Test Loss after Epoch 773: 0.8859979248046875\n",
      "Epoch 774, Loss: 0.6976172553168403\n",
      "Test Loss after Epoch 774: 0.8851221008300781\n",
      "Epoch 775, Loss: 0.6980860900878906\n",
      "Test Loss after Epoch 775: 0.8559652709960938\n",
      "Epoch 776, Loss: 0.6967508748372396\n",
      "Test Loss after Epoch 776: 0.8365580749511718\n",
      "Epoch 777, Loss: 0.6978552788628473\n",
      "Test Loss after Epoch 777: 0.8393107604980469\n",
      "Epoch 778, Loss: 0.6979752298990886\n",
      "Test Loss after Epoch 778: 0.8710458068847656\n",
      "Epoch 779, Loss: 0.6977368503146701\n",
      "Test Loss after Epoch 779: 0.8949651184082031\n",
      "Epoch 780, Loss: 0.6971233554416233\n",
      "Test Loss after Epoch 780: 0.8473702392578125\n",
      "Epoch 781, Loss: 0.697947747124566\n",
      "Test Loss after Epoch 781: 0.8307870483398437\n",
      "Epoch 782, Loss: 0.6976590440538194\n",
      "Test Loss after Epoch 782: 0.854121826171875\n",
      "Epoch 783, Loss: 0.6967289055718316\n",
      "Test Loss after Epoch 783: 0.8661724853515625\n",
      "Epoch 784, Loss: 0.69755177476671\n",
      "Test Loss after Epoch 784: 0.8648892822265625\n",
      "Epoch 785, Loss: 0.6976533745659722\n",
      "Test Loss after Epoch 785: 0.8690648193359375\n",
      "Epoch 786, Loss: 0.6973636271158854\n",
      "Test Loss after Epoch 786: 0.8404078369140625\n",
      "Epoch 787, Loss: 0.6969045579698351\n",
      "Test Loss after Epoch 787: 0.8283995361328125\n",
      "Epoch 788, Loss: 0.6976598714192709\n",
      "Test Loss after Epoch 788: 0.8358124389648437\n",
      "Epoch 789, Loss: 0.6966714986165364\n",
      "Test Loss after Epoch 789: 0.8463680725097656\n",
      "Epoch 790, Loss: 0.6972300177680122\n",
      "Test Loss after Epoch 790: 0.8556866149902344\n",
      "Epoch 791, Loss: 0.6979224683973524\n",
      "Test Loss after Epoch 791: 0.8628658447265625\n",
      "Epoch 792, Loss: 0.6967879672580295\n",
      "Test Loss after Epoch 792: 0.8562607727050782\n",
      "Epoch 793, Loss: 0.6970638359917535\n",
      "Test Loss after Epoch 793: 0.8440291748046875\n",
      "Epoch 794, Loss: 0.6967718641493056\n",
      "Test Loss after Epoch 794: 0.8424922180175781\n",
      "Epoch 795, Loss: 0.6970163404676649\n",
      "Test Loss after Epoch 795: 0.8396784973144531\n",
      "Epoch 796, Loss: 0.6976561041937934\n",
      "Test Loss after Epoch 796: 0.8408137512207031\n",
      "Epoch 797, Loss: 0.6966739366319444\n",
      "Test Loss after Epoch 797: 0.8360416259765625\n",
      "Epoch 798, Loss: 0.6970412428114149\n",
      "Test Loss after Epoch 798: 0.8394390869140625\n",
      "Epoch 799, Loss: 0.6973638814290365\n",
      "Test Loss after Epoch 799: 0.8710860290527344\n",
      "Epoch 800, Loss: 0.6975644395616319\n",
      "Test Loss after Epoch 800: 0.8908944091796875\n",
      "Epoch 801, Loss: 0.697576419406467\n",
      "Test Loss after Epoch 801: 0.8894306640625\n",
      "Epoch 802, Loss: 0.6969226548936632\n",
      "Test Loss after Epoch 802: 0.8808146362304687\n",
      "Epoch 803, Loss: 0.6966050415039062\n",
      "Test Loss after Epoch 803: 0.8832718811035156\n",
      "Epoch 804, Loss: 0.6970091722276476\n",
      "Test Loss after Epoch 804: 0.8883226928710938\n",
      "Epoch 805, Loss: 0.6971374138726129\n",
      "Test Loss after Epoch 805: 0.8839317016601562\n",
      "Epoch 806, Loss: 0.6961470913357205\n",
      "Test Loss after Epoch 806: 0.8659063110351563\n",
      "Epoch 807, Loss: 0.6973909810384115\n",
      "Test Loss after Epoch 807: 0.8755389099121094\n",
      "Epoch 808, Loss: 0.6962905409071181\n",
      "Test Loss after Epoch 808: 0.8954520263671875\n",
      "Epoch 809, Loss: 0.6965553724500868\n",
      "Test Loss after Epoch 809: 0.8654164733886719\n",
      "Epoch 810, Loss: 0.697216064453125\n",
      "Test Loss after Epoch 810: 0.8268018798828125\n",
      "Epoch 811, Loss: 0.6967513122558594\n",
      "Test Loss after Epoch 811: 0.8269337463378906\n",
      "Epoch 812, Loss: 0.6970547790527344\n",
      "Test Loss after Epoch 812: 0.8457513427734376\n",
      "Epoch 813, Loss: 0.6974960361056858\n",
      "Test Loss after Epoch 813: 0.8799928588867187\n",
      "Epoch 814, Loss: 0.6972334933810764\n",
      "Test Loss after Epoch 814: 0.84792138671875\n",
      "Epoch 815, Loss: 0.697381839328342\n",
      "Test Loss after Epoch 815: 0.8264668273925782\n",
      "Epoch 816, Loss: 0.6974691433376736\n",
      "Test Loss after Epoch 816: 0.8661869201660156\n",
      "Epoch 817, Loss: 0.6975154418945313\n",
      "Test Loss after Epoch 817: 0.9110989379882812\n",
      "Epoch 818, Loss: 0.6971973266601562\n",
      "Test Loss after Epoch 818: 0.860164794921875\n",
      "Epoch 819, Loss: 0.6968365478515625\n",
      "Test Loss after Epoch 819: 0.8155591430664062\n",
      "Epoch 820, Loss: 0.6972630377875434\n",
      "Test Loss after Epoch 820: 0.8180355529785156\n",
      "Epoch 821, Loss: 0.6968679843478732\n",
      "Test Loss after Epoch 821: 0.850774658203125\n",
      "Epoch 822, Loss: 0.6974706285264757\n",
      "Test Loss after Epoch 822: 0.8869614868164063\n",
      "Epoch 823, Loss: 0.6970216437445746\n",
      "Test Loss after Epoch 823: 0.8912427368164062\n",
      "Epoch 824, Loss: 0.6977723761664496\n",
      "Test Loss after Epoch 824: 0.8651654357910157\n",
      "Epoch 825, Loss: 0.6969664204915365\n",
      "Test Loss after Epoch 825: 0.8450306396484375\n",
      "Epoch 826, Loss: 0.6967371927897136\n",
      "Test Loss after Epoch 826: 0.8425281677246094\n",
      "Epoch 827, Loss: 0.6972183091905382\n",
      "Test Loss after Epoch 827: 0.8444385375976563\n",
      "Epoch 828, Loss: 0.6963926798502604\n",
      "Test Loss after Epoch 828: 0.8265072631835938\n",
      "Epoch 829, Loss: 0.6968890346950954\n",
      "Test Loss after Epoch 829: 0.8149451599121094\n",
      "Epoch 830, Loss: 0.6967974887424045\n",
      "Test Loss after Epoch 830: 0.8212228698730468\n",
      "Epoch 831, Loss: 0.6970231153700087\n",
      "Test Loss after Epoch 831: 0.8244590454101562\n",
      "Epoch 832, Loss: 0.6959503546820747\n",
      "Test Loss after Epoch 832: 0.8229890441894532\n",
      "Rolling back to best model from epoch 680\n",
      "Best test loss: 0.8111338500976563\n",
      "Epoch 833, Loss: 0.6993067016601563\n",
      "Test Loss after Epoch 833: 0.8256451721191407\n",
      "Epoch 834, Loss: 0.7003341064453125\n",
      "Test Loss after Epoch 834: 0.8421356201171875\n",
      "Epoch 835, Loss: 0.6987914360894097\n",
      "Test Loss after Epoch 835: 0.8451780395507813\n",
      "Epoch 836, Loss: 0.6993850436740452\n",
      "Test Loss after Epoch 836: 0.8428521728515626\n",
      "Epoch 837, Loss: 0.698288099500868\n",
      "Test Loss after Epoch 837: 0.8305007019042969\n",
      "Epoch 838, Loss: 0.6984677293565538\n",
      "Test Loss after Epoch 838: 0.8301190490722656\n",
      "Epoch 839, Loss: 0.6983796759711371\n",
      "Test Loss after Epoch 839: 0.8514850463867187\n",
      "Epoch 840, Loss: 0.6989900444878472\n",
      "Test Loss after Epoch 840: 0.8559835205078125\n",
      "Epoch 841, Loss: 0.6984434950086805\n",
      "Test Loss after Epoch 841: 0.8519600524902343\n",
      "Epoch 842, Loss: 0.699136698404948\n",
      "Test Loss after Epoch 842: 0.8430644836425781\n",
      "Epoch 843, Loss: 0.7001289265950521\n",
      "Test Loss after Epoch 843: 0.8440422058105469\n",
      "Epoch 844, Loss: 0.6995805087619358\n",
      "Test Loss after Epoch 844: 0.8613501281738282\n",
      "Epoch 845, Loss: 0.6990909050835503\n",
      "Test Loss after Epoch 845: 0.8686264343261719\n",
      "Epoch 846, Loss: 0.6997738850911458\n",
      "Test Loss after Epoch 846: 0.8663454895019531\n",
      "Epoch 847, Loss: 0.6989773254394531\n",
      "Test Loss after Epoch 847: 0.87617041015625\n",
      "Epoch 848, Loss: 0.6999196472167969\n",
      "Test Loss after Epoch 848: 0.8522315979003906\n",
      "Epoch 849, Loss: 0.6989569837782118\n",
      "Test Loss after Epoch 849: 0.8523396606445313\n",
      "Epoch 850, Loss: 0.6987754007975261\n",
      "Test Loss after Epoch 850: 0.8690418701171875\n",
      "Epoch 851, Loss: 0.6996746758355035\n",
      "Test Loss after Epoch 851: 0.8546209716796875\n",
      "Epoch 852, Loss: 0.7001594543457031\n",
      "Test Loss after Epoch 852: 0.8545937805175782\n",
      "Epoch 853, Loss: 0.6992322252061632\n",
      "Test Loss after Epoch 853: 0.8523560485839844\n",
      "Epoch 854, Loss: 0.6994785902235243\n",
      "Test Loss after Epoch 854: 0.8376770629882813\n",
      "Epoch 855, Loss: 0.6997707960340712\n",
      "Test Loss after Epoch 855: 0.8411286926269531\n",
      "Epoch 856, Loss: 0.699245107014974\n",
      "Test Loss after Epoch 856: 0.8330968933105469\n",
      "Epoch 857, Loss: 0.6995222608778212\n",
      "Test Loss after Epoch 857: 0.8173793029785156\n",
      "Epoch 858, Loss: 0.6992881300184461\n",
      "Test Loss after Epoch 858: 0.8181217651367187\n",
      "Epoch 859, Loss: 0.6998159417046441\n",
      "Test Loss after Epoch 859: 0.8221130981445313\n",
      "Epoch 860, Loss: 0.6980574544270833\n",
      "Test Loss after Epoch 860: 0.8374595947265625\n",
      "Epoch 861, Loss: 0.6986147054036458\n",
      "Test Loss after Epoch 861: 0.8372674865722656\n",
      "Epoch 862, Loss: 0.6989047580295139\n",
      "Test Loss after Epoch 862: 0.8225918273925781\n",
      "Epoch 863, Loss: 0.6988059997558593\n",
      "Test Loss after Epoch 863: 0.8141032104492187\n",
      "Epoch 864, Loss: 0.6984510430230034\n",
      "Test Loss after Epoch 864: 0.8240089721679688\n",
      "Epoch 865, Loss: 0.699565673828125\n",
      "Test Loss after Epoch 865: 0.8517868041992187\n",
      "Epoch 866, Loss: 0.699325920952691\n",
      "Test Loss after Epoch 866: 0.8539707946777344\n",
      "Epoch 867, Loss: 0.6981842719184028\n",
      "Test Loss after Epoch 867: 0.8364917602539063\n",
      "Epoch 868, Loss: 0.6992347412109375\n",
      "Test Loss after Epoch 868: 0.8318425903320312\n",
      "Epoch 869, Loss: 0.6988208143446181\n",
      "Test Loss after Epoch 869: 0.8235848388671875\n",
      "Epoch 870, Loss: 0.6983635559082031\n",
      "Test Loss after Epoch 870: 0.8291191711425782\n",
      "Epoch 871, Loss: 0.6987386779785156\n",
      "Test Loss after Epoch 871: 0.829325927734375\n",
      "Epoch 872, Loss: 0.6990691392686632\n",
      "Test Loss after Epoch 872: 0.8265891723632812\n",
      "Epoch 873, Loss: 0.698108164469401\n",
      "Test Loss after Epoch 873: 0.8373097839355469\n",
      "Epoch 874, Loss: 0.6982884216308594\n",
      "Test Loss after Epoch 874: 0.8396246032714844\n",
      "Epoch 875, Loss: 0.6976538764105903\n",
      "Test Loss after Epoch 875: 0.8306365966796875\n",
      "Epoch 876, Loss: 0.6986725260416666\n",
      "Test Loss after Epoch 876: 0.8303829650878907\n",
      "Epoch 877, Loss: 0.6994324374728733\n",
      "Test Loss after Epoch 877: 0.8316407165527344\n",
      "Epoch 878, Loss: 0.6983340318467882\n",
      "Test Loss after Epoch 878: 0.8253934326171875\n",
      "Epoch 879, Loss: 0.6987250535753038\n",
      "Test Loss after Epoch 879: 0.8275608825683594\n",
      "Epoch 880, Loss: 0.6973773498535156\n",
      "Test Loss after Epoch 880: 0.8292509155273438\n",
      "Epoch 881, Loss: 0.6980971204969618\n",
      "Test Loss after Epoch 881: 0.8274277954101562\n",
      "Epoch 882, Loss: 0.6981369222005208\n",
      "Test Loss after Epoch 882: 0.8328212890625\n",
      "Epoch 883, Loss: 0.6983324788411458\n",
      "Test Loss after Epoch 883: 0.8356916198730469\n",
      "Epoch 884, Loss: 0.6982867092556424\n",
      "Test Loss after Epoch 884: 0.8395326232910156\n",
      "Epoch 885, Loss: 0.6977199062771268\n",
      "Test Loss after Epoch 885: 0.8330634765625\n",
      "Epoch 886, Loss: 0.6986443956163194\n",
      "Test Loss after Epoch 886: 0.8341865234375\n",
      "Epoch 887, Loss: 0.6973124220106337\n",
      "Test Loss after Epoch 887: 0.8386909484863281\n",
      "Epoch 888, Loss: 0.6977026570638021\n",
      "Test Loss after Epoch 888: 0.8427802429199219\n",
      "Epoch 889, Loss: 0.6981073438856337\n",
      "Test Loss after Epoch 889: 0.8500743408203125\n",
      "Epoch 890, Loss: 0.6988487108018663\n",
      "Test Loss after Epoch 890: 0.8557213134765626\n",
      "Epoch 891, Loss: 0.6980686916775174\n",
      "Test Loss after Epoch 891: 0.8427847595214844\n",
      "Epoch 892, Loss: 0.6977121921115451\n",
      "Test Loss after Epoch 892: 0.8405973205566406\n",
      "Epoch 893, Loss: 0.6986417371961805\n",
      "Test Loss after Epoch 893: 0.8601549682617188\n",
      "Epoch 894, Loss: 0.6975047573513455\n",
      "Test Loss after Epoch 894: 0.8578692932128906\n",
      "Epoch 895, Loss: 0.6978265380859375\n",
      "Test Loss after Epoch 895: 0.8397284240722657\n",
      "Epoch 896, Loss: 0.6974047309027778\n",
      "Test Loss after Epoch 896: 0.8417348327636719\n",
      "Epoch 897, Loss: 0.6976769578721789\n",
      "Test Loss after Epoch 897: 0.85586181640625\n",
      "Epoch 898, Loss: 0.6972014024522569\n",
      "Test Loss after Epoch 898: 0.8766251525878906\n",
      "Epoch 899, Loss: 0.698459733751085\n",
      "Test Loss after Epoch 899: 0.8784703369140625\n",
      "Epoch 900, Loss: 0.697459706624349\n",
      "Test Loss after Epoch 900: 0.8718011779785156\n",
      "Epoch 901, Loss: 0.6977647603352864\n",
      "Test Loss after Epoch 901: 0.8709063110351563\n",
      "Epoch 902, Loss: 0.697937249077691\n",
      "Test Loss after Epoch 902: 0.871978759765625\n",
      "Epoch 903, Loss: 0.6960997416178385\n",
      "Test Loss after Epoch 903: 0.8636495971679687\n",
      "Epoch 904, Loss: 0.6977049967447917\n",
      "Test Loss after Epoch 904: 0.8470255737304687\n",
      "Epoch 905, Loss: 0.6973452012803819\n",
      "Test Loss after Epoch 905: 0.83369384765625\n",
      "Epoch 906, Loss: 0.6974471808539496\n",
      "Test Loss after Epoch 906: 0.8441329650878906\n",
      "Epoch 907, Loss: 0.6964365980360243\n",
      "Test Loss after Epoch 907: 0.8529100036621093\n",
      "Epoch 908, Loss: 0.6976804504394531\n",
      "Test Loss after Epoch 908: 0.849734375\n",
      "Epoch 909, Loss: 0.6969390903049045\n",
      "Test Loss after Epoch 909: 0.8545647277832031\n",
      "Epoch 910, Loss: 0.6981202223036024\n",
      "Test Loss after Epoch 910: 0.8830060729980469\n",
      "Epoch 911, Loss: 0.6978639763726129\n",
      "Test Loss after Epoch 911: 0.8873698120117187\n",
      "Epoch 912, Loss: 0.6972643432617187\n",
      "Test Loss after Epoch 912: 0.8490458374023437\n",
      "Epoch 913, Loss: 0.6977058308919271\n",
      "Test Loss after Epoch 913: 0.8469107666015625\n",
      "Epoch 914, Loss: 0.69683445570204\n",
      "Test Loss after Epoch 914: 0.8766800537109375\n",
      "Epoch 915, Loss: 0.6986909823947483\n",
      "Test Loss after Epoch 915: 0.8876703186035156\n",
      "Epoch 916, Loss: 0.6967651536729601\n",
      "Test Loss after Epoch 916: 0.865799560546875\n",
      "Epoch 917, Loss: 0.6970836520724827\n",
      "Test Loss after Epoch 917: 0.847744873046875\n",
      "Epoch 918, Loss: 0.697830576578776\n",
      "Test Loss after Epoch 918: 0.8595373229980469\n",
      "Epoch 919, Loss: 0.6976380750868055\n",
      "Test Loss after Epoch 919: 0.8813983154296875\n",
      "Epoch 920, Loss: 0.6972550116644965\n",
      "Test Loss after Epoch 920: 0.8741077575683593\n",
      "Epoch 921, Loss: 0.6970558437771267\n",
      "Test Loss after Epoch 921: 0.8530980529785156\n",
      "Epoch 922, Loss: 0.6973481174045139\n",
      "Test Loss after Epoch 922: 0.8478007202148438\n",
      "Epoch 923, Loss: 0.6970792948404948\n",
      "Test Loss after Epoch 923: 0.8772098388671875\n",
      "Epoch 924, Loss: 0.6971157565646702\n",
      "Test Loss after Epoch 924: 0.8846859741210937\n",
      "Epoch 925, Loss: 0.6975360514322917\n",
      "Test Loss after Epoch 925: 0.8678501281738281\n",
      "Epoch 926, Loss: 0.6976678466796875\n",
      "Test Loss after Epoch 926: 0.8652614440917968\n",
      "Epoch 927, Loss: 0.6977883605957031\n",
      "Test Loss after Epoch 927: 0.8732170104980469\n",
      "Epoch 928, Loss: 0.6975661960177951\n",
      "Test Loss after Epoch 928: 0.8805972290039062\n",
      "Epoch 929, Loss: 0.696621090359158\n",
      "Test Loss after Epoch 929: 0.8693563537597656\n",
      "Epoch 930, Loss: 0.6972873094346789\n",
      "Test Loss after Epoch 930: 0.8477360229492188\n",
      "Epoch 931, Loss: 0.6975318501790364\n",
      "Test Loss after Epoch 931: 0.8370446166992187\n",
      "Epoch 932, Loss: 0.697727294921875\n",
      "Test Loss after Epoch 932: 0.8590006103515625\n",
      "Epoch 933, Loss: 0.6963840094672309\n",
      "Test Loss after Epoch 933: 0.8998439025878906\n",
      "Epoch 934, Loss: 0.6968534071180555\n",
      "Test Loss after Epoch 934: 0.9222371520996093\n",
      "Epoch 935, Loss: 0.6971742655436198\n",
      "Test Loss after Epoch 935: 0.9024882202148438\n",
      "Epoch 936, Loss: 0.6970672166612413\n",
      "Test Loss after Epoch 936: 0.8801706237792969\n",
      "Epoch 937, Loss: 0.696975572374132\n",
      "Test Loss after Epoch 937: 0.8457818908691406\n",
      "Epoch 938, Loss: 0.6972007412380642\n",
      "Test Loss after Epoch 938: 0.831731689453125\n",
      "Epoch 939, Loss: 0.696181650797526\n",
      "Test Loss after Epoch 939: 0.8370507202148437\n",
      "Epoch 940, Loss: 0.6966698371039497\n",
      "Test Loss after Epoch 940: 0.8546868286132813\n",
      "Epoch 941, Loss: 0.6977143792046441\n",
      "Test Loss after Epoch 941: 0.8460003662109375\n",
      "Epoch 942, Loss: 0.6967389153374566\n",
      "Test Loss after Epoch 942: 0.8369164428710938\n",
      "Epoch 943, Loss: 0.6967091166178385\n",
      "Test Loss after Epoch 943: 0.8381560668945313\n",
      "Epoch 944, Loss: 0.6967239583333333\n",
      "Test Loss after Epoch 944: 0.8472667846679688\n",
      "Epoch 945, Loss: 0.6965196058485243\n",
      "Test Loss after Epoch 945: 0.8483070678710938\n",
      "Epoch 946, Loss: 0.6972146775987413\n",
      "Test Loss after Epoch 946: 0.8379399719238281\n",
      "Epoch 947, Loss: 0.6968991292317709\n",
      "Test Loss after Epoch 947: 0.8357560424804688\n",
      "Epoch 948, Loss: 0.6973152228461371\n",
      "Test Loss after Epoch 948: 0.8452536010742188\n",
      "Epoch 949, Loss: 0.6966682942708333\n",
      "Test Loss after Epoch 949: 0.851628173828125\n",
      "Epoch 950, Loss: 0.6976790839301216\n",
      "Test Loss after Epoch 950: 0.8413367309570312\n",
      "Epoch 951, Loss: 0.6966316223144531\n",
      "Test Loss after Epoch 951: 0.8357962341308594\n",
      "Epoch 952, Loss: 0.6969005466037327\n",
      "Test Loss after Epoch 952: 0.8257694702148437\n",
      "Epoch 953, Loss: 0.6968543463812934\n",
      "Test Loss after Epoch 953: 0.8220917053222656\n",
      "Epoch 954, Loss: 0.6964117024739583\n",
      "Test Loss after Epoch 954: 0.8216205749511719\n",
      "Epoch 955, Loss: 0.6968801439073351\n",
      "Test Loss after Epoch 955: 0.8107502746582032\n",
      "Epoch 956, Loss: 0.6971629163953993\n",
      "Test Loss after Epoch 956: 0.8151524047851563\n",
      "Epoch 957, Loss: 0.6966554938422309\n",
      "Test Loss after Epoch 957: 0.8356770629882813\n",
      "Epoch 958, Loss: 0.6963574795193143\n",
      "Test Loss after Epoch 958: 0.8571095581054687\n",
      "Epoch 959, Loss: 0.6970597330729167\n",
      "Test Loss after Epoch 959: 0.8403282165527344\n",
      "Epoch 960, Loss: 0.6970318603515625\n",
      "Test Loss after Epoch 960: 0.8367290954589843\n",
      "Epoch 961, Loss: 0.6966027899848091\n",
      "Test Loss after Epoch 961: 0.8488404846191406\n",
      "Epoch 962, Loss: 0.6966404113769531\n",
      "Test Loss after Epoch 962: 0.8612825622558594\n",
      "Epoch 963, Loss: 0.6975069173177083\n",
      "Test Loss after Epoch 963: 0.8427222595214844\n",
      "Epoch 964, Loss: 0.6963603413899739\n",
      "Test Loss after Epoch 964: 0.8321111450195312\n",
      "Epoch 965, Loss: 0.6973075425889756\n",
      "Test Loss after Epoch 965: 0.8253605041503906\n",
      "Epoch 966, Loss: 0.6966065334743924\n",
      "Test Loss after Epoch 966: 0.8295185241699219\n",
      "Epoch 967, Loss: 0.6965992363823784\n",
      "Test Loss after Epoch 967: 0.836584228515625\n",
      "Epoch 968, Loss: 0.6970642598470053\n",
      "Test Loss after Epoch 968: 0.8368216857910157\n",
      "Epoch 969, Loss: 0.6968257480197483\n",
      "Test Loss after Epoch 969: 0.84290966796875\n",
      "Epoch 970, Loss: 0.6963619249131945\n",
      "Test Loss after Epoch 970: 0.8532890625\n",
      "Epoch 971, Loss: 0.6969784817165798\n",
      "Test Loss after Epoch 971: 0.8544530639648438\n",
      "Epoch 972, Loss: 0.6958169420030382\n",
      "Test Loss after Epoch 972: 0.8318885803222656\n",
      "Epoch 973, Loss: 0.6961663140190972\n",
      "Test Loss after Epoch 973: 0.8147240905761719\n",
      "Epoch 974, Loss: 0.6973109300401475\n",
      "Test Loss after Epoch 974: 0.8084875183105469\n",
      "Epoch 975, Loss: 0.6961804606119791\n",
      "Test Loss after Epoch 975: 0.821623046875\n",
      "Epoch 976, Loss: 0.6965680304633246\n",
      "Test Loss after Epoch 976: 0.8279411926269531\n",
      "Epoch 977, Loss: 0.6966058688693576\n",
      "Test Loss after Epoch 977: 0.8346469116210937\n",
      "Epoch 978, Loss: 0.696522945827908\n",
      "Test Loss after Epoch 978: 0.8362931823730468\n",
      "Epoch 979, Loss: 0.6966506754557291\n",
      "Test Loss after Epoch 979: 0.8338674011230469\n",
      "Epoch 980, Loss: 0.6967521226671007\n",
      "Test Loss after Epoch 980: 0.8336316528320312\n",
      "Epoch 981, Loss: 0.6960529106987847\n",
      "Test Loss after Epoch 981: 0.820927001953125\n",
      "Epoch 982, Loss: 0.6964698723687066\n",
      "Test Loss after Epoch 982: 0.8108038330078124\n",
      "Epoch 983, Loss: 0.6961284111870659\n",
      "Test Loss after Epoch 983: 0.8080650634765625\n",
      "Epoch 984, Loss: 0.6954576144748263\n",
      "Test Loss after Epoch 984: 0.8121876525878906\n",
      "Epoch 985, Loss: 0.6964251573350695\n",
      "Test Loss after Epoch 985: 0.8080877075195313\n",
      "Epoch 986, Loss: 0.6960796813964844\n",
      "Test Loss after Epoch 986: 0.8168594360351562\n",
      "Epoch 987, Loss: 0.6958864101833767\n",
      "Test Loss after Epoch 987: 0.8304581909179688\n",
      "Epoch 988, Loss: 0.696148430718316\n",
      "Test Loss after Epoch 988: 0.8443062744140625\n",
      "Epoch 989, Loss: 0.6967466227213541\n",
      "Test Loss after Epoch 989: 0.8512879028320313\n",
      "Epoch 990, Loss: 0.6963157992892796\n",
      "Test Loss after Epoch 990: 0.8628424072265625\n",
      "Epoch 991, Loss: 0.6964822726779514\n",
      "Test Loss after Epoch 991: 0.8590313110351563\n",
      "Epoch 992, Loss: 0.6965342576768663\n",
      "Test Loss after Epoch 992: 0.8585179138183594\n",
      "Epoch 993, Loss: 0.6955974188910591\n",
      "Test Loss after Epoch 993: 0.8508414611816406\n",
      "Epoch 994, Loss: 0.6974430779351128\n",
      "Test Loss after Epoch 994: 0.84646142578125\n",
      "Epoch 995, Loss: 0.6956886121961805\n",
      "Test Loss after Epoch 995: 0.8471095275878906\n",
      "Epoch 996, Loss: 0.6964748229980469\n",
      "Test Loss after Epoch 996: 0.8407781677246093\n",
      "Epoch 997, Loss: 0.696711649576823\n",
      "Test Loss after Epoch 997: 0.8425381164550781\n",
      "Epoch 998, Loss: 0.6968105333116319\n",
      "Test Loss after Epoch 998: 0.8496690063476563\n",
      "Epoch 999, Loss: 0.6959989827473958\n",
      "Test Loss after Epoch 999: 0.8524203491210938\n",
      "Epoch 1000, Loss: 0.6969569363064236\n",
      "Test Loss after Epoch 1000: 0.8495426635742187\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_examples, test_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learnable_positional_encoding: torch.Size([32, 256]), torch.float32, 8192 params\n",
      "embedding.weight: torch.Size([39, 256]), torch.float32, 9984 params\n",
      "transformer.encoder.layers.0.self_attn.in_proj_weight: torch.Size([768, 256]), torch.float32, 196608 params\n",
      "transformer.encoder.layers.0.self_attn.in_proj_bias: torch.Size([768]), torch.float32, 768 params\n",
      "transformer.encoder.layers.0.self_attn.out_proj.weight: torch.Size([256, 256]), torch.float32, 65536 params\n",
      "transformer.encoder.layers.0.self_attn.out_proj.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.encoder.layers.0.linear1.weight: torch.Size([1024, 256]), torch.float32, 262144 params\n",
      "transformer.encoder.layers.0.linear1.bias: torch.Size([1024]), torch.float32, 1024 params\n",
      "transformer.encoder.layers.0.linear2.weight: torch.Size([256, 1024]), torch.float32, 262144 params\n",
      "transformer.encoder.layers.0.linear2.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.encoder.layers.0.norm1.weight: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.encoder.layers.0.norm1.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.encoder.layers.0.norm2.weight: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.encoder.layers.0.norm2.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.encoder.layers.1.self_attn.in_proj_weight: torch.Size([768, 256]), torch.float32, 196608 params\n",
      "transformer.encoder.layers.1.self_attn.in_proj_bias: torch.Size([768]), torch.float32, 768 params\n",
      "transformer.encoder.layers.1.self_attn.out_proj.weight: torch.Size([256, 256]), torch.float32, 65536 params\n",
      "transformer.encoder.layers.1.self_attn.out_proj.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.encoder.layers.1.linear1.weight: torch.Size([1024, 256]), torch.float32, 262144 params\n",
      "transformer.encoder.layers.1.linear1.bias: torch.Size([1024]), torch.float32, 1024 params\n",
      "transformer.encoder.layers.1.linear2.weight: torch.Size([256, 1024]), torch.float32, 262144 params\n",
      "transformer.encoder.layers.1.linear2.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.encoder.layers.1.norm1.weight: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.encoder.layers.1.norm1.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.encoder.layers.1.norm2.weight: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.encoder.layers.1.norm2.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.encoder.layers.2.self_attn.in_proj_weight: torch.Size([768, 256]), torch.float32, 196608 params\n",
      "transformer.encoder.layers.2.self_attn.in_proj_bias: torch.Size([768]), torch.float32, 768 params\n",
      "transformer.encoder.layers.2.self_attn.out_proj.weight: torch.Size([256, 256]), torch.float32, 65536 params\n",
      "transformer.encoder.layers.2.self_attn.out_proj.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.encoder.layers.2.linear1.weight: torch.Size([1024, 256]), torch.float32, 262144 params\n",
      "transformer.encoder.layers.2.linear1.bias: torch.Size([1024]), torch.float32, 1024 params\n",
      "transformer.encoder.layers.2.linear2.weight: torch.Size([256, 1024]), torch.float32, 262144 params\n",
      "transformer.encoder.layers.2.linear2.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.encoder.layers.2.norm1.weight: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.encoder.layers.2.norm1.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.encoder.layers.2.norm2.weight: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.encoder.layers.2.norm2.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.encoder.layers.3.self_attn.in_proj_weight: torch.Size([768, 256]), torch.float32, 196608 params\n",
      "transformer.encoder.layers.3.self_attn.in_proj_bias: torch.Size([768]), torch.float32, 768 params\n",
      "transformer.encoder.layers.3.self_attn.out_proj.weight: torch.Size([256, 256]), torch.float32, 65536 params\n",
      "transformer.encoder.layers.3.self_attn.out_proj.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.encoder.layers.3.linear1.weight: torch.Size([1024, 256]), torch.float32, 262144 params\n",
      "transformer.encoder.layers.3.linear1.bias: torch.Size([1024]), torch.float32, 1024 params\n",
      "transformer.encoder.layers.3.linear2.weight: torch.Size([256, 1024]), torch.float32, 262144 params\n",
      "transformer.encoder.layers.3.linear2.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.encoder.layers.3.norm1.weight: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.encoder.layers.3.norm1.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.encoder.layers.3.norm2.weight: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.encoder.layers.3.norm2.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.encoder.norm.weight: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.encoder.norm.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.0.self_attn.in_proj_weight: torch.Size([768, 256]), torch.float32, 196608 params\n",
      "transformer.decoder.layers.0.self_attn.in_proj_bias: torch.Size([768]), torch.float32, 768 params\n",
      "transformer.decoder.layers.0.self_attn.out_proj.weight: torch.Size([256, 256]), torch.float32, 65536 params\n",
      "transformer.decoder.layers.0.self_attn.out_proj.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.0.multihead_attn.in_proj_weight: torch.Size([768, 256]), torch.float32, 196608 params\n",
      "transformer.decoder.layers.0.multihead_attn.in_proj_bias: torch.Size([768]), torch.float32, 768 params\n",
      "transformer.decoder.layers.0.multihead_attn.out_proj.weight: torch.Size([256, 256]), torch.float32, 65536 params\n",
      "transformer.decoder.layers.0.multihead_attn.out_proj.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.0.linear1.weight: torch.Size([1024, 256]), torch.float32, 262144 params\n",
      "transformer.decoder.layers.0.linear1.bias: torch.Size([1024]), torch.float32, 1024 params\n",
      "transformer.decoder.layers.0.linear2.weight: torch.Size([256, 1024]), torch.float32, 262144 params\n",
      "transformer.decoder.layers.0.linear2.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.0.norm1.weight: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.0.norm1.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.0.norm2.weight: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.0.norm2.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.0.norm3.weight: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.0.norm3.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.1.self_attn.in_proj_weight: torch.Size([768, 256]), torch.float32, 196608 params\n",
      "transformer.decoder.layers.1.self_attn.in_proj_bias: torch.Size([768]), torch.float32, 768 params\n",
      "transformer.decoder.layers.1.self_attn.out_proj.weight: torch.Size([256, 256]), torch.float32, 65536 params\n",
      "transformer.decoder.layers.1.self_attn.out_proj.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.1.multihead_attn.in_proj_weight: torch.Size([768, 256]), torch.float32, 196608 params\n",
      "transformer.decoder.layers.1.multihead_attn.in_proj_bias: torch.Size([768]), torch.float32, 768 params\n",
      "transformer.decoder.layers.1.multihead_attn.out_proj.weight: torch.Size([256, 256]), torch.float32, 65536 params\n",
      "transformer.decoder.layers.1.multihead_attn.out_proj.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.1.linear1.weight: torch.Size([1024, 256]), torch.float32, 262144 params\n",
      "transformer.decoder.layers.1.linear1.bias: torch.Size([1024]), torch.float32, 1024 params\n",
      "transformer.decoder.layers.1.linear2.weight: torch.Size([256, 1024]), torch.float32, 262144 params\n",
      "transformer.decoder.layers.1.linear2.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.1.norm1.weight: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.1.norm1.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.1.norm2.weight: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.1.norm2.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.1.norm3.weight: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.1.norm3.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.2.self_attn.in_proj_weight: torch.Size([768, 256]), torch.float32, 196608 params\n",
      "transformer.decoder.layers.2.self_attn.in_proj_bias: torch.Size([768]), torch.float32, 768 params\n",
      "transformer.decoder.layers.2.self_attn.out_proj.weight: torch.Size([256, 256]), torch.float32, 65536 params\n",
      "transformer.decoder.layers.2.self_attn.out_proj.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.2.multihead_attn.in_proj_weight: torch.Size([768, 256]), torch.float32, 196608 params\n",
      "transformer.decoder.layers.2.multihead_attn.in_proj_bias: torch.Size([768]), torch.float32, 768 params\n",
      "transformer.decoder.layers.2.multihead_attn.out_proj.weight: torch.Size([256, 256]), torch.float32, 65536 params\n",
      "transformer.decoder.layers.2.multihead_attn.out_proj.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.2.linear1.weight: torch.Size([1024, 256]), torch.float32, 262144 params\n",
      "transformer.decoder.layers.2.linear1.bias: torch.Size([1024]), torch.float32, 1024 params\n",
      "transformer.decoder.layers.2.linear2.weight: torch.Size([256, 1024]), torch.float32, 262144 params\n",
      "transformer.decoder.layers.2.linear2.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.2.norm1.weight: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.2.norm1.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.2.norm2.weight: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.2.norm2.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.2.norm3.weight: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.2.norm3.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.3.self_attn.in_proj_weight: torch.Size([768, 256]), torch.float32, 196608 params\n",
      "transformer.decoder.layers.3.self_attn.in_proj_bias: torch.Size([768]), torch.float32, 768 params\n",
      "transformer.decoder.layers.3.self_attn.out_proj.weight: torch.Size([256, 256]), torch.float32, 65536 params\n",
      "transformer.decoder.layers.3.self_attn.out_proj.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.3.multihead_attn.in_proj_weight: torch.Size([768, 256]), torch.float32, 196608 params\n",
      "transformer.decoder.layers.3.multihead_attn.in_proj_bias: torch.Size([768]), torch.float32, 768 params\n",
      "transformer.decoder.layers.3.multihead_attn.out_proj.weight: torch.Size([256, 256]), torch.float32, 65536 params\n",
      "transformer.decoder.layers.3.multihead_attn.out_proj.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.3.linear1.weight: torch.Size([1024, 256]), torch.float32, 262144 params\n",
      "transformer.decoder.layers.3.linear1.bias: torch.Size([1024]), torch.float32, 1024 params\n",
      "transformer.decoder.layers.3.linear2.weight: torch.Size([256, 1024]), torch.float32, 262144 params\n",
      "transformer.decoder.layers.3.linear2.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.3.norm1.weight: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.3.norm1.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.3.norm2.weight: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.3.norm2.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.3.norm3.weight: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.layers.3.norm3.bias: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.norm.weight: torch.Size([256]), torch.float32, 256 params\n",
      "transformer.decoder.norm.bias: torch.Size([256]), torch.float32, 256 params\n",
      "fc_out.weight: torch.Size([39, 256]), torch.float32, 9984 params\n",
      "fc_out.bias: torch.Size([39]), torch.float32, 39 params\n",
      "Total number of parameters: 7402023\n"
     ]
    }
   ],
   "source": [
    "total_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    param_count = param.numel()  # Number of elements in the tensor\n",
    "    total_params += param_count\n",
    "    print(f\"{name}: {param.size()}, {param.dtype}, {param_count} params\")\n",
    "\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/home/minhk/Assignments/CSCI 5801/project/model/\"\n",
    "filename=\"base_transformers.pth\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "torch.save(model, save_dir+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PRS>pathotype pathotype\n",
      "torch.Size([1, 12])\n",
      "torch.Size([1, 12])\n",
      "pathotype\n",
      "<3SG>pathotype pathotypes\n",
      "torch.Size([1, 12])\n",
      "torch.Size([1, 12])\n",
      "pathotypes\n",
      "<PST>pathotype pathotyped\n",
      "torch.Size([1, 12])\n",
      "torch.Size([1, 12])\n",
      "pathotyped\n",
      "<PRS.PTCP>pathotype pathotyping\n",
      "torch.Size([1, 12])\n",
      "torch.Size([1, 12])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (12) must match the size of tensor b (13) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(src_tokenized\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(feature_mask\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 16\u001b[0m gen \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_tokenized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     17\u001b[0m gen \u001b[38;5;241m=\u001b[39m list_to_word([idx_to_char[\u001b[38;5;28mid\u001b[39m\u001b[38;5;241m.\u001b[39mitem()] \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m gen])\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(gen)\n",
      "Cell \u001b[0;32mIn[2], line 87\u001b[0m, in \u001b[0;36mCharTransformer.generate\u001b[0;34m(self, src, feature_mask, max_len, beam_size)\u001b[0m\n\u001b[1;32m     84\u001b[0m feature_mask_expanded \u001b[38;5;241m=\u001b[39m feature_mask\u001b[38;5;241m.\u001b[39mexpand(beams\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Forward pass on all beams at once\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_expanded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_mask_expanded\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     88\u001b[0m logits \u001b[38;5;241m=\u001b[39m out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# Get last-step logits (shape: [beams, vocab_size])\u001b[39;00m\n\u001b[1;32m     89\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog_softmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Convert logits to log-probabilities\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 43\u001b[0m, in \u001b[0;36mCharTransformer.forward\u001b[0;34m(self, src, tgt, feature_mask, tgt_is_causal, src_mask, tgt_mask, src_key_padding_mask, tgt_key_padding_mask)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Apply positional encoding only to non-feature tokens\u001b[39;00m\n\u001b[1;32m     42\u001b[0m src_emb \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m feature_mask[:, :src\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m]) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearnable_positional_encoding[:src\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], :]\n\u001b[0;32m---> 43\u001b[0m tgt_emb \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfeature_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mtgt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearnable_positional_encoding\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mtgt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Compute key padding masks if not provided\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m src_key_padding_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (12) must match the size of tensor b (13) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Select a random paradigm from validation set\n",
    "random_paradigm = random.choice(val_set)\n",
    "\n",
    "# Generate examples from the paradigm\n",
    "generated_examples = generate_examples(random_paradigm)\n",
    "\n",
    "def list_to_word(arr):\n",
    "    return ''.join(arr[1:-1])\n",
    "\n",
    "for src, tgt in generated_examples: \n",
    "    print(list_to_word(src), list_to_word(tgt))\n",
    "    feature_mask = create_feature_mask(src).unsqueeze(0)\n",
    "    src_tokenized = torch.tensor(tokenize(src, char_to_idx), device=device).unsqueeze(0)\n",
    "    print(src_tokenized.shape)\n",
    "    print(feature_mask.shape)\n",
    "    gen = model.generate(src_tokenized, feature_mask, beam_size=5).squeeze(0)\n",
    "    gen = list_to_word([idx_to_char[id.item()] for id in gen])\n",
    "    print(gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3334691788.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[22], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    for tag in tags:\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "word = 'run'\n",
    "tags = [\"PRS\", \"3SG\", \"PST\", \"PRS.PTCP\", \"PST.PTCP\"]\n",
    "\n",
    "for tag in tags: \n",
    "    tokens = [\"<s>\", f\"<{tag}>\"] + list(word) + [\"</s>\"]\n",
    "    feature_mask = create_feature_mask(src).unsqueeze(0)\n",
    "    src_tokenized = torch.tensor(tokenize(src, char_to_idx), device=device).unsqueeze(0)\n",
    "    gen = model.generate(src_tokenized, feature_mask).squeeze(0)\n",
    "    gen = [idx_to_char[id.item()] for id in gen]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3514 predictions correct out of 5000 total. Accuracy: 0.7028\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "for paradigm in val_set:\n",
    "    pairs = generate_examples(paradigm)\n",
    "    for src, tgt in pairs: \n",
    "        src = pad_sequence(src, max_len)\n",
    "        feature_mask = create_feature_mask(src).unsqueeze(0)\n",
    "        src_tokenized = torch.tensor(tokenize(src, char_to_idx), device=device).unsqueeze(0)\n",
    "        gen = model.generate(src_tokenized, feature_mask).squeeze(0)\n",
    "        gen = [idx_to_char[id.item()] for id in gen]\n",
    "        correct_predictions += (gen == tgt)\n",
    "        total_predictions += 1\n",
    "\n",
    "print(f\"{correct_predictions} predictions correct out of {total_predictions} total. Accuracy: {correct_predictions / total_predictions}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
